{"file_contents":{"docs-mastra.md":{"content":"---\ntitle: \"Dynamic Agents\"\ndescription: Dynamically configure your agent's instruction, model and tools using runtime context.\n---\n\n# Dynamic Agents\n[EN] Source: https://mastra.ai/en/docs/agents/dynamic-agents\n\nDynamic agents use [runtime context](./runtime-variables), like user IDs and other important parameters, to adjust their settings in real-time.\n\nThis means they can change the model they use, update their instructions, and select different tools as needed.\n\nBy using this context, agents can better respond to each user's needs. They can also call any API to gather more information, which helps improve what the agents can do.\n\n### Example Configuration\n\nHere's an example of a dynamic support agent that adjusts its behavior based on the user's subscription tier and language preferences:\n\n```typescript\nconst supportAgent = new Agent({\n  name: \"Dynamic Support Agent\",\n\n  instructions: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\");\n    const language = runtimeContext.get(\"language\");\n\n    return `You are a customer support agent for our SaaS platform.\n    The current user is on the ${userTier} tier and prefers ${language} language.\n    \n    For ${userTier} tier users:\n    ${userTier === \"free\" ? \"- Provide basic support and documentation links\" : \"\"}\n    ${userTier === \"pro\" ? \"- Offer detailed technical support and best practices\" : \"\"}\n    ${userTier === \"enterprise\" ? \"- Provide priority support with custom solutions\" : \"\"}\n    \n    Always respond in ${language} language.`;\n  },\n\n  model: ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\");\n    return userTier === \"enterprise\"\n      ? openai(\"gpt-4\")\n      : openai(\"gpt-3.5-turbo\");\n  },\n\n  tools: ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\");\n    const baseTools = [knowledgeBase, ticketSystem];\n\n    if (userTier === \"pro\" || userTier === \"enterprise\") {\n      baseTools.push(advancedAnalytics);\n    }\n\n    if (userTier === \"enterprise\") {\n      baseTools.push(customIntegration);\n    }\n\n    return baseTools;\n  },\n});\n```\n\nIn this example, the agent:\n\n- Adjusts its instructions based on the user's subscription tier (free, pro, or enterprise)\n- Uses a more powerful model (GPT-4) for enterprise users\n- Provides different sets of tools based on the user's tier\n- Responds in the user's preferred language\n\nThis demonstrates how a single agent can handle different types of users and scenarios by leveraging runtime context, making it more flexible and maintainable than creating separate agents for each use case.\n\nFor a complete implementation example including API routes, middleware setup, and runtime context handling, see our [Dynamic Agents Example](/examples/agents/dynamic-agents).\n\n\n---\ntitle: \"Agent Overview | Agent Documentation | Mastra\"\ndescription: Overview of agents in Mastra, detailing their capabilities and how they interact with tools, workflows, and external systems.\n---\n\n# Using Agents\n[EN] Source: https://mastra.ai/en/docs/agents/overview\n\n**Agents** are one of the core Mastra primitives. Agents use a language model to decide on a sequence of actions. They can call functions (known as _tools_). You can compose them with *workflows* (the other main Mastra primitive), either by giving an agent a workflow as a tool, or by running an agent from within a workflow.\n\nAgents can run autonomously in a loop, run once, or take turns with a user. You can give short-term, long-term, and working memory of their user interactions. They can stream text or return structured output (ie, JSON). They can access third-party APIs, query knowledge bases, and so on.\n\n## 1. Creating an Agent\n\nTo create an agent in Mastra, you use the `Agent` class and define its properties:\n\n```ts showLineNumbers filename=\"src/mastra/agents/index.ts\" copy\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\n\nexport const myAgent = new Agent({\n  name: \"My Agent\",\n  instructions: \"You are a helpful assistant.\",\n  model: openai(\"gpt-4o-mini\"),\n});\n```\n\n**Note:** Ensure that you have set the necessary environment variables, such as your OpenAI API key, in your `.env` file:\n\n```.env filename=\".env\" copy\nOPENAI_API_KEY=your_openai_api_key\n```\n\nAlso, make sure you have the `@mastra/core` package installed:\n\n```bash npm2yarn copy\nnpm install @mastra/core@latest\n```\n\n### Registering the Agent\n\nRegister your agent with Mastra to enable logging and access to configured tools and integrations:\n\n```ts showLineNumbers filename=\"src/mastra/index.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { myAgent } from \"./agents\";\n\nexport const mastra = new Mastra({\n  agents: { myAgent },\n});\n```\n\n## 2. Generating and streaming text\n\n### Generating text\n\nUse the `.generate()` method to have your agent produce text responses:\n\n```ts showLineNumbers filename=\"src/mastra/index.ts\" copy\nconst response = await myAgent.generate([\n  { role: \"user\", content: \"Hello, how can you assist me today?\" },\n]);\n\nconsole.log(\"Agent:\", response.text);\n```\n\nFor more details about the generate method and its options, see the [generate reference documentation](/reference/agents/generate).\n\n### Streaming responses\n\nFor more real-time responses, you can stream the agent's response:\n\n```ts showLineNumbers filename=\"src/mastra/index.ts\" copy\nconst stream = await myAgent.stream([\n  { role: \"user\", content: \"Tell me a story.\" },\n]);\n\nconsole.log(\"Agent:\");\n\nfor await (const chunk of stream.textStream) {\n  process.stdout.write(chunk);\n}\n```\n\nFor more details about streaming responses, see the [stream reference documentation](/reference/agents/stream).\n\n## 3. Structured Output\n\nAgents can return structured data by providing a JSON Schema or using a Zod schema.\n\n### Using JSON Schema\n\n```typescript\nconst schema = {\n  type: \"object\",\n  properties: {\n    summary: { type: \"string\" },\n    keywords: { type: \"array\", items: { type: \"string\" } },\n  },\n  additionalProperties: false,\n  required: [\"summary\", \"keywords\"],\n};\n\nconst response = await myAgent.generate(\n  [\n    {\n      role: \"user\",\n      content:\n        \"Please provide a summary and keywords for the following text: ...\",\n    },\n  ],\n  {\n    output: schema,\n  },\n);\n\nconsole.log(\"Structured Output:\", response.object);\n```\n\n### Using Zod\n\nYou can also use Zod schemas for type-safe structured outputs.\n\nFirst, install Zod:\n\n```bash npm2yarn copy\nnpm install zod\n```\n\nThen, define a Zod schema and use it with the agent:\n\n```ts showLineNumbers filename=\"src/mastra/index.ts\" copy\nimport { z } from \"zod\";\n\n// Define the Zod schema\nconst schema = z.object({\n  summary: z.string(),\n  keywords: z.array(z.string()),\n});\n\n// Use the schema with the agent\nconst response = await myAgent.generate(\n  [\n    {\n      role: \"user\",\n      content:\n        \"Please provide a summary and keywords for the following text: ...\",\n    },\n  ],\n  {\n    output: schema,\n  },\n);\n\nconsole.log(\"Structured Output:\", response.object);\n```\n\n### Using Tools\n\nIf you need to generate structured output alongside tool calls, you'll need to use the `experimental_output` property instead of `output`. Here's how:\n\n```typescript\nconst schema = z.object({\n  summary: z.string(),\n  keywords: z.array(z.string()),\n});\n\nconst response = await myAgent.generate(\n  [\n    {\n      role: \"user\",\n      content:\n        \"Please analyze this repository and provide a summary and keywords...\",\n    },\n  ],\n  {\n    // Use experimental_output to enable both structured output and tool calls\n    experimental_output: schema,\n  },\n);\n\nconsole.log(\"Structured Output:\", response.object);\n```\n\n<br />\n\nThis allows you to have strong typing and validation for the structured data returned by the agent.\n\n## 4. Multi-step tool use\n\nAgents can be enhanced with tools - functions that extend their capabilities beyond text generation. Tools allow agents to perform calculations, access external systems, and process data. Agents not only decide whether to call tools they're given, they determine the parameters that should be given to that tool.\n\nFor a detailed guide to creating and configuring tools, see the [Adding Tools documentation](/docs/agents/using-tools-and-mcp), but below are the important things to know.\n\n### Using `maxSteps`\n\nThe `maxSteps` parameter controls the maximum number of sequential LLM calls an agent can make, particularly important when using tool calls. By default, it is set to 1 to prevent infinite loops in case of misconfigured tools. You can increase this limit based on your use case:\n\n```ts showLineNumbers filename=\"src/mastra/agents/index.ts\" copy\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport * as mathjs from \"mathjs\";\nimport { z } from \"zod\";\n\nexport const myAgent = new Agent({\n  name: \"My Agent\",\n  instructions: \"You are a helpful assistant that can solve math problems.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    calculate: {\n      description: \"Calculator for mathematical expressions\",\n      schema: z.object({ expression: z.string() }),\n      execute: async ({ expression }) => mathjs.evaluate(expression),\n    },\n  },\n});\n\nconst response = await myAgent.generate(\n  [\n    {\n      role: \"user\",\n      content:\n        \"If a taxi driver earns $41 per hour and works 12 hours a day, how much do they earn in one day?\",\n    },\n  ],\n  {\n    maxSteps: 5, // Allow up to 5 tool usage steps\n  },\n);\n```\n\n### Streaming progress with `onStepFinish`\n\nYou can monitor the progress of multi-step operations using the `onStepFinish` callback. This is useful for debugging or providing progress updates to users.\n\n`onStepFinish` is only available when streaming or generating text without structured output.\n\n```ts showLineNumbers filename=\"src/mastra/agents/index.ts\" copy\nconst response = await myAgent.generate(\n  [{ role: \"user\", content: \"Calculate the taxi driver's daily earnings.\" }],\n  {\n    maxSteps: 5,\n    onStepFinish: ({ text, toolCalls, toolResults }) => {\n      console.log(\"Step completed:\", { text, toolCalls, toolResults });\n    },\n  },\n);\n```\n\n### Detecting completion with `onFinish`\n\nThe `onFinish` callback is available when streaming responses and provides detailed information about the completed interaction. It is called after the LLM has finished generating its response and all tool executions have completed.\nThis callback receives the final response text, execution steps, token usage statistics, and other metadata that can be useful for monitoring and logging:\n\n```ts showLineNumbers filename=\"src/mastra/agents/index.ts\" copy\nconst stream = await myAgent.stream(\n  [{ role: \"user\", content: \"Calculate the taxi driver's daily earnings.\" }],\n  {\n    maxSteps: 5,\n    onFinish: ({\n      steps,\n      text,\n      finishReason, // 'complete', 'length', 'tool', etc.\n      usage, // token usage statistics\n      reasoningDetails, // additional context about the agent's decisions\n    }) => {\n      console.log(\"Stream complete:\", {\n        totalSteps: steps.length,\n        finishReason,\n        usage,\n      });\n    },\n  },\n);\n```\n\n## 5. Testing agents locally\n\nMastra provides a CLI command `mastra dev` to run your agents behind an API. By default, this looks for exported agents in files in the `src/mastra/agents` directory. It generates endpoints for testing your agent (eg `http://localhost:5000/api/agents/myAgent/generate`) and provides a visual playground where you can chat with an agent and view traces.\n\nFor more details, see the [Local Dev Playground](/docs/server-db/local-dev-playground) docs.\n\n## Next Steps\n\n- Learn about Agent Memory in the [Agent Memory](./agent-memory.mdx) guide.\n- Learn about Agent Tools in the [Agent Tools and MCP](./using-tools-and-mcp.mdx) guide.\n- See an example agent in the [Chef Michel](../../guides/guide/chef-michel.mdx) example.\n\n\n---\ntitle: \"Runtime context | Agents | Mastra Docs\"\ndescription: Learn how to use Mastra's dependency injection system to provide runtime configuration to agents and tools.\n---\n\n# Agent Runtime Context\n[EN] Source: https://mastra.ai/en/docs/agents/runtime-variables\n\nMastra provides runtime context, which is a system based on dependency injection that enables you to configure your agents and tools with runtime variables. If you find yourself creating several different agents that do very similar things, runtime context allows you to combine them into one agent.\n\n## Overview\n\nThe dependency injection system allows you to:\n\n1. Pass runtime configuration variables to agents through a type-safe runtimeContext\n2. Access these variables within tool execution contexts\n3. Modify agent behavior without changing the underlying code\n4. Share configuration across multiple tools within the same agent\n\n## Basic Usage\n\n```typescript\nconst agent = mastra.getAgent(\"weatherAgent\");\n\n// Define your runtimeContext's type structure\ntype WeatherRuntimeContext = {\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\"; // Fixed typo in \"fahrenheit\"\n};\n\nconst runtimeContext = new RuntimeContext<WeatherRuntimeContext>();\nruntimeContext.set(\"temperature-scale\", \"celsius\");\n\nconst response = await agent.generate(\"What's the weather like today?\", {\n  runtimeContext,\n});\n\nconsole.log(response.text);\n```\n\n## Using with REST API\n\nHere's how to dynamically set temperature units based on a user's location using the Cloudflare `CF-IPCountry` header:\n\n```typescript filename=\"src/index.ts\"\nimport { Mastra } from \"@mastra/core\";\nimport { RuntimeContext } from \"@mastra/core/di\";\nimport { agent as weatherAgent } from \"./agents/weather\";\n\n// Define RuntimeContext type with clear, descriptive types\ntype WeatherRuntimeContext = {\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\";\n};\n\nexport const mastra = new Mastra({\n  agents: {\n    weather: weatherAgent,\n  },\n  server: {\n    middleware: [\n      async (c, next) => {\n        const country = c.req.header(\"CF-IPCountry\");\n        const runtimeContext = c.get<WeatherRuntimeContext>(\"runtimeContext\");\n\n        // Set temperature scale based on country\n        runtimeContext.set(\n          \"temperature-scale\",\n          country === \"US\" ? \"fahrenheit\" : \"celsius\",\n        );\n\n        await next(); // Don't forget to call next()\n      },\n    ],\n  },\n});\n```\n\n## Creating Tools with Variables\n\nTools can access runtimeContext variables and must conform to the agent's runtimeContext type:\n\n```typescript\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const weatherTool = createTool({\n  id: \"getWeather\",\n  description: \"Get the current weather for a location\",\n  inputSchema: z.object({\n    location: z.string().describe(\"The location to get weather for\"),\n  }),\n  execute: async ({ context, runtimeContext }) => {\n    // Type-safe access to runtimeContext variables\n    const temperatureUnit = runtimeContext.get(\"temperature-scale\");\n\n    const weather = await fetchWeather(context.location, {\n      temperatureUnit,\n    });\n\n    return { result: weather };\n  },\n});\n\nasync function fetchWeather(\n  location: string,\n  { temperatureUnit }: { temperatureUnit: \"celsius\" | \"fahrenheit\" },\n): Promise<WeatherResponse> {\n  // Implementation of weather API call\n  const response = await weatherApi.fetch(location, temperatureUnit);\n\n  return {\n    location,\n    temperature: \"72°F\",\n    conditions: \"Sunny\",\n    unit: temperatureUnit,\n  };\n}\n```\n\n\n---\ntitle: \"Using Tools with Agents | Agents | Mastra Docs\"\ndescription: Learn how to create tools, add them to Mastra agents, and integrate tools from MCP servers.\n---\n\n# Using Tools with Agents\n[EN] Source: https://mastra.ai/en/docs/agents/using-tools-and-mcp\n\nTools are typed functions that can be executed by agents or workflows. Each tool has a schema defining its inputs, an executor function implementing its logic, and optional access to configured integrations.\n\n## Creating Tools\n\nHere's a basic example of creating a tool:\n\n```typescript filename=\"src/mastra/tools/weatherInfo.ts\" copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const weatherInfo = createTool({\n  id: \"Get Weather Information\",\n  inputSchema: z.object({\n    city: z.string(),\n  }),\n  description: `Fetches the current weather information for a given city`,\n  execute: async ({ context: { city } }) => {\n    // Tool logic here (e.g., API call)\n    console.log(\"Using tool to fetch weather information for\", city);\n    return { temperature: 20, conditions: \"Sunny\" }; // Example return\n  },\n});\n```\n\nFor details on creating and designing tools, see the [Tools Overview](/docs/tools-mcp/overview).\n\n## Adding Tools to an Agent\n\nTo make a tool available to an agent, add it to the `tools` property in the agent's configuration.\n\n```typescript filename=\"src/mastra/agents/weatherAgent.ts\" {3,11}\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { weatherInfo } from \"../tools/weatherInfo\";\n\nexport const weatherAgent = new Agent({\n  name: \"Weather Agent\",\n  instructions:\n    \"You are a helpful assistant that provides current weather information. When asked about the weather, use the weather information tool to fetch the data.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    weatherInfo,\n  },\n});\n```\n\nWhen you call the agent, it can now decide to use the configured tool based on its instructions and the user's prompt.\n\n## Adding MCP Tools to an Agent\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) provides a standardized way for AI models to discover and interact with external tools and resources. You can connect your Mastra agent to MCP servers to use tools provided by third parties.\n\nFor more details on MCP concepts and how to set up MCP clients and servers, see the [MCP Overview](/docs/tools-mcp/mcp-overview).\n\n### Installation\n\nFirst, install the Mastra MCP package:\n\n```bash npm2yarn copy\nnpm install @mastra/mcp@latest\n```\n\n### Using MCP Tools\n\nBecause there are so many MCP server registries to choose from, we've created an [MCP Registry Registry](https://mastra.ai/mcp-registry-registry) to help you find MCP servers.\n\nOnce you have a server you want to use with your agent, import the Mastra `MCPClient` and add the server configuration.\n\n```typescript filename=\"src/mastra/mcp.ts\" {1,7-16}\nimport { MCPClient } from \"@mastra/mcp\";\n\n// Configure MCPClient to connect to your server(s)\nexport const mcp = new MCPClient({\n  servers: {\n    filesystem: {\n      command: \"npx\",\n      args: [\n        \"-y\",\n        \"@modelcontextprotocol/server-filesystem\",\n        \"/Users/username/Downloads\",\n      ],\n    },\n  },\n});\n```\n\nThen connect your agent to the server tools:\n\n```typescript filename=\"src/mastra/agents/mcpAgent.ts\" {7}\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { mcp } from \"../mcp\";\n\n// Create an agent and add tools from the MCP client\nconst agent = new Agent({\n  name: \"Agent with MCP Tools\",\n  instructions: \"You can use tools from connected MCP servers.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: await mcp.getTools(),\n});\n```\n\nFor more details on configuring `MCPClient` and the difference between static and dynamic MCP server configurations, see the [MCP Overview](/docs/tools-mcp/mcp-overview).\n\n## Accessing MCP Resources\n\nIn addition to tools, MCP servers can also expose resources - data or content that can be retrieved and used in your application.\n\n```typescript filename=\"src/mastra/resources.ts\" {3-8}\nimport { mcp } from \"./mcp\";\n\n// Get resources from all connected MCP servers\nconst resources = await mcp.getResources();\n\n// Access resources from a specific server\nif (resources.filesystem) {\n  const resource = resources.filesystem.find(\n    (r) => r.uri === \"filesystem://Downloads\",\n  );\n  console.log(`Resource: ${resource?.name}`);\n}\n```\n\nEach resource has a URI, name, description, and MIME type. The `getResources()` method handles errors gracefully - if a server fails or doesn't support resources, it will be omitted from the results.\n\n## Accessing MCP Prompts\n\nMCP servers can also expose prompts, which represent structured message templates or conversational context for agents.\n\n### Listing Prompts\n\n```typescript filename=\"src/mastra/prompts.ts\"\nimport { mcp } from \"./mcp\";\n\n// Get prompts from all connected MCP servers\nconst prompts = await mcp.prompts.list();\n\n// Access prompts from a specific server\nif (prompts.weather) {\n  const prompt = prompts.weather.find(\n    (p) => p.name === \"current\"\n  );\n  console.log(`Prompt: ${prompt?.name}`);\n}\n```\n\nEach prompt has a name, description, and (optional) version.\n\n### Retrieving a Prompt and Its Messages\n\n```typescript filename=\"src/mastra/prompts.ts\"\nconst { prompt, messages } = await mcp.prompts.get({ serverName: \"weather\", name: \"current\" });\nconsole.log(prompt);    // { name: \"current\", version: \"v1\", ... }\nconsole.log(messages);  // [ { role: \"assistant\", content: { type: \"text\", text: \"...\" } }, ... ]\n```\n\n## Exposing Agents as Tools via MCPServer\n\nIn addition to using tools from MCP servers, your Mastra Agents themselves can be exposed as tools to any MCP-compatible client using Mastra's `MCPServer`.\n\nWhen an `Agent` instance is provided to an `MCPServer` configuration:\n\n- It is automatically converted into a callable tool.\n- The tool is named `ask_<agentKey>`, where `<agentKey>` is the identifier you used when adding the agent to the `MCPServer`'s `agents` configuration.\n- The agent's `description` property (which must be a non-empty string) is used to generate the tool's description.\n\nThis allows other AI models or MCP clients to interact with your Mastra Agents as if they were standard tools, typically by \"asking\" them a question.\n\n**Example `MCPServer` Configuration with an Agent:**\n\n```typescript filename=\"src/mastra/mcp.ts\"\nimport { Agent } from \"@mastra/core/agent\";\nimport { MCPServer } from \"@mastra/mcp\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { weatherInfo } from \"../tools/weatherInfo\";\nimport { generalHelper } from \"../agents/generalHelper\";\n\nconst server = new MCPServer({\n  name: \"My Custom Server with Agent-Tool\",\n  version: \"1.0.0\",\n  tools: {\n    weatherInfo,\n  },\n  agents: { generalHelper }, // Exposes 'ask_generalHelper' tool\n});\n```\n\nFor an agent to be successfully converted into a tool by `MCPServer`, its `description` property must be set to a non-empty string in its constructor configuration. If the description is missing or empty, `MCPServer` will throw an error during initialization.\n\nFor more details on setting up and configuring `MCPServer`, refer to the [MCPServer reference documentation](/reference/tools/mcp-server).\n\n\n---\ntitle: \"Using with Vercel AI SDK\"\ndescription: \"Learn how Mastra leverages the Vercel AI SDK library and how you can leverage it further with Mastra\"\n---\n\nimport Image from \"next/image\";\n\n# Using with Vercel AI SDK\n[EN] Source: https://mastra.ai/en/docs/frameworks/agentic-uis/ai-sdk\n\nMastra leverages AI SDK's model routing (a unified interface on top of OpenAI, Anthropic, etc), structured output, and tool calling.\n\nWe explain this in greater detail in [this blog post](https://mastra.ai/blog/using-ai-sdk-with-mastra)\n\n## Mastra + AI SDK\n\nMastra acts as a layer on top of AI SDK to help teams productionize their proof-of-concepts quickly and easily.\n\n<Image\n  src=\"/image/mastra-ai-sdk.png\"\n  alt=\"Agent interaction trace showing spans, LLM calls, and tool executions\"\n  style={{ maxWidth: \"800px\", width: \"100%\", margin: \"8px 0\" }}\n  className=\"nextra-image rounded-md py-8\"\n  data-zoom\n  width={800}\n  height={400}\n/>\n\n## Model routing\n\nWhen creating agents in Mastra, you can specify any AI SDK-supported model:\n\n```typescript\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n  name: \"WeatherAgent\",\n  instructions: \"Instructions for the agent...\",\n  model: openai(\"gpt-4-turbo\"), // Model comes directly from AI SDK\n});\n\nconst result = await agent.generate(\"What is the weather like?\");\n```\n\n## AI SDK Hooks\n\nMastra is compatible with AI SDK's hooks for seamless frontend integration:\n\n### useChat\n\nThe `useChat` hook enables real-time chat interactions in your frontend application\n\n- Works with agent data streams i.e. `.toDataStreamResponse()`\n- The useChat `api` defaults to `/api/chat`\n- Works with the Mastra REST API agent stream endpoint `{MASTRA_BASE_URL}/agents/:agentId/stream` for data streams,\n  i.e. no structured output is defined.\n\n```typescript filename=\"app/api/chat/route.ts\" copy\nimport { mastra } from \"@/src/mastra\";\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n  const myAgent = mastra.getAgent(\"weatherAgent\");\n  const stream = await myAgent.stream(messages);\n\n  return stream.toDataStreamResponse();\n}\n```\n\n```typescript copy\nimport { useChat } from '@ai-sdk/react';\n\nexport function ChatComponent() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/path-to-your-agent-stream-api-endpoint'\n  });\n\n  return (\n    <div>\n      {messages.map(m => (\n        <div key={m.id}>\n          {m.role}: {m.content}\n        </div>\n      ))}\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          placeholder=\"Say something...\"\n        />\n      </form>\n    </div>\n  );\n}\n```\n\n> **Gotcha**: When using `useChat` with agent memory functionality, make sure to check out the [Agent Memory section](/docs/agents/agent-memory#usechat) for important implementation details.\n\n### useCompletion\n\nFor single-turn completions, use the `useCompletion` hook:\n\n- Works with agent data streams i.e. `.toDataStreamResponse()`\n- The useCompletion `api` defaults to `/api/completion`\n- Works with the Mastra REST API agent stream endpoint `{MASTRA_BASE_URL}/agents/:agentId/stream` for data streams,\n  i.e. no structured output is defined.\n\n```typescript filename=\"app/api/completion/route.ts\" copy\nimport { mastra } from \"@/src/mastra\";\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n  const myAgent = mastra.getAgent(\"weatherAgent\");\n  const stream = await myAgent.stream([{ role: \"user\", content: prompt }]);\n\n  return stream.toDataStreamResponse();\n}\n```\n\n```typescript\nimport { useCompletion } from \"@ai-sdk/react\";\n\nexport function CompletionComponent() {\n  const {\n    completion,\n    input,\n    handleInputChange,\n    handleSubmit,\n  } = useCompletion({\n  api: '/path-to-your-agent-stream-api-endpoint'\n  });\n\n  return (\n    <div>\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          placeholder=\"Enter a prompt...\"\n        />\n      </form>\n      <p>Completion result: {completion}</p>\n    </div>\n  );\n}\n```\n\n### useObject\n\nFor consuming text streams that represent JSON objects and parsing them into a complete object based on a schema.\n\n- Works with agent text streams i.e. `.toTextStreamResponse()`\n- Works with the Mastra REST API agent stream endpoint `{MASTRA_BASE_URL}/agents/:agentId/stream` for text streams,\n  i.e. structured output is defined.\n\n```typescript filename=\"app/api/use-object/route.ts\" copy\nimport { mastra } from \"@/src/mastra\";\n\nexport async function POST(req: Request) {\n  const body = await req.json();\n  const myAgent = mastra.getAgent(\"weatherAgent\");\n  const stream = await myAgent.stream(body, {\n    output: z.object({\n      weather: z.string(),\n    }),\n  });\n\n  return stream.toTextStreamResponse();\n}\n```\n\n```typescript\nimport { experimental_useObject as useObject } from '@ai-sdk/react';\n\nexport default function Page() {\n  const { object, submit } = useObject({\n    api: '/api/use-object',\n    schema: z.object({\n      weather: z.string(),\n    }),\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('example input')}>Generate</button>\n      {object?.weather && <p>{object.weather}</p>}\n    </div>\n  );\n}\n```\n\n### With additional data / RuntimeContext\n\nYou can send additional data via the UI hooks that can be leveraged in Mastra as RuntimeContext using the `sendExtraMessageFields` option.\n\n#### Frontend: Using sendExtraMessageFields\n\n```typescript\nimport { useChat } from '@ai-sdk/react';\n\nexport function ChatComponent() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat({\n    api: '/api/chat',\n    sendExtraMessageFields: true, // Enable sending extra fields\n  });\n\n  const handleFormSubmit = (e: React.FormEvent) => {\n        e.preventDefault();        \n        handleSubmit(e,{\n            // Add context data to the message\n            data: {\n                userId: 'user123',\n                preferences: { language: 'en', temperature: 'celsius' },\n            },\n        });\n  };\n\n  return (\n    <form onSubmit={handleFormSubmit}>\n      <input value={input} onChange={handleInputChange} />\n    </form>\n  );\n}\n```\n\n#### Backend: Handling in API Route\n\n```typescript filename=\"app/api/chat/route.ts\" copy\nimport { mastra } from \"@/src/mastra\";\nimport { RuntimeContext } from \"@mastra/core/runtime-context\";\n\nexport async function POST(req: Request) {\n  const { messages, data } = await req.json();\n  const myAgent = mastra.getAgent(\"weatherAgent\");\n  \n  const runtimeContext = new RuntimeContext();\n  \n  if (data) {\n    Object.entries(data).forEach(([key, value]) => {\n      runtimeContext.set(key, value);\n    });\n  }\n  \n  const stream = await myAgent.stream(messages, { runtimeContext });\n  return stream.toDataStreamResponse();\n}\n```\n\n#### Alternative: Server Middleware\n\nYou can also handle this at the server middleware level:\n\n```typescript filename=\"src/mastra/index.ts\" copy\nimport { Mastra } from \"@mastra/core\";\n\nexport const mastra = new Mastra({\n  agents: { weatherAgent },\n  server: {\n    middleware: [\n      async (c, next) => {\n        const runtimeContext = c.get(\"runtimeContext\");\n        \n        if (c.req.method === 'POST') {\n          try {\n            // Clone the request since reading the body can only be done once\n            const clonedReq = c.req.raw.clone();\n            const body = await clonedReq.json();\n            \n            \n            if (body?.data) {\n              Object.entries(body.data).forEach(([key, value]) => {\n                runtimeContext.set(key, value);\n              });\n            }\n          } catch {\n            // Continue without additional data\n          }\n        }\n        \n        await next();\n      },\n    ],\n  },\n});\n```\n\nYou can then access this data in your tools via the `runtimeContext` parameter. See the [Runtime Context documentation](/docs/agents/runtime-variables) for more details.\n\n## Tool Calling\n\n### AI SDK Tool Format\n\nMastra supports tools created using the AI SDK format, so you can use\nthem directly with Mastra agents. See our tools doc on [Vercel AI SDK Tool Format\n](/docs/agents/adding-tools#vercel-ai-sdk-tool-format) for more details.\n\n### Client-side tool calling\n\nMastra leverages AI SDK's tool calling, so what applies in AI SDK applies here still.\n[Agent Tools](/docs/agents/adding-tools) in Mastra are 100% percent compatible with AI SDK tools.\n\nMastra tools also expose an optional `execute` async function. It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.\n\nOne way to then leverage client-side tool calling is to use the `@ai-sdk/react` `useChat` hook's `onToolCall` property for\nclient-side tool execution\n\n## Custom DataStream\n\nIn certain scenarios you need to write custom data, message annotations to an agent's dataStream.\nThis can be useful for:\n\n- Streaming additional data to the client\n- Passing progress info back to the client in real time\n\nMastra integrates well with AI SDK to make this possible\n\n### CreateDataStream\n\nThe `createDataStream` function allows you to stream additional data to the client\n\n```typescript copy\nimport { createDataStream } from \"ai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const weatherAgent = new Agent({\n  name: \"Weather Agent\",\n  instructions: `\n          You are a helpful weather assistant that provides accurate weather information.\n\n          Your primary function is to help users get weather details for specific locations. When responding:\n          - Always ask for a location if none is provided\n          - If the location name isn't in English, please translate it\n          - If giving a location with multiple parts (e.g. \"New York, NY\"), use the most relevant part (e.g. \"New York\")\n          - Include relevant details like humidity, wind conditions, and precipitation\n          - Keep responses concise but informative\n\n          Use the weatherTool to fetch current weather data.\n    `,\n  model: openai(\"gpt-4o\"),\n  tools: { weatherTool },\n});\n\nconst stream = createDataStream({\n  async execute(dataStream) {\n    // Write data\n    dataStream.writeData({ value: \"Hello\" });\n\n    // Write annotation\n    dataStream.writeMessageAnnotation({ type: \"status\", value: \"processing\" });\n\n    //mastra agent stream\n    const agentStream = await weatherAgent.stream(\"What is the weather\");\n\n    // Merge agent stream\n    agentStream.mergeIntoDataStream(dataStream);\n  },\n  onError: (error) => `Custom error: ${error.message}`,\n});\n```\n\n### CreateDataStreamResponse\n\nThe `createDataStreamResponse` function creates a Response object that streams data to the client\n\n```typescript filename=\"app/api/chat/route.ts\" copy\nimport { mastra } from \"@/src/mastra\";\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n  const myAgent = mastra.getAgent(\"weatherAgent\");\n  //mastra agent stream\n  const agentStream = await myAgent.stream(messages);\n\n  const response = createDataStreamResponse({\n    status: 200,\n    statusText: \"OK\",\n    headers: {\n      \"Custom-Header\": \"value\",\n    },\n    async execute(dataStream) {\n      // Write data\n      dataStream.writeData({ value: \"Hello\" });\n\n      // Write annotation\n      dataStream.writeMessageAnnotation({\n        type: \"status\",\n        value: \"processing\",\n      });\n\n      // Merge agent stream\n      agentStream.mergeIntoDataStream(dataStream);\n    },\n    onError: (error) => `Custom error: ${error.message}`,\n  });\n\n  return response;\n}\n```\n\n\n---\ntitle: Using with Assistant UI\ndescription: \"Learn how to integrate Assistant UI with Mastra\"\n---\n\nimport { Callout, FileTree, Steps } from 'nextra/components'\n\n# Using with Assistant UI\n[EN] Source: https://mastra.ai/en/docs/frameworks/agentic-uis/assistant-ui\n\n[Assistant UI](https://assistant-ui.com) is the TypeScript/React library for AI Chat.\nBuilt on shadcn/ui and Tailwind CSS, it enables developers to create beautiful, enterprise-grade chat experiences in minutes.\n\n<Callout type=\"info\">\nFor a full-stack integration approach where Mastra runs directly in your Next.js API routes, see the [Full-Stack Integration Guide](https://www.assistant-ui.com/docs/runtimes/mastra/full-stack-integration) on Assistant UI's documentation site.\n</Callout>\n\n## Integration Guide\n\nRun Mastra as a standalone server and connect your Next.js frontend (with Assistant UI) to its API endpoints.\n\n<Steps>\n### Create Standalone Mastra Server\n\nSet up your directory structure. A possible directory structure could look like this:\n\n<FileTree>\n    <FileTree.Folder name=\"project-root\" defaultOpen>\n        <FileTree.Folder name=\"mastra-server\" defaultOpen>\n            <FileTree.Folder name=\"src\">\n                <FileTree.Folder name=\"mastra\" />\n            </FileTree.Folder>\n            <FileTree.File name=\"package.json\" />\n        </FileTree.Folder>\n        <FileTree.Folder name=\"nextjs-frontend\">\n            <FileTree.File name=\"package.json\" />\n        </FileTree.Folder>\n    </FileTree.Folder>\n</FileTree>\n\nBootstrap your Mastra server:\n\n```bash copy\nnpx create-mastra@latest\n```\n\nThis command will launch an interactive wizard to help you scaffold a new Mastra project, including prompting you for a project name and setting up basic configurations.\nFollow the prompts to create your server project.\n\nYou now have a basic Mastra server project ready. You should have the following files and folders:\n\n<FileTree>\n    <FileTree.Folder name=\"src\" defaultOpen>\n      <FileTree.Folder name=\"mastra\" defaultOpen>\n        <FileTree.File name=\"index.ts\" />\n        <FileTree.Folder name=\"agents\" defaultOpen>\n          <FileTree.File name=\"weather-agent.ts\" />\n        </FileTree.Folder>\n        <FileTree.Folder name=\"tools\" defaultOpen>\n          <FileTree.File name=\"weather-tool.ts\" />\n        </FileTree.Folder>\n        <FileTree.Folder name=\"workflows\" defaultOpen>\n          <FileTree.File name=\"weather-workflow.ts\" />\n        </FileTree.Folder>\n      </FileTree.Folder>\n    </FileTree.Folder>\n</FileTree>\n\n<Callout>\nEnsure that you have set the appropriate environment variables for your LLM provider in the `.env` file.\n</Callout>\n\n### Compatibility Fix\n\nCurrently, to ensure proper compatibility between Mastra and Assistant UI, you need to setup server middleware. Update your `/mastra/index.ts` file with the following configuration:\n\n```typescript showLineNumbers copy filename=\"src/mastra/index.ts\"\nexport const mastra = new Mastra({\n  //mastra server middleware\n  server:{\n  middleware: [{\n    path: '/api/agents/*/stream',\n    handler: async (c,next)=>{\n    \n      const body = await c.req.json();\n  \n      if ('state' in body && body.state == null) {\n        delete body.state;\n        delete body.tools;\n      }\n  \n       c.req.json = async() => body;\n  \n      return next()\n    }\n  }]\n },\n});\n```\n\nThis middleware ensures that when Assistant UI sends a request with `state: null` and `tools: {}` in the request body, we remove those properties to make the request work properly with Mastra.\n\n<Callout type=\"info\">\nThe `state: null` property can cause errors like `Cannot use 'in' operator to search for 'input' in null` in Mastra. Additionally, passing `tools: {}` overrides Mastra's built-in tools. Mastra only supports `clientTools` via the Mastra client SDK from the client side. For more information about client tools, see the [Client Tools documentation](/reference/client-js/agents#client-tools).\n</Callout>\n\n### Run the Mastra Server\n\nRun the Mastra server using the following command:\n\n```bash copy\nnpm run dev\n```\n\nBy default, the Mastra server will run on `http://localhost:5000`. Your `weatherAgent` should now be accessible via a POST request endpoint, typically `http://localhost:5000/api/agents/weatherAgent/stream`. Keep this server running for the next steps where we'll set up the Assistant UI frontend to connect to it.\n\n### Initialize Assistant UI\n\nCreate a new `assistant-ui` project with the following command.\n\n```bash copy\nnpx assistant-ui@latest create\n```\n\n<Callout>For detailed setup instructions, including adding API keys, basic configuration, and manual setup steps, please refer to [assistant-ui's official documentation](https://assistant-ui.com/docs).</Callout>\n\n### Configure Frontend API Endpoint\n\nThe default Assistant UI setup configures the chat runtime to use a local API route (`/api/chat`) within the Next.js project. Since our Mastra agent is running on a separate server, we need to update the frontend to point to that server's endpoint.\n\nFind the `useChatRuntime` hook in the `assistant-ui` project, typically at `app/assistant.tsx` and change the `api` property to the full URL of your Mastra agent's stream endpoint:\n\n```typescript showLineNumbers copy filename=\"app/assistant.tsx\" {2}\nconst runtime = useChatRuntime({\n    api: \"http://localhost:5000/api/agents/weatherAgent/stream\",\n});\n```\n\nNow, the Assistant UI frontend will send chat requests directly to your running Mastra server.\n\n### Run the Application\n\nYou're ready to connect the pieces! Make sure both the Mastra server and the Assistant UI frontend are running. Start the Next.js development server:\n\n```bash copy\nnpm run dev\n```\n\nYou should now be able to chat with your agent in the browser.\n\n</Steps>\n\nCongratulations! You have successfully integrated Mastra with Assistant UI using a separate server approach. Your Assistant UI frontend now communicates with a standalone Mastra agent server.\n\n\n---\ntitle: \"Using with CopilotKit\"\ndescription: \"Learn how Mastra leverages the CopilotKit's AGUI library and how you can leverage it to build user experiences\"\n---\n\nimport { Tabs } from \"nextra/components\";\nimport Image from \"next/image\";\n\n# Using with CopilotKit in React\n[EN] Source: https://mastra.ai/en/docs/frameworks/agentic-uis/copilotkit\n\nCopilotKit provides React components to quickly integrate customizable AI copilots into your application.\nCombined with Mastra, you can build sophisticated AI apps featuring bidirectional state synchronization and interactive UIs.\n\n## Create a Mastra Project\n{/*\nLLM CONTEXT:\nThis Tabs component shows commands for creating a new Mastra project using different package managers.\nEach tab displays the command for that specific package manager to create a Mastra project.\nThis is the first step in setting up Mastra with CopilotKit for building AI copilot applications.\nAll commands create the same Mastra project but use different package manager syntax.\n*/}\n\n<Tabs items={[\"npx\", \"npm\", \"yarn\", \"pnpm\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    npx create-mastra@latest\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    npm create mastra\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    yarn create mastra\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    pnpm create mastra\n    ```\n  </Tabs.Tab>\n</Tabs>\n\n\nSelect the agent example when scaffolding your project. This will give you a weather agent.\n\nFor detailed setup instructions, see the [installation guide](/docs/getting-started/installation).\n\n## Basic Setup\n\nIntegrating Mastra with CopilotKit involves two main steps: setting up the backend runtime and configuring your frontend components.\n{/*\nLLM CONTEXT: This Tabs component shows installation commands for the CopilotKit runtime package.\nEach tab displays the installation command for that specific package manager.\nThis installs the core CopilotKit runtime needed for backend integration with Mastra.\nAll commands install the same @copilotkit/runtime package but use different package manager syntax.\n*/}\n\n<Tabs items={[\"npm\", \"yarn\", \"pnpm\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    npm install @copilotkit/runtime\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    yarn add @copilotkit/runtime\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    pnpm add @copilotkit/runtime\n    ```\n  </Tabs.Tab>\n</Tabs>\n\n## Set up the runtime\n\nYou can leverage Mastra's custom API routes to add CopilotKit's runtime to your Mastra server.\n\nThe current version of the integration leverages `MastraClient` to format Mastra agents into the AGUI format of CopilotKit.\n\n{/*\nLLM CONTEXT: This Tabs component shows installation commands for the Mastra AGUI package.\nEach tab displays the installation command for that specific package manager.\nThis installs the alpha version of @ag-ui/mastra which provides CopilotKit integration capabilities.\nAll commands install the same @ag-ui/mastra package but use different package manager syntax.\n*/}\n\n<Tabs items={[\"npm\", \"yarn\", \"pnpm\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    npm install @ag-ui/mastra\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    yarn add @ag-ui/mastra\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    pnpm add @ag-ui/mastra\n    ```\n  </Tabs.Tab>\n</Tabs>\n\nNext, let's update the Mastra instance with a custom API route for CopilotKit.\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LibSQLStore } from \"@mastra/libsql\";\nimport { CopilotRuntime, copilotRuntimeNodeHttpEndpoint, ExperimentalEmptyAdapter } from \"@copilotkit/runtime\";\nimport { registerCopilotKit } from \"@ag-ui/mastra\";\nimport { weatherAgent } from \"./agents/weather-agent\";\n\nconst serviceAdapter = new ExperimentalEmptyAdapter();\n\nexport const mastra = new Mastra({\n  agents: { weatherAgent },\n  storage: new LibSQLStore({\n    // stores telemetry, evals, ... into memory storage,\n    // if you need to persist, change to file:../mastra.db\n    url: \":memory:\"\n  }),\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\"\n  }),\n  server: {\n    // We will be calling this from a Vite App. Allow CORS\n    cors: {\n      origin: \"*\",\n      allowMethods: [\"*\"],\n      allowHeaders: [\"*\"]\n    },\n    apiRoutes: [\n      registerCopilotKit({\n        path: \"/copilotkit\",\n        resourceId: \"weatherAgent\",\n        setContext: (c, runtimeContext) => {\n          // Add whatever you need to the runtimeContext\n          runtimeContext.set(\"user-id\", c.req.header(\"X-User-ID\"));\n          runtimeContext.set(\"temperature-scale\", \"celsius\");\n        }\n      })\n    ]\n  }\n});\n```\n\nWith this setup you now have CopilotKit running on your Mastra server. You can start your Mastra server with `mastra dev`.\n\n## Set up the UI\n\nInstall CopilotKit's React components:\n{/*\nLLM CONTEXT: This Tabs component shows installation commands for CopilotKit's React UI components.\nEach tab displays the installation command for that specific package manager.\nThis installs the React components needed for the frontend CopilotKit integration.\nAll commands install the same @copilotkit/react-core and @copilotkit/react-ui packages but use different package manager syntax.\n*/}\n\n<Tabs items={[\"npm\", \"yarn\", \"pnpm\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    npm install @copilotkit/react-core @copilotkit/react-ui\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    yarn add @copilotkit/react-core @copilotkit/react-ui\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    pnpm add @copilotkit/react-core @copilotkit/react-ui\n    ```\n  </Tabs.Tab>\n</Tabs>\n\nNext, add CopilotKit's React components to your frontend.\n\n```jsx copy\nimport { CopilotChat } from \"@copilotkit/react-ui\";\nimport { CopilotKit } from \"@copilotkit/react-core\";\nimport \"@copilotkit/react-ui/styles.css\";\n\nexport function CopilotKitComponent() {\n  return (\n    <CopilotKit\n      runtimeUrl=\"http://localhost:5000/copilotkit\"\n      agent=\"weatherAgent\"\n    >\n      <CopilotChat\n        labels={{\n          title: \"Your Assistant\",\n          initial: \"Hi! 👋 How can I assist you today?\",\n        }}\n      />\n    </CopilotKit>\n  );\n}\n```\n\nRender the component and start building the future!\n\n<br />\n\n<Image\n  className=\"rounded-lg\"\n  src=\"/image/copilotkit/cpkoutput.jpg\"\n  alt=\"CopilotKit output\"\n  width={700}\n  height={700}\n/>\n\n## Using with other frameworks (NextJS)\n\nYou can still leverage AGUI without going through Mastra Server.\n\n```typescript copy\n// import your mastra instance from dir\nimport { mastra } from \"../path/to/mastra\";\nimport {\n  CopilotRuntime,\n  ExperimentalEmptyAdapter,\n  copilotRuntimeNextJSAppRouterEndpoint,\n} from \"@copilotkit/runtime\";\nimport { NextRequest } from \"next/server\";\nimport { MastraAgent } from \"@ag-ui/mastra\";\n\nexport const POST = async (req: NextRequest) => {\n  // Clone the request before reading the body\n  const clonedReq = req.clone();\n  const body = await clonedReq.json();\n  const resourceId = body.resourceId || \"TEST\";\n\n  const mastraAgents = MastraAgent.getLocalAgents({\n    resourceId,\n    mastra,\n    runtimeContext,\n  });\n\n  const runtime = new CopilotRuntime({\n    agents: mastraAgents,\n  });\n\n  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({\n    runtime,\n    serviceAdapter: new ExperimentalEmptyAdapter(),\n    endpoint: \"/api/copilotkit\",\n  });\n\n  // Use the original request for handleRequest\n  return handleRequest(req);\n};\n```\n\n## Using with Mastra Client SDK\n\n```typescript copy\nimport { MastraClient } from \"@mastra/client-js\";\nimport {\n  CopilotRuntime,\n  ExperimentalEmptyAdapter,\n  copilotRuntimeNextJSAppRouterEndpoint,\n} from \"@copilotkit/runtime\";\nimport { NextRequest } from \"next/server\";\nimport { MastraAgent } from \"@ag-ui/mastra\";\n\nexport const POST = async (req: NextRequest) => {\n  // Clone the request before reading the body\n  const clonedReq = req.clone();\n  const body = await clonedReq.json();\n  const resourceId = body.resourceId || \"TEST\";\n\n  const baseUrl = process.env.MASTRA_BASE_URL || \"http://localhost:5000\";\n  const mastraClient = new MastraClient({\n    baseUrl,\n  });\n\n  const mastraAgents = await MastraAgent.getRemoteAgents({ mastraClient });\n\n  const runtime = new CopilotRuntime({\n    agents: mastraAgents,\n  });\n\n  const { handleRequest } = copilotRuntimeNextJSAppRouterEndpoint({\n    runtime,\n    serviceAdapter: new ExperimentalEmptyAdapter(),\n    endpoint: \"/api/copilotkit\",\n  });\n\n  // Use the original request for handleRequest\n  return handleRequest(req);\n};\n```\n\n\n## Using Typed Runtime Context\n\nFor better type safety, you can specify the type of your runtime context:\n\n```typescript filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LibSQLStore } from \"@mastra/libsql\";\nimport { registerCopilotKit } from \"@ag-ui/mastra\";\nimport { weatherAgent } from \"./agents\";\n\n// Define your runtime context type\ntype WeatherRuntimeContext = {\n  \"user-id\": string;\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\";\n  \"api-key\": string;\n};\n\nexport const mastra = new Mastra({\n  agents: { weatherAgent },\n  storage: new LibSQLStore({\n    url: \":memory:\",\n  }),\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\",\n  }),\n  server: {\n    cors: {\n      origin: \"*\",\n      allowMethods: [\"*\"],\n      allowHeaders: [\"*\"],\n    },\n    apiRoutes: [\n      registerCopilotKit<WeatherRuntimeContext>({\n        path: \"/copilotkit\",\n        resourceId: \"weatherAgent\",\n        setContext: (c, runtimeContext) => {\n          // TypeScript will enforce the correct types here\n          runtimeContext.set(\"user-id\", c.req.header(\"X-User-ID\") || \"anonymous\");\n          runtimeContext.set(\"temperature-scale\", \"celsius\"); // Only \"celsius\" | \"fahrenheit\" allowed\n          runtimeContext.set(\"api-key\", process.env.WEATHER_API_KEY || \"\");\n\n          // This would cause a TypeScript error:\n          // runtimeContext.set(\"invalid-key\", \"value\"); // ❌ Error: invalid key\n          // runtimeContext.set(\"temperature-scale\", \"kelvin\"); // ❌ Error: invalid value\n        }\n      }),\n    ],\n  },\n});\n```\n\n## Further Reading\n\n- [CopilotKit Documentation](https://docs.copilotkit.ai)\n- [React Hooks with CopilotKit](https://docs.copilotkit.ai/reference/hooks/useCoAgent)\n\n\n---\ntitle: \"Using with OpenRouter\"\ndescription: \"Learn how to integrate OpenRouter with Mastra\"\n---\n\nimport { Steps } from 'nextra/components'\n\n# Use OpenRouter with Mastra\n[EN] Source: https://mastra.ai/en/docs/frameworks/agentic-uis/openrouter\n\nIntegrate OpenRouter with Mastra to leverage the numerous models available on OpenRouter.\n\n<Steps>\n## Initialize a Mastra Project\n\nThe simplest way to get started with Mastra is to use the `mastra` CLI to initialize a new project:\n\n```bash copy\nnpx create-mastra@latest\n```\n\nYou'll be guided through prompts to set up your project. For this example, select:\n- Name your project: my-mastra-openrouter-app\n- Components: Agents (recommended)\n- For default provider, select OpenAI (recommended) - we'll configure OpenRouter manually later\n- Optionally include example code\n\n## Configure OpenRouter\n\nAfter creating your project with `create-mastra`, you'll find a `.env` file in your project root.\nSince we selected OpenAI during setup, we'll configure OpenRouter manually: \n\n```bash filename=\".env\" copy\nOPENROUTER_API_KEY=\n```\n\nWe remove the `@ai-sdk/openai` package from the project:\n\n```bash copy\nnpm uninstall @ai-sdk/openai\n```\n\nThen, we install the `@openrouter/ai-sdk-provider` package:\n\n```bash copy\nnpm install @openrouter/ai-sdk-provider\n```\n\n## Configure your Agent to use OpenRouter\n\nWe will now configure our agent to use OpenRouter.\n\n```typescript filename=\"src/mastra/agents/assistant.ts\" copy showLineNumbers {4-6,11}\nimport { Agent } from \"@mastra/core/agent\";\nimport { createOpenRouter } from \"@openrouter/ai-sdk-provider\";\n\nconst openrouter = createOpenRouter({\n    apiKey: process.env.OPENROUTER_API_KEY,\n})\n\nexport const assistant = new Agent({\n    name: \"assistant\",\n    instructions: \"You are a helpful assistant.\",\n    model: openrouter(\"anthropic/claude-sonnet-4\"),\n})\n```\n\nMake sure to register your agent to the Mastra instance:\n\n```typescript filename=\"src/mastra/index.ts\" copy showLineNumbers {4}\nimport { assistant } from \"./agents/assistant\";\n\nexport const mastra = new Mastra({\n    agents: { assistant }\n})\n```\n\n## Run and Test your Agent\n\n```bash copy\nnpm run dev\n```\n\nThis will start the Mastra development server.\n\nYou can now test your agent by visiting [http://localhost:5000](http://localhost:5000) for the playground or via the Mastra API at [http://localhost:5000/api/agents/assistant/stream](http://localhost:5000/api/agents/assistant/stream).\n\n</Steps>\n\n## Advanced Configuration\n\nFor more control over your OpenRouter requests, you can pass additional configuration options.\n\n### Provider-wide options:\n\nYou can pass provider-wide options to the OpenRouter provider:\n\n```typescript filename=\"src/mastra/agents/assistant.ts\" {6-10} copy showLineNumbers\nimport { Agent } from \"@mastra/core/agent\";\nimport { createOpenRouter } from \"@openrouter/ai-sdk-provider\";\n\nconst openrouter = createOpenRouter({\n    apiKey: process.env.OPENROUTER_API_KEY,\n    extraBody: {\n        reasoning: {\n            max_tokens: 10,\n        }\n    }\n})\n\nexport const assistant = new Agent({\n    name: \"assistant\",\n    instructions: \"You are a helpful assistant.\",\n    model: openrouter(\"anthropic/claude-sonnet-4\"),\n})\n```\n\n### Model-specific options:\n\nYou can pass model-specific options to the OpenRouter provider:\n\n```typescript filename=\"src/mastra/agents/assistant.ts\" {11-17} copy showLineNumbers\nimport { Agent } from \"@mastra/core/agent\";\nimport { createOpenRouter } from \"@openrouter/ai-sdk-provider\";\n\nconst openrouter = createOpenRouter({\n    apiKey: process.env.OPENROUTER_API_KEY,\n})\n\nexport const assistant = new Agent({\n    name: \"assistant\",\n    instructions: \"You are a helpful assistant.\",\n    model: openrouter(\"anthropic/claude-sonnet-4\", {\n        extraBody: {\n            reasoning: {\n                max_tokens: 10,\n            }\n        }\n    }),\n})\n```\n\n### Provider-specific options:\n\nYou can pass provider-specific options to the OpenRouter provider:\n\n```typescript copy showLineNumbers {7-12}\n// Get a response with provider-specific options\nconst response = await assistant.generate([\n  {\n    role: 'system',\n    content:\n      'You are Chef Michel, a culinary expert specializing in ketogenic (keto) diet...',\n    providerOptions: {\n      // Provider-specific options - key can be 'anthropic' or 'openrouter'\n      anthropic: {\n        cacheControl: { type: 'ephemeral' },\n      },\n    },\n  },\n  {\n    role: 'user',\n    content: 'Can you suggest a keto breakfast?',\n  },\n]);\n```\n\n\n# AI SDK v5 (beta) Migration Guide\n[EN] Source: https://mastra.ai/en/docs/frameworks/ai-sdk-v5\n\nThis guide covers Mastra-specific considerations when migrating from AI SDK v4 to v5 beta.\n\nPlease add any feedback or bug reports to the [AI SDK v5 mega issue in Github.](https://github.com/mastra-ai/mastra/issues/5470)\n\n## Official Migration Guide\n\n**Follow the official [AI SDK v5 Migration Guide](https://v5.ai-sdk.dev/docs/migration-guides/migration-guide-5-0)** for all AI SDK core breaking changes, package updates, and API changes.\n\nThis guide covers only the Mastra-specific aspects of the migration.\n\n## Warnings\n\n- **Data compatibility**: New data stored in v5 format will no longer work if you downgrade from the beta\n- **Backup recommendation**: Keep DB backups from before you upgrade to v5 beta\n- **Production use**: Wait for the AI SDK v5 stable release before using in production applications\n- **Prerelease status**: The Mastra `ai-v5` tag is a prerelease version and may have bugs\n\n## Memory Storage\n\nYour existing AI SDK v4 data will run through our internal `MessageList` class which handles converting to/from various message formats.\nThis includes converting from AI SDK v4->v5. This means you don't need to run any DB migrations and your data will be translated on the fly and will just work when you upgrade.\n\n\n## Migration Strategy\n\nMigrating to AI SDK v5 with Mastra involves updating both your **backend** (Mastra server) and **frontend**.\nWe provide a compatibility mode to handle stream format conversion during the transition.\n\n### Backend Upgrade\n\nBump Mastra to the new `ai-v5` prerelease version for all Mastra packages:\n\n```bash npm2yarn copy\nnpm i mastra@ai-v5 @mastra/core@ai-v5 @mastra/memory@ai-v5 [etc]\n```\n\nThen configure your Mastra instance with v4 compatibility so your existing frontend will continue to work:\n\n```typescript\nimport { Mastra } from '@mastra/core';\n\nexport const mastra = new Mastra({\n  agents: { myAgent },\n  aiSdkCompat: 'v4', // <- add this for compatibility\n});\n```\n\n#### Dependencies\n\nYou will need to upgrade all AI SDK dependencies to use the new v5 beta versions in your backend when you bump to the Mastra `ai-v5` prerelease tag.\n\nIn most cases this will only involve bumping your model provider packages. For example: `npm i @ai-sdk/openai@2.0.0-beta.1` - refer to the [AI SDK v5 documentation](https://v5.ai-sdk.dev/docs/migration-guides/migration-guide-5-0) for more info. Some model providers do not yet have V5 versions (Openrouter for example).\n\nAlso note that you need to bump all your Mastra dependencies to the new `ai-v5` tag, and you must upgrade `zod` to the latest version if you have it installed.\n\n#### Using Stream Compatibility Manually\n\nIf you have a frontend that calls Mastra agents in an endpoint, you can wrap the new `response.toUIMessageStreamResponse()` manually.\n\n```ts\nimport { mastra } from \"@/src/mastra\";\nimport { createV4CompatibleResponse } from \"@mastra/core/agent\";\n\nconst myAgent = mastra.getAgent(\"weatherAgent\");\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n  const stream = await myAgent.stream(messages);\n\n  return createV4CompatibleResponse(stream.toUIMessageStreamResponse().body!);\n}\n```\n\n### Using Mastra Playground\n\nCurrently playground is still an AI SDK v4 frontend. For now you need to set `aiSdkCompat: 'v4'` for it to work.\nWe'll handle this automatically for you soon.\n\n### Frontend Upgrade\n\nWhen you're ready, remove the compatibility flag and upgrade your frontend:\n\n1. Remove `aiSdkCompat: 'v4'` from your Mastra configuration\n2. Follow the AI SDK guide on upgrading your frontend dependencies\n3. Update your frontend code for v5 breaking changes\n\n## Discussion and Bug Reports\n\nPlease add any feedback or bug reports to the [AI SDK v5 mega issue in Github.](https://github.com/mastra-ai/mastra/issues/5470)\n\n\n\n---\ntitle: \"Getting started with Mastra and Express | Mastra Guides\"\ndescription: A step-by-step guide to integrating Mastra with an Express backend.\n---\n\nimport { Callout, Steps, Tabs, FileTree } from \"nextra/components\";\n\n# Integrate Mastra in your Express project\n[EN] Source: https://mastra.ai/en/docs/frameworks/servers/express\n\nMastra integrates with Express, making it easy to:\n\n- Build flexible APIs to serve AI-powered features\n- Maintain full control over your server logic and routing\n- Scale your backend independently of your frontend\n\nUse this guide to scaffold and integrate Mastra with your Express project.\n\n<Callout type=\"warning\">\nThis setup is compatible with the following package versions:\n- `express`: 4.x\n- `@types/express`: 4.x\n\nType compatibility in 5.x can be inconsistent while `express` and `@types/express` evolve toward alignment.\n\n</Callout>\n\n<Steps>\n## Install Mastra\n\nInstall the required Mastra packages:\n\n<Tabs items={[\"npm\", \"yarn\", \"pnpm\", \"bun\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    npm install mastra@latest @mastra/core@latest @mastra/libsql@latest\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    yarn add mastra@latest @mastra/core@latest @mastra/libsql@latest\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    pnpm add mastra@latest @mastra/core@latest @mastra/libsql@latest\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    bun add mastra@latest @mastra/core@latest @mastra/libsql@latest\n    ```\n  </Tabs.Tab>\n</Tabs>\n\n## Integrate Mastra\n\nTo integrate Mastra in your project, you have two options:\n\n### 1. Use the One-Liner\n\nRun the following command to quickly scaffold the default Weather agent with sensible defaults:\n\n```bash copy\nnpx mastra@latest init --default\n```\n\n> See [mastra init](/reference/cli/init) for more information.\n\n### 2. Use the Interactive CLI\n\nIf you prefer to customize the setup, run the `init` command and choose from the options when prompted:\n\n```bash copy\nnpx mastra@latest init\n```\n\nAdd the `dev` and `build` scripts to `package.json`:\n\n```json filename=\"package.json\"\n{\n  \"scripts\": {\n    ...\n    \"dev\": \"mastra dev\",\n    \"build\": \"mastra build\"\n  }\n}\n```\n\n> If your project already uses `dev` and `build` scripts, we recommend using: `dev:mastra` and `build:mastra`.\n\n## Initialize TypeScript\n\nCreate a `tsconfig.json` file in your project root with the following configuration:\n\n```json filename=\"tsconfig.json\" copy\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ES2022\",\n    \"moduleResolution\": \"bundler\",\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"outDir\": \"dist\"\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\", \".mastra\"]\n}\n```\n\n> This TypeScript configuration is optimized for Mastra projects, using modern module resolution and strict type checking.\n\n## Set Up API Key\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\n> Each llm provider uses a different env var. See [Model Capabilities](/docs/getting-started/model-capability) for more information.\n\n## Start the Mastra Dev Server\n\nStart the Mastra dev server to expose your agents as REST endpoints:\n\n<Tabs items={[\"npm\", \"CLI\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    npm run dev\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    mastra dev\n    ```\n  </Tabs.Tab>\n</Tabs>\n\n> Once running, your agents are available locally. See [Local Development Environment](/docs/server-db/local-dev-playground) for more information.\n\n## Example Express App\n\nThis example creates an `/api/weather` endpoint that expects a `city` query parameter.\n\n```typescript filename=\"src/server.ts\" showLineNumbers copy\nimport \"dotenv/config\";\nimport express, { Request, Response } from \"express\";\n\nimport { mastra } from \"./mastra\";\n\nconst app = express();\nconst port = process.env.PORT ?? 3000;\n\napp.get(\"/api/weather\", async (req: Request, res: Response) => {\n  const { city } = req.query as { city?: string };\n\n  if (!city) {\n    return res.status(400).send(\"Missing 'city' query parameter\");\n  }\n\n  const agent = mastra.getAgent(\"weatherAgent\");\n\n  try {\n    const result = await agent.generate(`What's the weather like in ${city}?`);\n    res.send(result.text);\n  } catch (error) {\n    console.error(\"Agent error:\", error);\n    res.status(500).send(\"An error occurred while processing your request\");\n  }\n});\n\napp.listen(port, () => {\n  console.log(`Server listening on port ${port}`);\n});\n```\n\nWith the Mastra dev server running, start your Express app separately. For example:\n\n```bash copy\nnpx tsx --watch src/server.ts --watch-dir src\n```\n\nYou can now make a request to the endpoint using one of the following:\n\n<Tabs items={[\"http\", \"curl\"]}>\n  <Tabs.Tab>\n    ```bash copy\n    http://localhost:3000/api/weather?city=London\n    ```\n  </Tabs.Tab>\n  <Tabs.Tab>\n    ```bash copy\n    curl \"http://localhost:3000/api/weather?city=London\"\n    ```\n  </Tabs.Tab>\n</Tabs>\n\nYou should see output similar to the below:\n\n```plaintext\nThe current weather in London is as follows:\n\n- **Temperature:** 12.9°C (Feels like 9.7°C)\n- **Humidity:** 63%\n- **Wind Speed:** 14.7 km/h\n- **Wind Gusts:** 32.4 km/h\n- **Conditions:** Overcast\n\nLet me know if you need more information!\n```\n\n</Steps>\n\n## Next Steps\n\n- [Mastra Client SDK](/docs/deployment/client)\n\n\n---\ntitle: \"Installing Mastra | Getting Started | Mastra Docs\"\ndescription: Guide on installing Mastra and setting up the necessary prerequisites for running it with various LLM providers.\n---\n\nimport { Callout, Steps } from \"nextra/components\";\nimport { Tabs, Tab } from \"@/components/tabs\";\n\n# Install Mastra\n[EN] Source: https://mastra.ai/en/docs/getting-started/installation\n\nTo get started with Mastra, you’ll need access to a large language model (LLM). By default, Mastra is set up to work with [OpenAI](https://platform.openai.com/), so you’ll need an API key to begin.\n\nMastra also supports other LLM providers. For a full list of supported models and setup instructions, see [Model Providers](/docs/getting-started/model-providers).\n\n\n## Prerequisites\n\n- Node.js `v20.0` or higher\n- An API key from a supported [Model Provider](/docs/getting-started/model-providers)\n\n<Steps>\n\n## Install using the `create-mastra` CLI\n\nOur CLI is the fastest way to get started with Mastra. Run the following command to start the interactive setup:\n\n{/*\nLLM CONTEXT: This Tabs component shows different package manager commands for creating a new Mastra project.\nEach tab displays the equivalent command for that specific package manager (npx, npm, yarn, pnpm, bun).\nThis helps users choose their preferred package manager while following the same installation process.\nAll commands achieve the same result - creating a new Mastra project with the interactive setup.\n*/}\n\n<Tabs items={[\"npx\", \"npm\", \"yarn\", \"pnpm\", \"bun\"]}>\n  <Tab>\n    ```bash copy\n    npx create-mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    npm create mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    yarn create mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    pnpm create mastra@latest\n    ```\n  </Tab>\n  <Tab>\n    ```bash copy\n    bun create mastra@latest\n    ```\n  </Tab>\n</Tabs>\n\n\n**Install using CLI flags**\n\nYou can also run the Mastra CLI in non-interactive mode by passing all required flags, for example:\n\n```bash copy\nnpx create-mastra@latest --project-name hello-mastra --example --components tools,agents,workflows --llm openai\n```\n\n> See the [create-mastra](/reference/cli/create-mastra) documentation for a full list of available CLI options.\n\n### Add your API key\n\nAdd your API key to the `.env` file:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n> This example uses OpenAI. Each LLM provider uses a unique name. See [Model Capabilities](/docs/getting-started/model-capability) for more information.\n\nYou can now launch the [Mastra Development Server](/docs/server-db/local-dev-playground) and test your agent using the Mastra Playground.\n\n</Steps>\n\n## Install manually\n\nThe following steps will walk you through installing Mastra manually.\n\n<Steps>\n\n### Create a new project\n\nCreate a new project and change directory:\n\n```bash copy\nmkdir hello-mastra && cd hello-mastra\n```\n\nInitialize a TypeScript project including the `@mastra/core` package:\n\n{/*\nLLM CONTEXT: This Tabs component shows manual installation commands for different package managers.\nEach tab displays the complete setup process for that package manager including project initialization,\ndev dependencies installation, and core Mastra packages installation.\nThis helps users manually set up a Mastra project with their preferred package manager.\n*/}\n\n<Tabs items={[\"npm\", \"pnpm\", \"yarn\", \"bun\"]}>\n\n  <Tab>\n    ```bash copy\n    npm init -y\n\n    npm install typescript tsx @types/node mastra@latest --save-dev\n\n    npm install @mastra/core@latest zod@^3 @ai-sdk/openai\n    ```\n\n  </Tab>\n  <Tab>\n    ```bash copy\n    pnpm init\n\n    pnpm add typescript tsx @types/node mastra@latest --save-dev\n\n    pnpm add @mastra/core@latest zod@^3 @ai-sdk/openai\n    ```\n\n  </Tab>\n  <Tab>\n    ```bash copy\n    yarn init -y\n\n    yarn add typescript tsx @types/node mastra@latest --dev\n\n    yarn add @mastra/core@latest zod@^3 @ai-sdk/openai\n    ```\n\n  </Tab>\n  <Tab>\n    ```bash copy\n    bun init -y\n\n    bun add typescript tsx @types/node mastra@latest --dev\n\n    bun add @mastra/core@latest zod@^3 @ai-sdk/openai\n    ```\n\n  </Tab>\n</Tabs>\n\nAdd the `dev` and `build` scripts to `package.json`:\n\n```json filename=\"package.json\" copy\n{\n  \"scripts\": {\n    // ...\n    \"dev\": \"mastra dev\",\n    \"build\": \"mastra build\"\n  }\n}\n```\n\n### Initialize TypeScript\n\nCreate a `tsconfig.json` file:\n\n```bash copy\ntouch tsconfig.json\n```\n\nAdd the following configuration:\n\n```json filename=\"tsconfig.json\" copy\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"ES2022\",\n    \"moduleResolution\": \"bundler\",\n    \"esModuleInterop\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"strict\": true,\n    \"skipLibCheck\": true,\n    \"noEmit\": true,\n    \"outDir\": \"dist\"\n  },\n  \"include\": [\n    \"src/**/*\"\n  ]\n}\n```\n\n> This TypeScript configuration is optimized for Mastra projects, using modern module resolution and strict type checking.\n\n### Set up your API key\n\nCreate `.env` file:\n\n```bash copy\ntouch .env\n```\n\nAdd your API key:\n\n```bash filename=\".env\" copy\nOPENAI_API_KEY=<your-api-key>\n```\n\n> This example uses OpenAI. Each LLM provider uses a unique name. See [Model Capabilities](/docs/getting-started/model-capability) for more information.\n\n### Create a Tool\n\nCreate a `weather-tool.ts` file:\n\n```bash copy\nmkdir -p src/mastra/tools && touch src/mastra/tools/weather-tool.ts\n```\n\nAdd the following code:\n\n```ts filename=\"src/mastra/tools/weather-tool.ts\" showLineNumbers copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const weatherTool = createTool({\n  id: \"get-weather\",\n  description: \"Get current weather for a location\",\n  inputSchema: z.object({\n    location: z.string().describe(\"City name\")\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  }),\n  execute: async () => {\n    return {\n      output: \"The weather is sunny\"\n    };\n  }\n});\n```\n\n> See the full weatherTool example in [Giving an Agent a Tool](/examples/agents/using-a-tool).\n\n### Create an Agent\n\nCreate a `weather-agent.ts` file:\n\n```bash copy\nmkdir -p src/mastra/agents && touch src/mastra/agents/weather-agent.ts\n```\n\nAdd the following code:\n\n```ts filename=\"src/mastra/agents/weather-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { weatherTool } from \"../tools/weather-tool\";\n\nexport const weatherAgent = new Agent({\n  name: 'Weather Agent',\n  instructions: `\n      You are a helpful weather assistant that provides accurate weather information.\n\n      Your primary function is to help users get weather details for specific locations. When responding:\n      - Always ask for a location if none is provided\n      - If the location name isn’t in English, please translate it\n      - If giving a location with multiple parts (e.g. \"New York, NY\"), use the most relevant part (e.g. \"New York\")\n      - Include relevant details like humidity, wind conditions, and precipitation\n      - Keep responses concise but informative\n\n      Use the weatherTool to fetch current weather data.\n`,\n  model: openai('gpt-4o-mini'),\n  tools: { weatherTool }\n});\n```\n\n### Register the Agent\n\nCreate the Mastra entry point and register agent:\n\n```bash copy\ntouch src/mastra/index.ts\n```\n\nAdd the following code:\n\n```ts filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { weatherAgent } from \"./agents/weather-agent\";\n\nexport const mastra = new Mastra({\n  agents: { weatherAgent }\n});\n```\n\nYou can now launch the [Mastra Development Server](/docs/server-db/local-dev-playground) and test your agent using the Mastra Playground.\n\n</Steps>\n\n## Add to an existing project\n\nMastra can be installed and integrated into a wide range of projects. Below are links to integration guides to help you get started:\n\n- [Next.js](/docs/frameworks/web-frameworks/next-js)\n- [Vite + React](/docs/frameworks/web-frameworks/vite-react)\n- [Astro](/docs/frameworks/web-frameworks/astro)\n- [Express](/docs/frameworks/servers/express)\n\n\n### `mastra init`\n\nTo install Mastra in an existing project, use the `mastra init` command.\n\n> See [mastra init](/reference/cli/init) for more information.\n\n## Next steps\n\n- [Local Development](/docs/server-db/local-dev-playground)\n- [Deploy to Mastra Cloud](/docs/deployment/overview)\n\n\n/docs/server-db/local-dev-playground\n\n\n## Model Capabilities\n[EN] Source: https://mastra.ai/en/docs/getting-started/model-capability\n\nimport { ProviderTable } from \"@/components/provider-table\";\n\nThe AI providers support different language models with various capabilities. Not all models support structured output, image input, object generation, tool usage, or tool streaming.\n\nHere are the capabilities of popular models:\n\n<ProviderTable />\n\nSource: [AI SDK | Model Capabilities](https://sdk.vercel.ai/docs/foundations/providers-and-models#model-capabilities)\n\n\n---\ntitle: \"Model Providers | Getting Started | Mastra Docs\"\ndescription: \"Learn how to configure and use different model providers with Mastra.\"\n---\n\nimport { Callout } from 'nextra/components'\n\n# Model Providers\n[EN] Source: https://mastra.ai/en/docs/getting-started/model-providers\n\nModel providers are used to interact with different language models. Mastra uses [Vercel's AI SDK](https://sdk.vercel.ai) as a model routing layer to provide a similar syntax for many models:\n\n```typescript showLineNumbers copy {1,7} filename=\"src/mastra/agents/weather-agent.ts\"\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n  name: \"WeatherAgent\",\n  instructions: \"Instructions for the agent...\",\n  model: openai(\"gpt-4-turbo\"),\n});\n\nconst result = await agent.generate(\"What is the weather like?\");\n```\n\n## Types of AI SDK model providers\n\nModel providers from the AI SDK can be grouped into three main categories:\n\n- [Official providers maintained by the AI SDK team](/docs/getting-started/model-providers#official-providers)\n- [OpenAI-compatible providers](/docs/getting-started/model-providers#openai-compatible-providers)\n- [Community providers](/docs/getting-started/model-providers#community-providers)\n\n> You can find a list of all available model providers in the [AI SDK documentation](https://ai-sdk.dev/providers/ai-sdk-providers).\n\n<Callout>\nAI SDK model providers are packages that need to be installed in your Mastra project.\nThe default model provider selected during the installation process is installed in the project.\n\nIf you want to use a different model provider, you need to install it in your project as well.\n</Callout>\n\nHere are some examples of how Mastra agents can be configured to use the different types of model providers:\n\n### Official providers\n\nOfficial model providers are maintained by the AI SDK team.\nTheir packages are usually prefixed with `@ai-sdk/`, e.g. `@ai-sdk/anthropic`, `@ai-sdk/openai`, etc.\n\n```typescript showLineNumbers copy {1,7} filename=\"src/mastra/agents/weather-agent.ts\"\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n  name: \"WeatherAgent\",\n  instructions: \"Instructions for the agent...\",\n  model: openai(\"gpt-4-turbo\"),\n});\n```\n\nAdditional configuration may be done by importing a helper function from the AI SDK provider.\nHere's an example using the OpenAI provider:\n\n```typescript showLineNumbers copy filename=\"src/mastra/agents/weather-agent.ts\" {1,4-8,13}\nimport { createOpenAI } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\"\n\nconst openai = createOpenAI({\n    baseUrl: \"<your-custom-base-url>\",\n    apiKey: \"<your-custom-api-key>\",\n    ...otherOptions\n});\n\nconst agent = new Agent({\n    name: \"WeatherAgent\",\n    instructions: \"Instructions for the agent...\",\n    model: openai(\"<model-name>\"),\n});\n```\n\n### OpenAI-compatible providers\n\nSome language model providers implement the OpenAI API. For these providers, you can use the [`@ai-sdk/openai-compatible`](https://www.npmjs.com/package/@ai-sdk/openai-compatible) provider.\n\nHere's the general setup and provider instance creation:\n\n```typescript showLineNumbers copy filename=\"src/mastra/agents/weather-agent.ts\" {1,4-14,19}\nimport { createOpenAICompatible } from \"@ai-sdk/openai-compatible\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst openaiCompatible = createOpenAICompatible({\n    name: \"<model-name>\",\n    baseUrl: \"<base-url>\",\n    apiKey: \"<api-key>\",\n    headers: {},\n    queryParams: {},\n    fetch: async (url, options) => {\n        // custom fetch logic\n        return fetch(url, options);\n    }\n});\n\nconst agent = new Agent({\n    name: \"WeatherAgent\",\n    instructions: \"Instructions for the agent...\",\n    model: openaiCompatible(\"<model-name>\"),\n});\n```\n\nFor more information on the OpenAI-compatible provider, please refer to the [AI SDK documentation](https://ai-sdk.dev/providers/openai-compatible-providers).\n\n### Community providers\n\nThe AI SDK provides a [Language Model Specification](https://github.com/vercel/ai/tree/main/packages/provider/src/language-model/v1).\nFollowing this specification, you can create your own model provider compatible with the AI SDK.\n\nSome community providers have implemented this specification and are compatible with the AI SDK.\nWe will look at one such provider, the Ollama provider available in the [`ollama-ai-provider`](https://github.com/sgomez/ollama-ai-provider) package.\n\nHere's an example:\n\n```typescript showLineNumbers copy filename=\"src/mastra/agents/weather-agent.ts\" {1,7}\nimport { ollama } from \"ollama-ai-provider\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst agent = new Agent({\n    name: \"WeatherAgent\",\n    instructions: \"Instructions for the agent...\",\n    model: ollama(\"llama3.2:latest\"),\n});\n```\n\nYou can also configure the Ollama provider like so:\n\n```typescript showLineNumbers copy filename=\"src/mastra/agents/weather-agent.ts\" {1,4-7,12}\nimport { createOllama } from \"ollama-ai-provider\";\nimport { Agent } from \"@mastra/core/agent\";\n\nconst ollama = createOllama({\n    baseUrl: \"<your-custom-base-url>\",\n    ...otherOptions,\n});\n\nconst agent = new Agent({\n    name: \"WeatherAgent\",\n    instructions: \"Instructions for the agent...\",\n    model: ollama(\"llama3.2:latest\"),\n});\n```\n\nFor more information on the Ollama provider and other available community providers, please refer to the [AI SDK documentation](https://ai-sdk.dev/providers/community-providers).\n\n<Callout>\nWhile this example shows how to use the Ollama provider, other providers like `openrouter`, `azure`, etc. may also be used.\n</Callout>\n\nDifferent AI providers may have different options for configuration. Please refer to the [AI SDK documentation](https://ai-sdk.dev/providers/ai-sdk-providers) for more information.\n\n\n---\ntitle: \"Local Project Structure | Getting Started | Mastra Docs\"\ndescription: Guide on organizing folders and files in Mastra, including best practices and recommended structures.\n---\n\nimport { FileTree } from \"nextra/components\";\n\n# Project Structure\n[EN] Source: https://mastra.ai/en/docs/getting-started/project-structure\n\nThis page provides a guide for organizing folders and files in Mastra. Mastra is a modular framework, and you can use any of the modules separately or together.\n\nYou could write everything in a single file, or separate each agent, tool, and workflow into their own files.\n\nWe don't enforce a specific folder structure, but we do recommend some best practices, and the CLI will scaffold a project with a sensible structure.\n\n## Example Project Structure\n\nA default project created with the CLI looks like this:\n\n<FileTree>\n  <FileTree.Folder name=\"src\" defaultOpen>\n    <FileTree.Folder name=\"mastra\" defaultOpen>\n      <FileTree.Folder name=\"agents\" defaultOpen>\n        <FileTree.File name=\"agent-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"tools\" defaultOpen>\n        <FileTree.File name=\"tool-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"workflows\" defaultOpen>\n        <FileTree.File name=\"workflow-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.File name=\"index.ts\" />\n    </FileTree.Folder>\n  </FileTree.Folder>\n  <FileTree.File name=\".env\" />\n  <FileTree.File name=\"package.json\" />\n  <FileTree.File name=\"tsconfig.json\" />\n</FileTree>\n{/*\n```\nroot/\n├── src/\n│   └── mastra/\n│       ├── agents/\n│       │   └── index.ts\n│       ├── tools/\n│       │   └── index.ts\n│       ├── workflows/\n│       │   └── index.ts\n│       ├── index.ts\n├── .env\n├── package.json\n├── tssconfig.json\n``` */}\n\n### Top-level Folders\n\n| Folder                 | Description                          |\n| ---------------------- | ------------------------------------ |\n| `src/mastra`           | Core application folder              |\n| `src/mastra/agents`    | Agent configurations and definitions |\n| `src/mastra/tools`     | Custom tool definitions              |\n| `src/mastra/workflows` | Workflow definitions                 |\n\n### Top-level Files\n\n| File                  | Description                                         |\n| --------------------- | --------------------------------------------------- |\n| `src/mastra/index.ts` | Main configuration file for Mastra                  |\n| `.env`                | Environment variables                               |\n| `package.json`        | Node.js project metadata, scripts, and dependencies |\n| `tsconfig.json`       | TypeScript compiler configuration                   |\n\n\n---\ntitle: \"Introduction | Mastra Docs\"\ndescription: \"Mastra is a TypeScript agent framework. It helps you build AI applications and features quickly. It gives you the set of primitives you need: workflows, agents, RAG, integrations, syncs and evals.\"\n---\n\n# About Mastra\n[EN] Source: https://mastra.ai/en/docs\n\nMastra is an open-source TypeScript agent framework.\n\nIt's designed to give you the primitives you need to build AI applications and features.\n\nYou can use Mastra to build [AI agents](/docs/agents/overview.mdx) that have memory and can execute functions, or chain LLM calls in deterministic [workflows](/docs/workflows/overview.mdx). You can chat with your agents in Mastra's [local dev environment](/docs/local-dev/mastra-dev.mdx), feed them application-specific knowledge with [RAG](/docs/rag/overview.mdx), and score their outputs with Mastra's [evals](/docs/evals/overview.mdx).\n\nThe main features include:\n\n- **[Model routing](https://sdk.vercel.ai/docs/introduction)**: Mastra uses the [Vercel AI SDK](https://sdk.vercel.ai/docs/introduction) for model routing, providing a unified interface to interact with any LLM provider including OpenAI, Anthropic, and Google Gemini.\n- **[Agent memory and tool calling](/docs/agents/agent-memory.mdx)**: With Mastra, you can give your agent tools (functions) that it can call. You can persist agent memory and retrieve it based on recency, semantic similarity, or conversation thread.\n- **[Workflow graphs](/docs/workflows/overview.mdx)**: When you want to execute LLM calls in a deterministic way, Mastra gives you a graph-based workflow engine. You can define discrete steps, log inputs and outputs at each step of each run, and pipe them into an observability tool. Mastra workflows have a simple syntax for control flow (`.then()`, `.branch()`, `.parallel()`) that allows branching and chaining.\n- **[Agent development environment](/docs/local-dev/mastra-dev.mdx)**: When you're developing an agent locally, you can chat with it and see its state and memory in Mastra's agent development environment.\n- **[Retrieval-augmented generation (RAG)](/docs/rag/overview.mdx)**: Mastra gives you APIs to process documents (text, HTML, Markdown, JSON) into chunks, create embeddings, and store them in a vector database. At query time, it retrieves relevant chunks to ground LLM responses in your data, with a unified API on top of multiple vector stores (Pinecone, pgvector, etc) and embedding providers (OpenAI, Cohere, etc).\n- **[Deployment](/docs/deployment/deployment.mdx)**: Mastra supports bundling your agents and workflows within an existing React, Next.js, or Node.js application, or into standalone endpoints. The Mastra deploy helper lets you easily bundle agents and workflows into a Node.js server using Hono, or deploy it onto a serverless platform like Vercel, Cloudflare Workers, or Netlify.\n- **[Evals](/docs/evals/overview.mdx)**: Mastra provides automated evaluation metrics that use model-graded, rule-based, and statistical methods to assess LLM outputs, with built-in metrics for toxicity, bias, relevance, and factual accuracy. You can also define your own evals.\n\n\n---\ntitle: Understanding the Mastra Cloud Dashboard\ndescription: Details of each feature available in Mastra Cloud\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\n\n# Navigating the Dashboard\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/dashboard\n\nThis page explains how to navigate the Mastra Cloud dashboard, where you can configure your project, view deployment details, and interact with agents and workflows using the built-in [Playground](/docs/mastra-cloud/dashboard#playground).\n\n<MastraCloudCallout />\n\n## Overview\n\nThe **Overview** page provides details about your application, including its domain URL, status, latest deployment, and connected agents and workflows.\n\n![Project dashboard](/image/mastra-cloud/mastra-cloud-project-dashboard.jpg)\n\nKey features:\n\nEach project shows its current deployment status, active domains, and environment variables, so you can quickly understand how your application is running.\n\n## Deployments\n\nThe **Deployments** page shows recent builds and gives you quick access to detailed build logs. Click any row to view more information about a specific deployment.\n\n![Dashboard deployment](/image/mastra-cloud/mastra-cloud-dashboard-deployments.jpg)\n\nKey features:\n\nEach deployment includes its current status, the Git branch it was deployed from, and a title generated from the commit hash.\n\n## Logs\n\nThe **Logs** page is where you'll find detailed information to help debug and monitor your application's behavior in the production environment.\n\n![Dashboard logs](/image/mastra-cloud/mastra-cloud-dashboard-logs.jpg)\n\nKey features:\n\nEach log includes a severity level and detailed messages showing agent, workflow, and storage activity.\n\n## Settings\n\nOn the **Settings** page you can modify the configuration of your application.\n\n![Dashboard settings](/image/mastra-cloud/mastra-cloud-dashboard-settings.jpg)\n\nKey features:\n\nYou can manage environment variables, edit key project settings like the name and branch, configure storage with LibSQLStore, and set a stable URL for your endpoints.\n\n> Changes to configuration require a new deployment before taking effect.\n\n## Playground\n\n### Agents\n\nOn the **Agents** page you'll see all agents used in your application. Click any agent to interact using the chat interface.\n\n![Dashboard playground agents](/image/mastra-cloud/mastra-cloud-dashboard-playground-agents.jpg)\n\nKey features:\n\nTest your agents in real time using the chat interface, review traces of each interaction, and see evaluation scores for every response.\n\n### Workflows\n\nOn the **Workflows** page you'll see all workflows used in your application. Click any workflow to interact using the runner interface.\n\n![Dashboard playground workflows](/image/mastra-cloud/mastra-cloud-dashboard-playground-workflows.jpg)\n\nKey features:\n\nVisualize your workflow with a step-by-step graph, view execution traces, and run workflows directly using the built-in runner.\n\n### Tools\n\nOn the **Tools** page you'll see all tools used by your agents. Click any tool to interact using the input interface.\n\n![Dashboard playground tools](/image/mastra-cloud/mastra-cloud-dashboard-playground-tools.jpg)\n\nKey features:\n\nTest your tools by providing an input that matches the schema and viewing the structured output.\n\n## MCP Servers\n\nThe **MCP Servers** page lists all MCP Servers included in your application. Click any MCP Server for more information.\n\n![Dashboard playground mcp servers](/image/mastra-cloud/mastra-cloud-dashboard-playground-mcpservers.jpg)\n\nKey features:\n\nEach MCP Server includes API endpoints for HTTP and SSE, along with IDE configuration snippets for tools like Cursor and Windsurf.\n\n## Next steps\n\n- [Understanding Tracing and Logs](/docs/mastra-cloud/observability)\n\n\n---\ntitle: Observability in Mastra Cloud\ndescription: Monitoring and debugging tools for Mastra Cloud deployments\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\n\n# Understanding Tracing and Logs\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/observability\n\nMastra Cloud captures execution data to help you monitor your application's behavior in the production environment.\n\n<MastraCloudCallout />\n\n## Logs\n\nYou can view detailed logs for debugging and monitoring your application's behavior on the [Logs](/docs/mastra-cloud/dashboard#logs) page of the Dashboard.\n\n![Dashboard logs](/image/mastra-cloud/mastra-cloud-dashboard-logs.jpg)\n\nKey features:\n\nEach log entry includes its severity level and a detailed message showing agent, workflow, or storage activity.\n\n## Traces\n\nMore detailed traces are available for both agents and workflows by using a [logger](/docs/observability/logging) or enabling [telemetry](/docs/observability/tracing) using one of our [supported providers](/reference/observability/providers).\n\n### Agents\n\nWith a [logger](/docs/observability/logging) enabled, you can view detailed outputs from your agents in the **Traces** section of the Agents Playground.\n\n![observability agents](/image/mastra-cloud/mastra-cloud-observability-agents.jpg)\n\nKey features:\n\nTools passed to the agent during generation are standardized using `convertTools`. This includes retrieving client-side tools, memory tools, and tools exposed from workflows.\n\n\n### Workflows\n\nWith a [logger](/docs/observability/logging) enabled, you can view detailed outputs from your workflows in the **Traces** section of the Workflows Playground.\n\n![observability workflows](/image/mastra-cloud/mastra-cloud-observability-workflows.jpg)\n\nKey features:\n\nWorkflows are created using `createWorkflow`, which sets up steps, metadata, and tools. You can run them with `runWorkflow` by passing input and options.\n\n## Next steps\n\n- [Logging](/docs/observability/logging)\n- [Tracing](/docs/observability/tracing)\n\n\n---\ntitle: Mastra Cloud\ndescription: Deployment and monitoring service for Mastra applications\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\nimport { FileTree } from \"nextra/components\";\n\n# Mastra Cloud\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/overview\n\n[Mastra Cloud](https://mastra.ai/cloud) is a platform for deploying, managing, monitoring, and debugging Mastra applications. When you [deploy](/docs/mastra-cloud/setting-up) your application, Mastra Cloud exposes your agents, tools, and workflows as REST API endpoints.\n\n<MastraCloudCallout />\n\n## Platform features\n\nDeploy and manage your applications with automated builds, organized projects, and no additional configuration.\n\n![Platform features](/image/mastra-cloud/mastra-cloud-platform-features.jpg)\n\nKey features:\n\nMastra Cloud supports zero-config deployment, continuous integration with GitHub, and atomic deployments that package agents, tools, and workflows together.\n\n## Project Dashboard\n\nMonitor and debug your applications with detailed output logs, deployment state, and interactive tools.\n\n![Project dashboard](/image/mastra-cloud/mastra-cloud-project-dashboard.jpg)\n\nKey features:\n\nThe Project Dashboard gives you an overview of your application's status and deployments, with access to logs and a built-in playground for testing agents and workflows.\n\n## Project structure\n\nUse a standard Mastra project structure for proper detection and deployment.\n\n<FileTree>\n  <FileTree.Folder name=\"src\" defaultOpen>\n    <FileTree.Folder name=\"mastra\" defaultOpen>\n      <FileTree.Folder name=\"agents\" defaultOpen>\n        <FileTree.File name=\"agent-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"tools\" defaultOpen>\n        <FileTree.File name=\"tool-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.Folder name=\"workflows\" defaultOpen>\n        <FileTree.File name=\"workflow-name.ts\" />\n      </FileTree.Folder>\n      <FileTree.File name=\"index.ts\" />\n    </FileTree.Folder>\n  </FileTree.Folder>\n  <FileTree.File name=\"package.json\" />\n</FileTree>\n\nMastra Cloud scans your repository for:\n\n- **Agents**: Defined using: `new Agent({...})`\n- **Tools**: Defined using: `createTool({...})`\n- **Workflows**: Defined using: `createWorkflow({...})`\n- **Steps**: Defined using: `createStep({...})`\n- **Environment Variables**: API keys and configuration variables\n\n## Technical implementation\n\nMastra Cloud is purpose-built for Mastra agents, tools, and workflows. It handles long-running requests, records detailed traces for every execution, and includes built-in support for evals.\n\n## Next steps\n\n- [Setting Up and Deploying](/docs/mastra-cloud/setting-up)\n\n\n---\ntitle: Setting Up a Project\ndescription: Configuration steps for Mastra Cloud projects\n---\n\nimport { MastraCloudCallout } from '@/components/mastra-cloud-callout'\nimport { Steps } from \"nextra/components\";\n\n# Setting Up and Deploying\n[EN] Source: https://mastra.ai/en/docs/mastra-cloud/setting-up\n\nThis page explains how to set up a project on [Mastra Cloud](https://mastra.ai/cloud) with automatic deployments using our GitHub integration.\n\n<MastraCloudCallout />\n\n## Prerequisites\n\n- A [Mastra Cloud](https://mastra.ai/cloud) account\n- A GitHub account / repository containing a Mastra application\n\n> See our [Getting started](/docs/getting-started/installation) guide to scaffold out a new Mastra project with sensible defaults.\n\n## Setup and Deploy process\n\n<Steps>\n\n### Sign in to Mastra Cloud\n\nHead over to [https://cloud.mastra.ai/](https://cloud.mastra.ai) and sign in with either:\n\n- **GitHub**\n- **Google**\n\n### Install the Mastra GitHub app\n\nWhen prompted, install the Mastra GitHub app.\n\n![Install GitHub](/image/mastra-cloud/mastra-cloud-install-github.jpg)\n\n### Create a new project\n\nClick the **Create new project** button to create a new project.\n\n![Create new project](/image/mastra-cloud/mastra-cloud-create-new-project.jpg)\n\n### Import a Git repository\n\nSearch for a repository, then click **Import**.\n\n![Import Git repository](/image/mastra-cloud/mastra-cloud-import-git-repository.jpg)\n\n### Configure the deployment\n\nMastra Cloud automatically detects the right build settings, but you can customize them using the options described below.\n\n![Deployment details](/image/mastra-cloud/mastra-cloud-deployment-details.jpg)\n\n- **Importing from GitHub**: The GitHub repository name\n- **Project name**: Customize the project name\n- **Branch**: The branch to deploy from\n- **Project root**: The root directory of your project\n- **Mastra directory**: Where Mastra files are located\n- **Environment variables**: Add environment variables used by the application\n- **Build and Store settings**:\n   - **Install command**: Runs pre-build to install project dependencies\n   - **Project setup command**: Runs pre-build to prepare any external dependencies\n   - **Port**: The network port the server will use\n   - **Store settings**: Use Mastra Cloud's built-in [LibSQLStore](/docs/storage/overview) storage\n- **Deploy Project**: Starts the deployment process\n\n### Deploy project\n\nClick **Deploy Project** to create and deploy your application using the configuration you’ve set.\n\n</Steps>\n\n## Successful deployment\n\nAfter a successful deployment you'll be shown the **Overview** screen where you can view your project's status, domains, latest deployments and connected agents and workflows.\n\n![Successful deployment](/image/mastra-cloud/mastra-cloud-successful-deployment.jpg)\n\n## Continuous integration\n\nYour project is now configured with automatic deployments which occur whenever you push to the configured branch of your GitHub repository.\n\n## Testing your application\n\nAfter a successful deployment you can test your agents and workflows from the [Playground](/docs/mastra-cloud/dashboard#playground) in Mastra Cloud, or interact with them using our [Client SDK](/docs/client-js/overview).\n\n## Next steps\n\n- [Navigating the Dashboard](/docs/mastra-cloud/dashboard)\n\n\n---\ntitle: \"Logging | Mastra Observability Documentation\"\ndescription: Documentation on effective logging in Mastra, crucial for understanding application behavior and improving AI accuracy.\n---\n\nimport Image from \"next/image\";\n\n# Logging\n[EN] Source: https://mastra.ai/en/docs/observability/logging\n\nIn Mastra, logs can detail when certain functions run, what input data they receive, and how they respond.\n\n## Basic Setup\n\nHere's a minimal example that sets up a **console logger** at the `INFO` level. This will print out informational messages and above (i.e., `DEBUG`, `INFO`, `WARN`, `ERROR`) to the console.\n\n```typescript filename=\"mastra.config.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core\";\nimport { PinoLogger } from \"@mastra/loggers\";\n\nexport const mastra = new Mastra({\n  // Other Mastra configuration...\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\",\n  }),\n});\n```\n\nIn this configuration:\n\n- `name: \"Mastra\"` specifies the name to group logs under.\n- `level: \"info\"` sets the minimum severity of logs to record.\n\n## Configuration\n\n- For more details on the options you can pass to `PinoLogger()`, see the [PinoLogger reference documentation](/reference/observability/logger).\n- Once you have a `Logger` instance, you can call its methods (e.g., `.info()`, `.warn()`, `.error()`) in the [Logger instance reference documentation](/reference/observability/logger).\n- If you want to send your logs to an external service for centralized collection, analysis, or storage, you can configure other logger types such as Upstash Redis. Consult the [Logger reference documentation](/reference/observability/logger) for details on parameters like `url`, `token`, and `key` when using the `UPSTASH` logger type.\n\n\n---\ntitle: \"Next.js Tracing | Mastra Observability Documentation\"\ndescription: \"Set up OpenTelemetry tracing for Next.js applications\"\n---\n\n# Next.js Tracing\n[EN] Source: https://mastra.ai/en/docs/observability/nextjs-tracing\n\nNext.js requires additional configuration to enable OpenTelemetry tracing.\n\n### Step 1: Next.js Configuration\n\nStart by enabling the instrumentation hook in your Next.js config:\n\n```ts filename=\"next.config.ts\" showLineNumbers copy\nimport type { NextConfig } from \"next\";\n\nconst nextConfig: NextConfig = {\n  experimental: {\n    instrumentationHook: true, // Not required in Next.js 15+\n  },\n};\n\nexport default nextConfig;\n```\n\n### Step 2: Mastra Configuration\n\nConfigure your Mastra instance:\n\n```typescript filename=\"mastra.config.ts\" copy\nimport { Mastra } from \"@mastra/core\";\n\nexport const mastra = new Mastra({\n  // ... other config\n  telemetry: {\n    serviceName: \"your-project-name\",\n    enabled: true,\n  },\n});\n```\n\n### Step 3: Configure your providers\n\nIf you're using Next.js, you have two options for setting up OpenTelemetry instrumentation:\n\n#### Option 1: Using a Custom Exporter\n\nThe default that will work across providers is to configure a custom exporter:\n\n1. Install the required dependencies (example using Langfuse):\n\n```bash copy\nnpm install @opentelemetry/api langfuse-vercel\n```\n\n2. Create an instrumentation file:\n\n```ts filename=\"instrumentation.ts\" copy\nimport {\n  NodeSDK,\n  ATTR_SERVICE_NAME,\n  resourceFromAttributes,\n} from \"@mastra/core/telemetry/otel-vendor\";\nimport { LangfuseExporter } from \"langfuse-vercel\";\n\nexport function register() {\n  const exporter = new LangfuseExporter({\n    // ... Langfuse config\n  });\n\n  const sdk = new NodeSDK({\n    resource: resourceFromAttributes({\n      [ATTR_SERVICE_NAME]: \"ai\",\n    }),\n    traceExporter: exporter,\n  });\n\n  sdk.start();\n}\n```\n\n#### Option 2: Using Vercel's Otel Setup\n\nIf you're deploying to Vercel, you can use their OpenTelemetry setup:\n\n1. Install the required dependencies:\n\n```bash copy\nnpm install @opentelemetry/api @vercel/otel\n```\n\n2. Create an instrumentation file at the root of your project (or in the src folder if using one):\n\n```ts filename=\"instrumentation.ts\" copy\nimport { registerOTel } from \"@vercel/otel\";\n\nexport function register() {\n  registerOTel({ serviceName: \"your-project-name\" });\n}\n```\n\n### Summary\n\nThis setup will enable OpenTelemetry tracing for your Next.js application and Mastra operations.\n\nFor more details, see the documentation for:\n\n- [Next.js Instrumentation](https://nextjs.org/docs/app/building-your-application/optimizing/instrumentation)\n- [Vercel OpenTelemetry](https://vercel.com/docs/observability/otel-overview/quickstart)\n\n\n---\ntitle: \"Tracing | Mastra Observability Documentation\"\ndescription: \"Set up OpenTelemetry tracing for Mastra applications\"\n---\n\nimport Image from \"next/image\";\n\n# Tracing\n[EN] Source: https://mastra.ai/en/docs/observability/tracing\n\nMastra supports the OpenTelemetry Protocol (OTLP) for tracing and monitoring your application. When telemetry is enabled, Mastra automatically traces all core primitives including agent operations, LLM interactions, tool executions, integration calls, workflow runs, and database operations. Your telemetry data can then be exported to any OTEL collector.\n\n### Basic Configuration\n\nHere's a simple example of enabling telemetry:\n\n```ts filename=\"mastra.config.ts\" showLineNumbers copy\nexport const mastra = new Mastra({\n  // ... other config\n  telemetry: {\n    serviceName: \"my-app\",\n    enabled: true,\n    sampling: {\n      type: \"always_on\",\n    },\n    export: {\n      type: \"otlp\",\n      endpoint: \"http://localhost:4318\", // SigNoz local endpoint\n    },\n  },\n});\n```\n\n### Configuration Options\n\nThe telemetry config accepts these properties:\n\n```ts\ntype OtelConfig = {\n  // Name to identify your service in traces (optional)\n  serviceName?: string;\n\n  // Enable/disable telemetry (defaults to true)\n  enabled?: boolean;\n\n  // Control how many traces are sampled\n  sampling?: {\n    type: \"ratio\" | \"always_on\" | \"always_off\" | \"parent_based\";\n    probability?: number; // For ratio sampling\n    root?: {\n      probability: number; // For parent_based sampling\n    };\n  };\n\n  // Where to send telemetry data\n  export?: {\n    type: \"otlp\" | \"console\";\n    endpoint?: string;\n    headers?: Record<string, string>;\n  };\n};\n```\n\nSee the [OtelConfig reference documentation](../../reference/observability/otel-config.mdx) for more details.\n\n### Environment Variables\n\nYou can configure the OTLP endpoint and headers through environment variables:\n\n```env filename=\".env\" copy\nOTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\nOTEL_EXPORTER_OTLP_HEADERS=x-api-key=your-api-key\n```\n\nThen in your config:\n\n```ts filename=\"mastra.config.ts\" showLineNumbers copy\nexport const mastra = new Mastra({\n  // ... other config\n  telemetry: {\n    serviceName: \"my-app\",\n    enabled: true,\n    export: {\n      type: \"otlp\",\n      // endpoint and headers will be picked up from env vars\n    },\n  },\n});\n```\n\n### Example: SigNoz Integration\n\nHere's what a traced agent interaction looks like in [SigNoz](https://signoz.io):\n\n<img\n  src=\"/image/signoz-telemetry-demo.png\"\n  alt=\"Agent interaction trace showing spans, LLM calls, and tool executions\"\n  style={{ maxWidth: \"800px\", width: \"100%\", margin: \"8px 0\" }}\n  className=\"nextra-image rounded-md\"\n  data-zoom\n  width={800}\n  height={400}\n/>\n\n### Other Supported Providers\n\nFor a complete list of supported observability providers and their configuration details, see the [Observability Providers reference](../../reference/observability/providers/).\n\n### Custom Instrumentation files\n\nYou can define custom instrumentation files in your Mastra project by placing them in the `/mastra` folder. Mastra automatically detects and bundles these files instead of using the default instrumentation.\n\n#### Supported File Types\n\nMastra looks for instrumentation files with these extensions:\n- `instrumentation.js`\n- `instrumentation.ts` \n- `instrumentation.mjs`\n\n#### Example\n\n```ts filename=\"/mastra/instrumentation.ts\" showLineNumbers copy\nimport { NodeSDK } from '@opentelemetry/sdk-node';\nimport { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';\nimport { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';\n\nconst sdk = new NodeSDK({\n  traceExporter: new OTLPTraceExporter({\n    url: 'http://localhost:4318/v1/traces',\n  }),\n  instrumentations: [getNodeAutoInstrumentations()],\n});\n\nsdk.start();\n```\n\nWhen Mastra finds a custom instrumentation file, it automatically replaces the default instrumentation and bundles it during the build process.\n\n### Next.js-specific Tracing steps\n\nIf you're using Next.js, you have three additional configuration steps:\n\n1. Enable the instrumentation hook in `next.config.ts`\n2. Configure Mastra telemetry settings\n3. Set up an OpenTelemetry exporter\n\nFor implementation details, see the [Next.js Tracing](./nextjs-tracing) guide.\n\n\n---\ntitle: Chunking and Embedding Documents | RAG | Mastra Docs\ndescription: Guide on chunking and embedding documents in Mastra for efficient processing and retrieval.\n---\n\n## Chunking and Embedding Documents\n[EN] Source: https://mastra.ai/en/docs/rag/chunking-and-embedding\n\nBefore processing, create a MDocument instance from your content. You can initialize it from various formats:\n\n```ts showLineNumbers copy\nconst docFromText = MDocument.fromText(\"Your plain text content...\");\nconst docFromHTML = MDocument.fromHTML(\"<html>Your HTML content...</html>\");\nconst docFromMarkdown = MDocument.fromMarkdown(\"# Your Markdown content...\");\nconst docFromJSON = MDocument.fromJSON(`{ \"key\": \"value\" }`);\n```\n\n## Step 1: Document Processing\n\nUse `chunk` to split documents into manageable pieces. Mastra supports multiple chunking strategies optimized for different document types:\n\n- `recursive`: Smart splitting based on content structure\n- `character`: Simple character-based splits\n- `token`: Token-aware splitting\n- `markdown`: Markdown-aware splitting\n- `html`: HTML structure-aware splitting\n- `json`: JSON structure-aware splitting\n- `latex`: LaTeX structure-aware splitting\n\nHere's an example of how to use the `recursive` strategy:\n\n```ts showLineNumbers copy\nconst chunks = await doc.chunk({\n  strategy: \"recursive\",\n  size: 512,\n  overlap: 50,\n  separator: \"\\n\",\n  extract: {\n    metadata: true, // Optionally extract metadata\n  },\n});\n```\n\n**Note:** Metadata extraction may use LLM calls, so ensure your API key is set.\n\nWe go deeper into chunking strategies in our [chunk documentation](/reference/rag/chunk.mdx).\n\n## Step 2: Embedding Generation\n\nTransform chunks into embeddings using your preferred provider. Mastra supports many embedding providers, including OpenAI and Cohere:\n\n### Using OpenAI\n\n```ts showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { embedMany } from \"ai\";\n\nconst { embeddings } = await embedMany({\n  model: openai.embedding(\"text-embedding-3-small\"),\n  values: chunks.map((chunk) => chunk.text),\n});\n```\n\n### Using Cohere\n\n```ts showLineNumbers copy\nimport { cohere } from \"@ai-sdk/cohere\";\nimport { embedMany } from \"ai\";\n\nconst { embeddings } = await embedMany({\n  model: cohere.embedding(\"embed-english-v3.0\"),\n  values: chunks.map((chunk) => chunk.text),\n});\n```\n\nThe embedding functions return vectors, arrays of numbers representing the semantic meaning of your text, ready for similarity searches in your vector database.\n\n### Configuring Embedding Dimensions\n\nEmbedding models typically output vectors with a fixed number of dimensions (e.g., 1536 for OpenAI's `text-embedding-3-small`).\nSome models support reducing this dimensionality, which can help:\n\n- Decrease storage requirements in vector databases\n- Reduce computational costs for similarity searches\n\nHere are some supported models:\n\nOpenAI (text-embedding-3 models):\n\n```ts\nconst { embeddings } = await embedMany({\n  model: openai.embedding(\"text-embedding-3-small\", {\n    dimensions: 256, // Only supported in text-embedding-3 and later\n  }),\n  values: chunks.map((chunk) => chunk.text),\n});\n```\n\nGoogle (text-embedding-004):\n\n```ts\nconst { embeddings } = await embedMany({\n  model: google.textEmbeddingModel(\"text-embedding-004\", {\n    outputDimensionality: 256, // Truncates excessive values from the end\n  }),\n  values: chunks.map((chunk) => chunk.text),\n});\n```\n\n### Vector Database Compatibility\n\nWhen storing embeddings, the vector database index must be configured to match the output size of your embedding model. If the dimensions do not match, you may get errors or data corruption.\n\n## Example: Complete Pipeline\n\nHere's an example showing document processing and embedding generation with both providers:\n\n```ts showLineNumbers copy\nimport { embedMany } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { cohere } from \"@ai-sdk/cohere\";\n\nimport { MDocument } from \"@mastra/rag\";\n\n// Initialize document\nconst doc = MDocument.fromText(`\n  Climate change poses significant challenges to global agriculture.\n  Rising temperatures and changing precipitation patterns affect crop yields.\n`);\n\n// Create chunks\nconst chunks = await doc.chunk({\n  strategy: \"recursive\",\n  size: 256,\n  overlap: 50,\n});\n\n// Generate embeddings with OpenAI\nconst { embeddings: openAIEmbeddings } = await embedMany({\n  model: openai.embedding(\"text-embedding-3-small\"),\n  values: chunks.map((chunk) => chunk.text),\n});\n\n// OR\n\n// Generate embeddings with Cohere\nconst { embeddings: cohereEmbeddings } = await embedMany({\n  model: cohere.embedding(\"embed-english-v3.0\"),\n  values: chunks.map((chunk) => chunk.text),\n});\n\n// Store embeddings in your vector database\nawait vectorStore.upsert({\n  indexName: \"embeddings\",\n  vectors: embeddings,\n});\n```\n\n##\n\nFor more examples of different chunking strategies and embedding configurations, see:\n\n- [Adjust Chunk Size](/reference/rag/chunk.mdx#adjust-chunk-size)\n- [Adjust Chunk Delimiters](/reference/rag/chunk.mdx#adjust-chunk-delimiters)\n- [Embed Text with Cohere](/reference/rag/embeddings.mdx#using-cohere)\n\nFor more details on vector databases and embeddings, see:\n\n- [Vector Databases](./vector-databases.mdx)\n- [Embedding API Reference](/reference/rag/embeddings.mdx)\n\n\n---\ntitle: RAG (Retrieval-Augmented Generation) in Mastra | Mastra Docs\ndescription: Overview of Retrieval-Augmented Generation (RAG) in Mastra, detailing its capabilities for enhancing LLM outputs with relevant context.\n---\n\n# RAG (Retrieval-Augmented Generation) in Mastra\n[EN] Source: https://mastra.ai/en/docs/rag/overview\n\nRAG in Mastra helps you enhance LLM outputs by incorporating relevant context from your own data sources, improving accuracy and grounding responses in real information.\n\nMastra's RAG system provides:\n\n- Standardized APIs to process and embed documents\n- Support for multiple vector stores\n- Chunking and embedding strategies for optimal retrieval\n- Observability for tracking embedding and retrieval performance\n\n## Example\n\nTo implement RAG, you process your documents into chunks, create embeddings, store them in a vector database, and then retrieve relevant context at query time.\n\n```ts showLineNumbers copy\nimport { embedMany } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { PgVector } from \"@mastra/pg\";\nimport { MDocument } from \"@mastra/rag\";\nimport { z } from \"zod\";\n\n// 1. Initialize document\nconst doc = MDocument.fromText(`Your document text here...`);\n\n// 2. Create chunks\nconst chunks = await doc.chunk({\n  strategy: \"recursive\",\n  size: 512,\n  overlap: 50,\n});\n\n// 3. Generate embeddings; we need to pass the text of each chunk\nconst { embeddings } = await embedMany({\n  values: chunks.map((chunk) => chunk.text),\n  model: openai.embedding(\"text-embedding-3-small\"),\n});\n\n// 4. Store in vector database\nconst pgVector = new PgVector({\n  connectionString: process.env.POSTGRES_CONNECTION_STRING,\n});\nawait pgVector.upsert({\n  indexName: \"embeddings\",\n  vectors: embeddings,\n}); // using an index name of 'embeddings'\n\n// 5. Query similar chunks\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: queryVector,\n  topK: 3,\n}); // queryVector is the embedding of the query\n\nconsole.log(\"Similar chunks:\", results);\n```\n\nThis example shows the essentials: initialize a document, create chunks, generate embeddings, store them, and query for similar content.\n\n## Document Processing\n\nThe basic building block of RAG is document processing. Documents can be chunked using various strategies (recursive, sliding window, etc.) and enriched with metadata. See the [chunking and embedding doc](./chunking-and-embedding.mdx).\n\n## Vector Storage\n\nMastra supports multiple vector stores for embedding persistence and similarity search, including pgvector, Pinecone, Qdrant, and MongoDB. See the [vector database doc](./vector-databases.mdx).\n\n## Observability and Debugging\n\nMastra's RAG system includes observability features to help you optimize your retrieval pipeline:\n\n- Track embedding generation performance and costs\n- Monitor chunk quality and retrieval relevance\n- Analyze query patterns and cache hit rates\n- Export metrics to your observability platform\n\nSee the [OTel Configuration](../../reference/observability/otel-config.mdx) page for more details.\n\n## More resources\n\n- [Chain of Thought RAG Example](../../examples/rag/usage/cot-rag.mdx)\n- [All RAG Examples](../../examples/) (including different chunking strategies, embedding models, and vector stores)\n\n\n---\ntitle: \"Retrieval, Semantic Search, Reranking | RAG | Mastra Docs\"\ndescription: Guide on retrieval processes in Mastra's RAG systems, including semantic search, filtering, and re-ranking.\n---\n\nimport { Tabs } from \"nextra/components\";\n\n## Retrieval in RAG Systems\n[EN] Source: https://mastra.ai/en/docs/rag/retrieval\n\nAfter storing embeddings, you need to retrieve relevant chunks to answer user queries.\n\nMastra provides flexible retrieval options with support for semantic search, filtering, and re-ranking.\n\n## How Retrieval Works\n\n1. The user's query is converted to an embedding using the same model used for document embeddings\n2. This embedding is compared to stored embeddings using vector similarity\n3. The most similar chunks are retrieved and can be optionally:\n\n- Filtered by metadata\n- Re-ranked for better relevance\n- Processed through a knowledge graph\n\n## Basic Retrieval\n\nThe simplest approach is direct semantic search. This method uses vector similarity to find chunks that are semantically similar to the query:\n\n```ts showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { embed } from \"ai\";\nimport { PgVector } from \"@mastra/pg\";\n\n// Convert query to embedding\nconst { embedding } = await embed({\n  value: \"What are the main points in the article?\",\n  model: openai.embedding(\"text-embedding-3-small\"),\n});\n\n// Query vector store\nconst pgVector = new PgVector({\n  connectionString: process.env.POSTGRES_CONNECTION_STRING,\n});\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: embedding,\n  topK: 10,\n});\n\n// Display results\nconsole.log(results);\n```\n\nResults include both the text content and a similarity score:\n\n```ts showLineNumbers copy\n[\n  {\n    text: \"Climate change poses significant challenges...\",\n    score: 0.89,\n    metadata: { source: \"article1.txt\" },\n  },\n  {\n    text: \"Rising temperatures affect crop yields...\",\n    score: 0.82,\n    metadata: { source: \"article1.txt\" },\n  },\n  // ... more results\n];\n```\n\nFor an example of how to use the basic retrieval method, see the [Retrieve Results](../../examples/rag/query/retrieve-results.mdx) example.\n\n## Advanced Retrieval options\n\n### Metadata Filtering\n\nFilter results based on metadata fields to narrow down the search space. This is useful when you have documents from different sources, time periods, or with specific attributes. Mastra provides a unified MongoDB-style query syntax that works across all supported vector stores.\n\nFor detailed information about available operators and syntax, see the [Metadata Filters Reference](/reference/rag/metadata-filters).\n\nBasic filtering examples:\n\n```ts showLineNumbers copy\n// Simple equality filter\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: embedding,\n  topK: 10,\n  filter: {\n    source: \"article1.txt\",\n  },\n});\n\n// Numeric comparison\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: embedding,\n  topK: 10,\n  filter: {\n    price: { $gt: 100 },\n  },\n});\n\n// Multiple conditions\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: embedding,\n  topK: 10,\n  filter: {\n    category: \"electronics\",\n    price: { $lt: 1000 },\n    inStock: true,\n  },\n});\n\n// Array operations\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: embedding,\n  topK: 10,\n  filter: {\n    tags: { $in: [\"sale\", \"new\"] },\n  },\n});\n\n// Logical operators\nconst results = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: embedding,\n  topK: 10,\n  filter: {\n    $or: [{ category: \"electronics\" }, { category: \"accessories\" }],\n    $and: [{ price: { $gt: 50 } }, { price: { $lt: 200 } }],\n  },\n});\n```\n\nCommon use cases for metadata filtering:\n\n- Filter by document source or type\n- Filter by date ranges\n- Filter by specific categories or tags\n- Filter by numerical ranges (e.g., price, rating)\n- Combine multiple conditions for precise querying\n- Filter by document attributes (e.g., language, author)\n\nFor an example of how to use metadata filtering, see the [Hybrid Vector Search](../../examples/rag/query/hybrid-vector-search.mdx) example.\n\n### Vector Query Tool\n\nSometimes you want to give your agent the ability to query a vector database directly. The Vector Query Tool allows your agent to be in charge of retrieval decisions, combining semantic search with optional filtering and reranking based on the agent's understanding of the user's needs.\n\n```ts showLineNumbers copy\nconst vectorQueryTool = createVectorQueryTool({\n  vectorStoreName: \"pgVector\",\n  indexName: \"embeddings\",\n  model: openai.embedding(\"text-embedding-3-small\"),\n});\n```\n\nWhen creating the tool, pay special attention to the tool's name and description - these help the agent understand when and how to use the retrieval capabilities. For example, you might name it \"SearchKnowledgeBase\" and describe it as \"Search through our documentation to find relevant information about X topic.\"\n\nThis is particularly useful when:\n\n- Your agent needs to dynamically decide what information to retrieve\n- The retrieval process requires complex decision-making\n- You want the agent to combine multiple retrieval strategies based on context\n\n#### Database-Specific Configurations\n\nThe Vector Query Tool supports database-specific configurations that enable you to leverage unique features and optimizations of different vector stores:\n\n```ts showLineNumbers copy\n// Pinecone with namespace\nconst pineconeQueryTool = createVectorQueryTool({\n  vectorStoreName: \"pinecone\",\n  indexName: \"docs\",\n  model: openai.embedding(\"text-embedding-3-small\"),\n  databaseConfig: {\n    pinecone: {\n      namespace: \"production\"  // Isolate data by environment\n    }\n  }\n});\n\n// pgVector with performance tuning\nconst pgVectorQueryTool = createVectorQueryTool({\n  vectorStoreName: \"postgres\",\n  indexName: \"embeddings\", \n  model: openai.embedding(\"text-embedding-3-small\"),\n  databaseConfig: {\n    pgvector: {\n      minScore: 0.7,    // Filter low-quality results\n      ef: 200,          // HNSW search parameter\n      probes: 10        // IVFFlat probe parameter\n    }\n  }\n});\n\n// Chroma with advanced filtering\nconst chromaQueryTool = createVectorQueryTool({\n  vectorStoreName: \"chroma\",\n  indexName: \"documents\",\n  model: openai.embedding(\"text-embedding-3-small\"),\n  databaseConfig: {\n    chroma: {\n      where: { \"category\": \"technical\" },\n      whereDocument: { \"$contains\": \"API\" }\n    }\n  }\n});\n\n// LanceDB with table specificity\nconst lanceQueryTool = createVectorQueryTool({\n  vectorStoreName: \"lance\",\n  indexName: \"documents\",\n  model: openai.embedding(\"text-embedding-3-small\"),\n  databaseConfig: {\n    lance: {\n      tableName: \"myVectors\",     // Specify which table to query\n      includeAllColumns: true     // Include all metadata columns in results\n    }\n  }\n});\n```\n\n**Key Benefits:**\n- **Pinecone namespaces**: Organize vectors by tenant, environment, or data type\n- **pgVector optimization**: Control search accuracy and speed with ef/probes parameters\n- **Quality filtering**: Set minimum similarity thresholds to improve result relevance\n- **LanceDB tables**: Separate data into tables for better organization and performance\n- **Runtime flexibility**: Override configurations dynamically based on context\n\n**Common Use Cases:**\n- Multi-tenant applications using Pinecone namespaces\n- Performance optimization in high-load scenarios\n- Environment-specific configurations (dev/staging/prod)\n- Quality-gated search results\n- Embedded, file-based vector storage with LanceDB for edge deployment scenarios\n\nYou can also override these configurations at runtime using the runtime context:\n\n```ts showLineNumbers copy\nimport { RuntimeContext } from '@mastra/core/runtime-context';\n\nconst runtimeContext = new RuntimeContext();\nruntimeContext.set('databaseConfig', {\n  pinecone: {\n    namespace: 'runtime-namespace'\n  }\n});\n\nawait pineconeQueryTool.execute({\n  context: { queryText: 'search query' },\n  mastra,\n  runtimeContext\n});\n```\n\nFor detailed configuration options and advanced usage, see the [Vector Query Tool Reference](/reference/tools/vector-query-tool).\n\n### Vector Store Prompts\n\nVector store prompts define query patterns and filtering capabilities for each vector database implementation.\nWhen implementing filtering, these prompts are required in the agent's instructions to specify valid operators and syntax for each vector store implementation.\n\n{/* \nLLM CONTEXT: This Tabs component displays vector store configuration examples for different database providers.\nEach tab shows how to configure a RAG agent with the appropriate prompt for that specific vector store.\nThe tabs demonstrate the consistent pattern of importing the store-specific prompt and adding it to agent instructions.\nThis helps users understand how to properly configure their RAG agents for different vector database backends.\nThe providers include Pg Vector, Pinecone, Qdrant, Chroma, Astra, LibSQL, Upstash, Cloudflare, MongoDB, and OpenSearch.\n*/}\n<Tabs items={['Pg Vector', 'Pinecone', 'Qdrant', 'Chroma', 'Astra', 'LibSQL', 'Upstash', 'Cloudflare', 'MongoDB', 'OpenSearch']}>\n  <Tabs.Tab>\n```ts showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { PGVECTOR_PROMPT } from \"@mastra/pg\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${PGVECTOR_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { PINECONE_PROMPT } from \"@mastra/pinecone\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${PINECONE_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { QDRANT_PROMPT } from \"@mastra/qdrant\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${QDRANT_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { CHROMA_PROMPT } from \"@mastra/chroma\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${CHROMA_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { ASTRA_PROMPT } from \"@mastra/astra\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${ASTRA_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { LIBSQL_PROMPT } from \"@mastra/libsql\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${LIBSQL_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { UPSTASH_PROMPT } from \"@mastra/upstash\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${UPSTASH_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { VECTORIZE_PROMPT } from \"@mastra/vectorize\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${VECTORIZE_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { MONGODB_PROMPT } from \"@mastra/mongodb\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${MONGODB_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { openai } from '@ai-sdk/openai';\nimport { OPENSEARCH_PROMPT } from \"@mastra/opensearch\";\n\nexport const ragAgent = new Agent({\n  name: 'RAG Agent',\n  model: openai('gpt-4o-mini'),\n  instructions: `\n  Process queries using the provided context. Structure responses to be concise and relevant.\n  ${OPENSEARCH_PROMPT}\n  `,\n  tools: { vectorQueryTool },\n});\n```\n</Tabs.Tab>\n\n</Tabs>\n\n### Re-ranking\n\nInitial vector similarity search can sometimes miss nuanced relevance. Re-ranking is a more computationally expensive process, but more accurate algorithm that improves results by:\n\n- Considering word order and exact matches\n- Applying more sophisticated relevance scoring\n- Using a method called cross-attention between query and documents\n\nHere's how to use re-ranking:\n\n```ts showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { \n  rerankWithScorer as rerank, \n  MastraAgentRelevanceScorer \n} from \"@mastra/rag\";\n\n// Get initial results from vector search\nconst initialResults = await pgVector.query({\n  indexName: \"embeddings\",\n  queryVector: queryEmbedding,\n  topK: 10,\n});\n\n// Create a relevance scorer\nconst relevanceProvider = new MastraAgentRelevanceScorer('relevance-scorer', openai(\"gpt-4o-mini\"));\n\n// Re-rank the results\nconst rerankedResults = await rerank({\n  results: initialResults,\n  query,\n  provider: relevanceProvider,\n  options: {\n    topK: 10,\n  },\n);\n```\n\n> **Note:** For semantic scoring to work properly during re-ranking, each result must include the text content in its `metadata.text` field.\n\nYou can also use other relevance score providers like Cohere or ZeroEntropy:\n\n```ts showLineNumbers copy\nconst relevanceProvider = new CohereRelevanceScorer('rerank-v3.5');\n```\n\n```ts showLineNumbers copy\nconst relevanceProvider = new ZeroEntropyRelevanceScorer('zerank-1');\n```\n\nThe re-ranked results combine vector similarity with semantic understanding to improve retrieval quality.\n\nFor more details about re-ranking, see the [rerank()](/reference/rag/rerankWithScorer) method.\n\nFor an example of how to use the re-ranking method, see the [Re-ranking Results](../../examples/rag/rerank/rerank.mdx) example.\n\n### Graph-based Retrieval\n\nFor documents with complex relationships, graph-based retrieval can follow connections between chunks. This helps when:\n\n- Information is spread across multiple documents\n- Documents reference each other\n- You need to traverse relationships to find complete answers\n\nExample setup:\n\n```ts showLineNumbers copy\nconst graphQueryTool = createGraphQueryTool({\n  vectorStoreName: \"pgVector\",\n  indexName: \"embeddings\",\n  model: openai.embedding(\"text-embedding-3-small\"),\n  graphOptions: {\n    threshold: 0.7,\n  },\n});\n```\n\nFor more details about graph-based retrieval, see the [GraphRAG](/reference/rag/graph-rag) class and the [createGraphQueryTool()](/reference/tools/graph-rag-tool) function.\n\nFor an example of how to use the graph-based retrieval method, see the [Graph-based Retrieval](../../examples/rag/usage/graph-rag.mdx) example.\n\n\n---\ntitle: \"Storing Embeddings in A Vector Database | Mastra Docs\"\ndescription: Guide on vector storage options in Mastra, including embedded and dedicated vector databases for similarity search.\n---\n\nimport { Tabs } from \"nextra/components\";\n\n## Storing Embeddings in A Vector Database\n[EN] Source: https://mastra.ai/en/docs/rag/vector-databases\n\nAfter generating embeddings, you need to store them in a database that supports vector similarity search. Mastra provides a consistent interface for storing and querying embeddings across various vector databases.\n\n## Supported Databases\n\n{/*\nLLM CONTEXT: This Tabs component showcases different vector database implementations supported by Mastra.\nEach tab demonstrates the setup and configuration for a specific vector database provider.\nThe tabs show consistent API patterns across different databases, helping users understand how to switch between providers.\nEach tab includes import statements, initialization code, and basic operations (createIndex, upsert) for that specific database.\nThe providers include Pg Vector, Pinecone, Qdrant, Chroma, Astra, LibSQL, Upstash, Cloudflare, MongoDB, OpenSearch, and Couchbase.\n*/}\n\n<Tabs items={['MongoDB', 'Pg Vector', 'Pinecone', 'Qdrant', 'Chroma', 'Astra', 'LibSQL', 'Upstash', 'Cloudflare', 'OpenSearch', 'Couchbase', 'LanceDB']}>\n  <Tabs.Tab>\n    ```ts filename=\"vector-store.ts\" showLineNumbers copy\n    import { MongoDBVector } from '@mastra/mongodb'\n\n    const store = new MongoDBVector({\n      uri: process.env.MONGODB_URI,\n      dbName: process.env.MONGODB_DATABASE\n    })\n    await store.createIndex({\n      indexName: \"myCollection\",\n      dimension: 1536,\n    });\n    await store.upsert({\n      indexName: \"myCollection\",\n      vectors: embeddings,\n      metadata: chunks.map(chunk => ({ text: chunk.text })),\n    });\n\n    ```\n    ### Using MongoDB Atlas Vector search\n\n    For detailed setup instructions and best practices, see the [official MongoDB Atlas Vector Search documentation](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-overview/?utm_campaign=devrel&utm_source=third-party-content&utm_medium=cta&utm_content=mastra-docs). \n  </Tabs.Tab>\n\n  <Tabs.Tab>\n  ```ts filename=\"vector-store.ts\" showLineNumbers copy\n  import { PgVector } from '@mastra/pg';\n\n  const store = new PgVector({ connectionString: process.env.POSTGRES_CONNECTION_STRING })\n\n  await store.createIndex({\n    indexName: \"myCollection\",\n    dimension: 1536,\n  });\n\n  await store.upsert({\n    indexName: \"myCollection\",\n    vectors: embeddings,\n    metadata: chunks.map(chunk => ({ text: chunk.text })),\n  });\n```\n\n### Using PostgreSQL with pgvector\n\nPostgreSQL with the pgvector extension is a good solution for teams already using PostgreSQL who want to minimize infrastructure complexity.\nFor detailed setup instructions and best practices, see the [official pgvector repository](https://github.com/pgvector/pgvector).\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { PineconeVector } from '@mastra/pinecone'\n\nconst store = new PineconeVector({\n  apiKey: process.env.PINECONE_API_KEY,\n})\nawait store.createIndex({\n  indexName: \"myCollection\",\n  dimension: 1536,\n});\nawait store.upsert({\n  indexName: \"myCollection\",\n  vectors: embeddings,\n  metadata: chunks.map(chunk => ({ text: chunk.text })),\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n  ```ts filename=\"vector-store.ts\" showLineNumbers copy\n  import { QdrantVector } from '@mastra/qdrant'\n\n  const store = new QdrantVector({\n    url: process.env.QDRANT_URL,\n    apiKey: process.env.QDRANT_API_KEY\n  })\n\n  await store.createIndex({\n    indexName: \"myCollection\",\n    dimension: 1536,\n  });\n\n  await store.upsert({\n    indexName: \"myCollection\",\n    vectors: embeddings,\n    metadata: chunks.map(chunk => ({ text: chunk.text })),\n  });\n\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { ChromaVector } from '@mastra/chroma'\n\nconst store = new ChromaVector()\n\nawait store.createIndex({\n  indexName: \"myCollection\",\n  dimension: 1536,\n});\n\nawait store.upsert({\n  indexName: \"myCollection\",\n  vectors: embeddings,\n  metadata: chunks.map(chunk => ({ text: chunk.text })),\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n  ```ts filename=\"vector-store.ts\" showLineNumbers copy\n  import { AstraVector } from '@mastra/astra'\n\n  const store = new AstraVector({\n    token: process.env.ASTRA_DB_TOKEN,\n    endpoint: process.env.ASTRA_DB_ENDPOINT,\n    keyspace: process.env.ASTRA_DB_KEYSPACE\n  })\n\n  await store.createIndex({\n    indexName: \"myCollection\",\n    dimension: 1536,\n  });\n\n  await store.upsert({\n    indexName: \"myCollection\",\n    vectors: embeddings,\n    metadata: chunks.map(chunk => ({ text: chunk.text })),\n  });\n\n```\n</Tabs.Tab>\n\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { LibSQLVector } from \"@mastra/core/vector/libsql\";\n\nconst store = new LibSQLVector({\n  connectionUrl: process.env.DATABASE_URL,\n  authToken: process.env.DATABASE_AUTH_TOKEN // Optional: for Turso cloud databases\n})\n\nawait store.createIndex({\n  indexName: \"myCollection\",\n  dimension: 1536,\n});\n\nawait store.upsert({\n  indexName: \"myCollection\",\n  vectors: embeddings,\n  metadata: chunks.map(chunk => ({ text: chunk.text })),\n});\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n  ```ts filename=\"vector-store.ts\" showLineNumbers copy\n  import { UpstashVector } from '@mastra/upstash'\n\n  // In upstash they refer to the store as an index\n  const store = new UpstashVector({\n    url: process.env.UPSTASH_URL,\n    token: process.env.UPSTASH_TOKEN\n  })\n\n  // There is no store.createIndex call here, Upstash creates indexes (known as namespaces in Upstash) automatically\n  // when you upsert if that namespace does not exist yet.\n  await store.upsert({\n    indexName: \"myCollection\", // the namespace name in Upstash\n    vectors: embeddings,\n    metadata: chunks.map(chunk => ({ text: chunk.text })),\n  });\n```\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { CloudflareVector } from '@mastra/vectorize'\n\nconst store = new CloudflareVector({\n  accountId: process.env.CF_ACCOUNT_ID,\n  apiToken: process.env.CF_API_TOKEN\n})\nawait store.createIndex({\n  indexName: \"myCollection\",\n  dimension: 1536,\n});\nawait store.upsert({\n  indexName: \"myCollection\",\n  vectors: embeddings,\n  metadata: chunks.map(chunk => ({ text: chunk.text })),\n});\n```\n\n</Tabs.Tab>\n\n<Tabs.Tab>\n```ts filename=\"vector-store.ts\" showLineNumbers copy\nimport { OpenSearchVector } from '@mastra/opensearch'\n\nconst store = new OpenSearchVector({ url: process.env.OPENSEARCH_URL })\n\nawait store.createIndex({\n  indexName: \"my-collection\",\n  dimension: 1536,\n});\n\nawait store.upsert({\n  indexName: \"my-collection\",\n  vectors: embeddings,\n  metadata: chunks.map(chunk => ({ text: chunk.text })),\n});\n```\n\n</Tabs.Tab>\n<Tabs.Tab>\n  ```ts filename=\"vector-store.ts\" showLineNumbers copy\n  import { CouchbaseVector } from '@mastra/couchbase'\n\n  const store = new CouchbaseVector({\n    connectionString: process.env.COUCHBASE_CONNECTION_STRING,\n    username: process.env.COUCHBASE_USERNAME,\n    password: process.env.COUCHBASE_PASSWORD,\n    bucketName: process.env.COUCHBASE_BUCKET,\n    scopeName: process.env.COUCHBASE_SCOPE,\n    collectionName: process.env.COUCHBASE_COLLECTION,\n  })\n  await store.createIndex({\n    indexName: \"myCollection\",\n    dimension: 1536,\n  });\n  await store.upsert({\n    indexName: \"myCollection\",\n    vectors: embeddings,\n    metadata: chunks.map(chunk => ({ text: chunk.text })),\n  });\n  ```\n</Tabs.Tab>\n<Tabs.Tab>\n  ```ts filename=\"vector-store.ts\" showLineNumbers copy\n  import { LanceVectorStore } from '@mastra/lance'\n\n  const store = await LanceVectorStore.create('/path/to/db')\n  \n  await store.createIndex({\n    tableName: \"myVectors\",\n    indexName: \"myCollection\",\n    dimension: 1536,\n  });\n  \n  await store.upsert({\n    tableName: \"myVectors\",\n    vectors: embeddings,\n    metadata: chunks.map(chunk => ({ text: chunk.text })),\n  });\n  ```\n\n  ### Using LanceDB\n  \n  LanceDB is an embedded vector database built on the Lance columnar format, suitable for local development or cloud deployment.\n  For detailed setup instructions and best practices, see the [official LanceDB documentation](https://lancedb.github.io/lancedb/).\n</Tabs.Tab>\n</Tabs>\n\n## Using Vector Storage\n\nOnce initialized, all vector stores share the same interface for creating indexes, upserting embeddings, and querying.\n\n### Creating Indexes\n\nBefore storing embeddings, you need to create an index with the appropriate dimension size for your embedding model:\n\n```ts filename=\"store-embeddings.ts\" showLineNumbers copy\n// Create an index with dimension 1536 (for text-embedding-3-small)\nawait store.createIndex({\n  indexName: \"myCollection\",\n  dimension: 1536,\n});\n```\n\nThe dimension size must match the output dimension of your chosen embedding model. Common dimension sizes are:\n\n- OpenAI text-embedding-3-small: 1536 dimensions (or custom, e.g., 256)\n- Cohere embed-multilingual-v3: 1024 dimensions\n- Google `text-embedding-004`: 768 dimensions (or custom)\n\n> **Important**: Index dimensions cannot be changed after creation. To use a different model, delete and recreate the index with the new dimension size.\n\n### Naming Rules for Databases\n\nEach vector database enforces specific naming conventions for indexes and collections to ensure compatibility and prevent conflicts.\n\n{/*\nLLM CONTEXT: This Tabs component displays naming convention rules for different vector databases.\nEach tab explains the specific naming requirements and restrictions for that database provider.\nThis helps users understand the constraints and avoid naming conflicts when creating indexes or collections.\nThe tabs provide examples of valid and invalid names to clarify the rules for each database.\n*/}\n\n<Tabs items={['MongoDB', 'Pg Vector', 'Pinecone', 'Qdrant', 'Chroma', 'Astra', 'LibSQL', 'Upstash', 'Cloudflare', 'OpenSearch']}>\n  <Tabs.Tab>\n    Collection (index) names must:\n    - Start with a letter or underscore\n    - Be up to 120 bytes long\n    - Contain only letters, numbers, underscores, or dots\n    - Cannot contain `$` or the null character\n    - Example: `my_collection.123` is valid\n    - Example: `my-index` is not valid (contains hyphen)\n    - Example: `My$Collection` is not valid (contains `$`)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Index names must:\n    - Start with a letter or underscore\n    - Contain only letters, numbers, and underscores\n    - Example: `my_index_123` is valid\n    - Example: `my-index` is not valid (contains hyphen)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Index names must:\n    - Use only lowercase letters, numbers, and dashes\n    - Not contain dots (used for DNS routing)\n    - Not use non-Latin characters or emojis\n    - Have a combined length (with project ID) under 52 characters\n      - Example: `my-index-123` is valid\n      - Example: `my.index` is not valid (contains dot)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Collection names must:\n    - Be 1-255 characters long\n    - Not contain any of these special characters:\n      - `< > : \" / \\ | ? *`\n      - Null character (`\\0`)\n      - Unit separator (`\\u{1F}`)\n    - Example: `my_collection_123` is valid\n    - Example: `my/collection` is not valid (contains slash)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Collection names must:\n    - Be 3-63 characters long\n    - Start and end with a letter or number\n    - Contain only letters, numbers, underscores, or hyphens\n    - Not contain consecutive periods (..)\n    - Not be a valid IPv4 address\n    - Example: `my-collection-123` is valid\n    - Example: `my..collection` is not valid (consecutive periods)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Collection names must:\n    - Not be empty\n    - Be 48 characters or less\n    - Contain only letters, numbers, and underscores\n    - Example: `my_collection_123` is valid\n    - Example: `my-collection` is not valid (contains hyphen)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Index names must:\n    - Start with a letter or underscore\n    - Contain only letters, numbers, and underscores\n    - Example: `my_index_123` is valid\n    - Example: `my-index` is not valid (contains hyphen)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Namespace names must:\n    - Be 2-100 characters long\n    - Contain only:\n      - Alphanumeric characters (a-z, A-Z, 0-9)\n      - Underscores, hyphens, dots\n    - Not start or end with special characters (_, -, .)\n    - Can be case-sensitive\n    - Example: `MyNamespace123` is valid\n    - Example: `_namespace` is not valid (starts with underscore)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Index names must:\n    - Start with a letter\n    - Be shorter than 32 characters\n    - Contain only lowercase ASCII letters, numbers, and dashes\n    - Use dashes instead of spaces\n    - Example: `my-index-123` is valid\n    - Example: `My_Index` is not valid (uppercase and underscore)\n  </Tabs.Tab>\n  <Tabs.Tab>\n    Index names must:\n    - Use only lowercase letters\n    - Not begin with underscores or hyphens\n    - Not contain spaces, commas\n    - Not contain special characters (e.g. `:`, `\"`, `*`, `+`, `/`, `\\`, `|`, `?`, `#`, `>`, `<`)\n    - Example: `my-index-123` is valid\n    - Example: `My_Index` is not valid (contains uppercase letters)\n    - Example: `_myindex` is not valid (begins with underscore)\n  </Tabs.Tab>\n</Tabs>\n\n### Upserting Embeddings\n\nAfter creating an index, you can store embeddings along with their basic metadata:\n\n```ts filename=\"store-embeddings.ts\" showLineNumbers copy\n// Store embeddings with their corresponding metadata\nawait store.upsert({\n  indexName: \"myCollection\", // index name\n  vectors: embeddings, // array of embedding vectors\n  metadata: chunks.map((chunk) => ({\n    text: chunk.text, // The original text content\n    id: chunk.id, // Optional unique identifier\n  })),\n});\n```\n\nThe upsert operation:\n\n- Takes an array of embedding vectors and their corresponding metadata\n- Updates existing vectors if they share the same ID\n- Creates new vectors if they don't exist\n- Automatically handles batching for large datasets\n\nFor complete examples of upserting embeddings in different vector stores, see the [Upsert Embeddings](../../examples/rag/upsert/upsert-embeddings.mdx) guide.\n\n## Adding Metadata\n\nVector stores support rich metadata (any JSON-serializable fields) for filtering and organization. Since metadata is stored with no fixed schema, use consistent field naming to avoid unexpected query results.\n\n**Important**: Metadata is crucial for vector storage - without it, you'd only have numerical embeddings with no way to return the original text or filter results. Always store at least the source text as metadata.\n\n```ts showLineNumbers copy\n// Store embeddings with rich metadata for better organization and filtering\nawait store.upsert({\n  indexName: \"myCollection\",\n  vectors: embeddings,\n  metadata: chunks.map((chunk) => ({\n    // Basic content\n    text: chunk.text,\n    id: chunk.id,\n\n    // Document organization\n    source: chunk.source,\n    category: chunk.category,\n\n    // Temporal metadata\n    createdAt: new Date().toISOString(),\n    version: \"1.0\",\n\n    // Custom fields\n    language: chunk.language,\n    author: chunk.author,\n    confidenceScore: chunk.score,\n  })),\n});\n```\n\nKey metadata considerations:\n\n- Be strict with field naming - inconsistencies like 'category' vs 'Category' will affect queries\n- Only include fields you plan to filter or sort by - extra fields add overhead\n- Add timestamps (e.g., 'createdAt', 'lastUpdated') to track content freshness\n\n## Best Practices\n\n- Create indexes before bulk insertions\n- Use batch operations for large insertions (the upsert method handles batching automatically)\n- Only store metadata you'll query against\n- Match embedding dimensions to your model (e.g., 1536 for `text-embedding-3-small`)\n\n\n---\ntitle: \"Advanced Tool Usage | Tools & MCP | Mastra Docs\"\ndescription: This page covers advanced features for Mastra tools, including abort signals and compatibility with the Vercel AI SDK tool format.\n---\n\n# Advanced Tool Usage\n[EN] Source: https://mastra.ai/en/docs/tools-mcp/advanced-usage\n\nThis page covers more advanced techniques and features related to using tools in Mastra.\n\n## Abort Signals\n\nWhen you initiate an agent interaction using `generate()` or `stream()`, you can provide an `AbortSignal`. Mastra automatically forwards this signal to any tool executions that occur during that interaction.\n\nThis allows you to cancel long-running operations within your tools, such as network requests or intensive computations, if the parent agent call is aborted.\n\nYou access the `abortSignal` in the second parameter of the tool's `execute` function.\n\n```typescript\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nexport const longRunningTool = createTool({\n  id: \"long-computation\",\n  description: \"Performs a potentially long computation\",\n  inputSchema: z.object({ /* ... */ }),\n  execute: async ({ context }, { abortSignal }) => {\n    // Example: Forwarding signal to fetch\n    const response = await fetch(\"https://api.example.com/data\", {\n      signal: abortSignal, // Pass the signal here\n    });\n\n    if (abortSignal?.aborted) {\n      console.log(\"Tool execution aborted.\");\n      throw new Error(\"Aborted\");\n    }\n\n    // Example: Checking signal during a loop\n    for (let i = 0; i < 1000000; i++) {\n      if (abortSignal?.aborted) {\n        console.log(\"Tool execution aborted during loop.\");\n        throw new Error(\"Aborted\");\n      }\n      // ... perform computation step ...\n    }\n\n    const data = await response.json();\n    return { result: data };\n  },\\n});\n```\n\nTo use this, provide an `AbortController`'s signal when calling the agent:\n\n```typescript\nimport { Agent } from \"@mastra/core/agent\";\n// Assume 'agent' is an Agent instance with longRunningTool configured\n\nconst controller = new AbortController();\n\n// Start the agent call\nconst promise = agent.generate(\"Perform the long computation.\", {\n  abortSignal: controller.signal,\n});\n\n// Sometime later, if needed:\n// controller.abort();\n\ntry {\n  const result = await promise;\n  console.log(result.text);\n} catch (error) {\n  if (error.name === \"AbortError\") {\n    console.log(\"Agent generation was aborted.\");\n  } else {\n    console.error(\"An error occurred:\", error);\n  }\n}\n```\n\n## AI SDK Tool Format\n\nMastra maintains compatibility with the tool format used by the Vercel AI SDK (`ai` package). You can define tools using the `tool` function from the `ai` package and use them directly within your Mastra agents alongside tools created with Mastra's `createTool`.\n\nFirst, ensure you have the `ai` package installed:\n\n```bash npm2yarn copy\nnpm install ai\n```\n\nHere's an example of a tool defined using the Vercel AI SDK format:\n\n```typescript filename=\"src/mastra/tools/vercelWeatherTool.ts\" copy\nimport { tool } from \"ai\";\nimport { z } from \"zod\";\n\nexport const vercelWeatherTool = tool({\n  description: \"Fetches current weather using Vercel AI SDK format\",\n  parameters: z.object({\n    city: z.string().describe(\"The city to get weather for\"),\n  }),\n  execute: async ({ city }) => {\n    console.log(`Fetching weather for ${city} (Vercel format tool)`);\n    // Replace with actual API call\n    const data = await fetch(`https://api.example.com/weather?city=${city}`);\n    return data.json();\n  },\n});\n```\n\nYou can then add this tool to your Mastra agent just like any other tool:\n\n```typescript filename=\"src/mastra/agents/mixedToolsAgent.ts\"\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { vercelWeatherTool } from \"../tools/vercelWeatherTool\"; // Vercel AI SDK tool\nimport { mastraTool } from \"../tools/mastraTool\"; // Mastra createTool tool\n\nexport const mixedToolsAgent = new Agent({\n  name: \"Mixed Tools Agent\",\n  instructions: \"You can use tools defined in different formats.\",\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    weatherVercel: vercelWeatherTool,\n    someMastraTool: mastraTool,\n  },\n});\n```\n\nMastra supports both tool formats, allowing you to mix and match as needed.\n\n\n---\ntitle: \"Dynamic Tool Context | Tools & MCP | Mastra Docs\"\ndescription: Learn how to use Mastra's RuntimeContext to provide dynamic, request-specific configuration to tools.\n---\n\nimport { Callout } from \"nextra/components\";\n\n# Dynamic Tool Context\n[EN] Source: https://mastra.ai/en/docs/tools-mcp/dynamic-context\n\nMastra provides `RuntimeContext`, a system based on dependency injection, that allows you to pass dynamic, request-specific configuration to your tools during execution. This is useful when a tool's behavior needs to change based on user identity, request headers, or other runtime factors, without altering the tool's core code.\n\n<Callout>\n  **Note:** `RuntimeContext` is primarily used for passing data *into* tool\n  executions. It's distinct from agent memory, which handles conversation\n  history and state persistence across multiple calls.\n</Callout>\n\n## Basic Usage\n\nTo use `RuntimeContext`, first define a type structure for your dynamic configuration. Then, create an instance of `RuntimeContext` typed with your definition and set the desired values. Finally, include the `runtimeContext` instance in the options object when calling `agent.generate()` or `agent.stream()`.\n\n```typescript\nimport { RuntimeContext } from \"@mastra/core/di\";\n// Assume 'agent' is an already defined Mastra Agent instance\n\n// Define the context type\ntype WeatherRuntimeContext = {\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\";\n};\n\n// Instantiate RuntimeContext and set values\nconst runtimeContext = new RuntimeContext<WeatherRuntimeContext>();\nruntimeContext.set(\"temperature-scale\", \"celsius\");\n\n// Pass to agent call\nconst response = await agent.generate(\"What's the weather like today?\", {\n  runtimeContext, // Pass the context here\n});\n\nconsole.log(response.text);\n```\n\n## Accessing Context in Tools\n\nTools receive the `runtimeContext` as part of the second argument to their `execute` function. You can then use the `.get()` method to retrieve values.\n\n```typescript filename=\"src/mastra/tools/weather-tool.ts\"\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n// Assume WeatherRuntimeContext is defined as above and accessible here\n\n// Dummy fetch function\nasync function fetchWeather(\n  location: string,\n  options: { temperatureUnit: \"celsius\" | \"fahrenheit\" },\n): Promise<any> {\n  console.log(`Fetching weather for ${location} in ${options.temperatureUnit}`);\n  // Replace with actual API call\n  return { temperature: options.temperatureUnit === \"celsius\" ? 20 : 68 };\n}\n\nexport const weatherTool = createTool({\n  id: \"getWeather\",\n  description: \"Get the current weather for a location\",\n  inputSchema: z.object({\n    location: z.string().describe(\"The location to get weather for\"),\n  }),\n  // The tool's execute function receives runtimeContext\n  execute: async ({ context, runtimeContext }) => {\n    // Type-safe access to runtimeContext variables\n    const temperatureUnit = runtimeContext.get(\"temperature-scale\");\n\n    // Use the context value in the tool logic\n    const weather = await fetchWeather(context.location, {\n      temperatureUnit,\n    });\n\n    return {\n      result: `The temperature is ${weather.temperature}°${temperatureUnit === \"celsius\" ? \"C\" : \"F\"}`,\n    };\n  },\n});\n```\n\nWhen the agent uses `weatherTool`, the `temperature-scale` value set in the `runtimeContext` during the `agent.generate()` call will be available inside the tool's `execute` function.\n\n## Using with Server Middleware\n\nIn server environments (like Express or Next.js), you can use middleware to automatically populate `RuntimeContext` based on incoming request data, such as headers or user sessions.\n\nHere's an example using Mastra's built-in server middleware support (which uses Hono internally) to set the temperature scale based on the Cloudflare `CF-IPCountry` header:\n\n```typescript filename=\"src/mastra/index.ts\"\nimport { Mastra } from \"@mastra/core\";\nimport { RuntimeContext } from \"@mastra/core/di\";\nimport { weatherAgent } from \"./agents/weather\"; // Assume agent is defined elsewhere\n\n// Define RuntimeContext type\ntype WeatherRuntimeContext = {\n  \"temperature-scale\": \"celsius\" | \"fahrenheit\";\n};\n\nexport const mastra = new Mastra({\n  agents: {\n    weather: weatherAgent,\n  },\n  server: {\n    middleware: [\n      async (c, next) => {\n        // Get the RuntimeContext instance\n        const runtimeContext =\n          c.get<RuntimeContext<WeatherRuntimeContext>>(\"runtimeContext\");\n\n        // Get country code from request header\n        const country = c.req.header(\"CF-IPCountry\");\n\n        // Set temperature scale based on country\n        runtimeContext.set(\n          \"temperature-scale\",\n          country === \"US\" ? \"fahrenheit\" : \"celsius\",\n        );\n\n        // Continue request processing\n        await next();\n      },\n    ],\n  },\n});\n```\n\nWith this middleware in place, any agent call handled by this Mastra server instance will automatically have the `temperature-scale` set in its `RuntimeContext` based on the user's inferred country, and tools like `weatherTool` will use it accordingly.\n\n\n---\ntitle: \"MCP Overview | Tools & MCP | Mastra Docs\"\ndescription: Learn about the Model Context Protocol (MCP), how to use third-party tools via MCPClient, connect to registries, and share your own tools using MCPServer.\n---\n\nimport { Tabs } from \"nextra/components\";\n\n# MCP Overview\n[EN] Source: https://mastra.ai/en/docs/tools-mcp/mcp-overview\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard designed to let AI models discover and interact with external tools and resources. Think of it as a universal plugin system for AI agents, allowing them to use tools regardless of the language they were written in or where they are hosted.\n\nMastra uses MCP to connect agents to external tool servers.\n\n## Use third-party tools with an MCP Client\n\nMastra provides the `MCPClient` class to manage connections to one or more MCP servers and access their tools.\n\n### Installation\n\nIf you haven't already, install the Mastra MCP package:\n\n```bash npm2yarn copy\nnpm install @mastra/mcp@latest\n```\n\n### Registering the MCPServer\n\nRegister your MCP server with Mastra to enable logging and access to configured tools and integrations:\n\n```ts showLineNumbers filename=\"src/mastra/index.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { myMcpServer } from \"./mcpServers\";\n\nexport const mastra = new Mastra({\n  mcpServers: { myMcpServer },\n});\n```\n\n### Configuring `MCPClient`\n\nYou configure `MCPClient` with a map of servers you want to connect to. It supports connections via subprocess (Stdio) or HTTP (Streamable HTTP with SSE fallback).\n\n```typescript\nimport { MCPClient } from \"@mastra/mcp\";\n\nconst mcp = new MCPClient({\n  servers: {\n    // Stdio example\n    sequential: {\n      command: \"npx\",\n      args: [\"-y\", \"@modelcontextprotocol/server-sequential-thinking\"],\n    },\n    // HTTP example\n    weather: {\n      url: new URL(\"http://localhost:8080/mcp\"),\n      requestInit: {\n        headers: {\n          Authorization: \"Bearer your-token\",\n        },\n      },\n    },\n  },\n});\n```\n\nFor detailed configuration options, see the [`MCPClient` reference documentation](/reference/tools/mcp-client).\n\n### Static vs Dynamic Tool Configurations\n\n`MCPClient` offers two approaches to retrieving tools from connected servers, suitable for different application architectures:\n\n| Feature           | Static Configuration (`await mcp.getTools()`) | Dynamic Configuration (`await mcp.getToolsets()`)  |\n| :---------------- | :-------------------------------------------- | :------------------------------------------------- |\n| **Use Case**      | Single-user, static config (e.g., CLI tool)   | Multi-user, dynamic config (e.g., SaaS app)        |\n| **Configuration** | Fixed at agent initialization                 | Per-request, dynamic                               |\n| **Credentials**   | Shared across all uses                        | Can vary per user/request                          |\n| **Agent Setup**   | Tools added in `Agent` constructor            | Tools passed in `generate()` or `stream()` options |\n\n- **Static Configuration (`getTools()`):** Fetches all tools from all configured servers. Best when the tool configuration (like API keys) is static and shared across all users or requests. You typically call this once and pass the result to the `tools` property when defining your `Agent`.\n  [Reference: `getTools()`](/reference/tools/mcp-client#gettools)\n\n  ```typescript\n  import { Agent } from \"@mastra/core/agent\";\n  // ... mcp client setup\n\n  const agent = new Agent({\n    // ... other agent config\n    tools: await mcp.getTools(),\n  });\n  ```\n\n- **Dynamic Configuration (`getToolsets()`):** Designed for scenarios where configuration might change per request or per user (e.g., different API keys for different tenants in a multi-user application). You pass the result of `getToolsets()` to the `toolsets` option in the agent's `generate()` or `stream()` method.\n  [Reference: `getToolsets()`](/reference/tools/mcp-client#gettoolsets)\n\n  ```typescript\n  import { Agent } from \"@mastra/core/agent\";\n  // ... agent setup without tools initially\n\n  async function handleRequest(userPrompt: string, userApiKey: string) {\n    const userMcp = new MCPClient({\n      /* config with userApiKey */\n    });\n    const toolsets = await userMcp.getToolsets();\n\n    const response = await agent.stream(userPrompt, {\n      toolsets, // Pass dynamic toolsets\n    });\n    // ... handle response\n    await userMcp.disconnect();\n  }\n  ```\n\n## Connecting to an MCP registry\n\nMCP servers can be discovered through registries. Here's how to connect to some popular ones using `MCPClient`:\n\n\n{/*\nLLM CONTEXT: This Tabs component shows how to connect to different MCP (Model Context Protocol) registries.\nEach tab demonstrates the configuration for a specific MCP registry service (mcp.run, Composio.dev, Smithery.ai).\nThe tabs help users understand how to connect to various MCP server providers and their different authentication methods.\nEach tab shows the specific URL patterns and configuration needed for that registry service.\n*/}\n\n<Tabs items={[\"mcp.run\", \"Composio.dev\", \"Smithery.ai\", \"Ampersand\"]}>\n  <Tabs.Tab>\n    [mcp.run](https://www.mcp.run/) provides pre-authenticated, managed MCP servers. Tools are grouped into Profiles, each with a unique, signed URL.\n\n    ```typescript\n    import { MCPClient } from \"@mastra/mcp\";\n\n    const mcp = new MCPClient({\n      servers: {\n        marketing: { // Example profile name\n          url: new URL(process.env.MCP_RUN_SSE_URL!), // Get URL from mcp.run profile\n        },\n      },\n    });\n    ```\n\n    > **Important:** Treat the mcp.run SSE URL like a password. Store it securely, for example, in an environment variable.\n    > ```bash filename=\".env\"\n    > MCP_RUN_SSE_URL=https://www.mcp.run/api/mcp/sse?nonce=...\n    > ```\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n    [Composio.dev](https://composio.dev) offers a registry of [SSE-based MCP servers](https://mcp.composio.dev). You can use the SSE URL generated for tools like Cursor directly.\n\n    ```typescript\n    import { MCPClient } from \"@mastra/mcp\";\n\n    const mcp = new MCPClient({\n      servers: {\n        googleSheets: {\n          url: new URL(\"https://mcp.composio.dev/googlesheets/[private-url-path]\"),\n        },\n        gmail: {\n          url: new URL(\"https://mcp.composio.dev/gmail/[private-url-path]\"),\n        },\n      },\n    });\n    ```\n\n    Authentication with services like Google Sheets often happens interactively through the agent conversation.\n\n    *Note: Composio URLs are typically tied to a single user account, making them best suited for personal automation rather than multi-tenant applications.*\n\n  </Tabs.Tab>\n  <Tabs.Tab>\n    [Smithery.ai](https://smithery.ai) provides a registry accessible via their CLI.\n\n    ```typescript\n    // Unix/Mac\n    import { MCPClient } from \"@mastra/mcp\";\n\n    const mcp = new MCPClient({\n      servers: {\n        sequentialThinking: {\n          command: \"npx\",\n          args: [\n            \"-y\",\n            \"@smithery/cli@latest\",\n            \"run\",\n            \"@smithery-ai/server-sequential-thinking\",\n            \"--config\",\n            \"{}\",\n          ],\n        },\n      },\n    });\n    ```\n\n    ```typescript\n    // Windows\n    import { MCPClient } from \"@mastra/mcp\";\n\n    const mcp = new MCPClient({\n      servers: {\n        sequentialThinking: {\n          command: \"npx\",\n          args: [\n            \"-y\",\n            \"@smithery/cli@latest\",\n            \"run\",\n            \"@smithery-ai/server-sequential-thinking\",\n            \"--config\",\n            \"{}\",\n          ],\n        },\n      },\n    });\n    ```\n\n  </Tabs.Tab>\n    <Tabs.Tab>\n\n    [Ampersand](https://withampersand.com?utm_source=mastra-docs) offers an [MCP Server](https://docs.withampersand.com/mcp) that allows you to connect your agent to 150+ integrations with SaaS products like Salesforce, Hubspot, and Zendesk.\n    \n\n    ```typescript\n\n    // MCPClient with Ampersand MCP Server using SSE\n    export const mcp = new MCPClient({\n        servers: {\n        \"@amp-labs/mcp-server\": {\n          \"url\": `https://mcp.withampersand.com/v1/sse?${new URLSearchParams({\n            apiKey: process.env.AMPERSAND_API_KEY,\n            project: process.env.AMPERSAND_PROJECT_ID,\n            integrationName: process.env.AMPERSAND_INTEGRATION_NAME,\n            groupRef: process.env.AMPERSAND_GROUP_REF\n          })}`\n        }\n      }\n    });\n\n    ```\n\n    ```typescript\n    // If you prefer to run the MCP server locally:\n    \n    import { MCPClient } from \"@mastra/mcp\";\n\n    // MCPClient with Ampersand MCP Server using stdio transport\n    export const mcp = new MCPClient({\n        servers: {\n          \"@amp-labs/mcp-server\": {\n            command: \"npx\",\n            args: [\n              \"-y\",\n              \"@amp-labs/mcp-server@latest\",\n              \"--transport\",\n              \"stdio\",\n              \"--project\",\n              process.env.AMPERSAND_PROJECT_ID,\n              \"--integrationName\",\n              process.env.AMPERSAND_INTEGRATION_NAME,\n              \"--groupRef\",\n              process.env.AMPERSAND_GROUP_REF, // optional\n            ],\n            env: {\n              AMPERSAND_API_KEY: process.env.AMPERSAND_API_KEY,\n            },\n          },\n        },\n    });\n    ```\n\n    As an alternative to MCP, Ampersand's AI SDK also has an adapter for Mastra, so you can [directly import Ampersand tools](https://docs.withampersand.com/ai-sdk#use-with-mastra) for your agent to access.\n\n  </Tabs.Tab>\n</Tabs>\n\n## Share your tools with an MCP server\n\nIf you have created your own Mastra tools, you can expose them to any MCP-compatible client using Mastra's `MCPServer` class.\n\nSimilarly, Mastra `Agent` and `Workflow` instances can also be exposed as tools via `MCPServer`. This allows other MCP clients to interact with your agents by \"asking\" them questions or run your workflows. Each agent provided in the `MCPServer` configuration will be converted into a tool named `ask_<agentKey>`, using the agent's `description` property. Each workflow will be converted into a tool named `run_<workflowKey>`, using its `inputSchema` and `description`.\n\nThis allows others to use your tools, agents, and workflows without needing direct access to your codebase.\n\n### Using `MCPServer`\n\nYou initialize `MCPServer` with a name, version, and the Mastra tools, agents, and/or workflows you want to share.\n\n```typescript\nimport { MCPServer } from \"@mastra/mcp\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { weatherTool } from \"./tools\"; // Your Mastra tool\nimport { weatherAgent } from \"./agents\"; // Your Mastra Agent\nimport { dataWorkflow } from \"./workflows\"; // Your Mastra Workflow\n\nconst server = new MCPServer({\n  name: \"My Custom Server\",\n  version: \"1.0.0\",\n  tools: { weatherTool }, // Provide your tool(s) here\n  agents: { weatherAgent }, // Provide your agent(s) here\n  workflows: { dataWorkflow }, // Provide your workflow(s) here\n});\n\n// Start the server (e.g., using stdio for a CLI tool)\n// await server.startStdio();\n\n// Or integrate with an HTTP server using startSSE()\n// See MCPServer reference for details\n```\n\nFor an agent to be exposed as a tool, it must have a non-empty `description` string. Similarly, for a workflow to be exposed, its `description` must also be a non-empty string. If the description is missing or empty for either, `MCPServer` will throw an error during initialization.\nWorkflows will use their `inputSchema` for the tool's input.\n\n### Tools with Structured Outputs\n\nYou can define an `outputSchema` for your tools to enforce a specific structure for the tool's output. This is useful for ensuring that the tool returns data in a consistent and predictable format, which can then be validated by the client.\n\nWhen a tool includes an `outputSchema`, its `execute` function **must** return an object. The value of the object must conform to the `outputSchema`. Mastra will automatically validate this output on both the server and client sides.\n\nHere's an example of a tool with an `outputSchema`:\n\n```typescript filename=\"src/tools/structured-tool.ts\"\nimport { createTool } from '@mastra/core';\nimport { z } from 'zod';\n\nexport const structuredTool = createTool({\n  description: 'A test tool that returns structured data.',\n  parameters: z.object({\n    input: z.string().describe('Some input string.'),\n  }),\n  outputSchema: z.object({\n    processedInput: z.string().describe('The processed input string.'),\n    timestamp: z.string().describe('An ISO timestamp.'),\n  }),\n  execute: async ({ input }) => {\n    // When outputSchema is defined, you must return an object\n    return {\n      processedInput: `processed: ${input}`,\n      timestamp: new Date().toISOString(),\n    };\n  },\n});\n```\n\nWhen this tool is called, the MCP client will receive both the structured data and a text representation of it.\n\n```\nTool result\n\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"{\\\"processedInput\\\": \\\"hello\\\", \\\"timestamp\\\": \\\"2025-06-19T16:53:16.472Z\\\"}\"\n    }\n  ],\n  \"structuredContent\": {\n    \"processedInput\": \"processed: hello\",\n    \"timestamp\": \"2025-06-19T16:53:16.472Z\",\n  }\n}\n```\n\nFor detailed usage and examples, see the [`MCPServer` reference documentation](/reference/tools/mcp-server).\n\n\n---\ntitle: \"Tools Overview | Tools & MCP | Mastra Docs\"\ndescription: Understand what tools are in Mastra, how to add them to agents, and best practices for designing effective tools.\n---\n\n# Tools Overview\n[EN] Source: https://mastra.ai/en/docs/tools-mcp/overview\n\nTools are functions that agents can execute to perform specific tasks or access external information. They extend an agent's capabilities beyond simple text generation, allowing interaction with APIs, databases, or other systems.\n\nEach tool typically defines:\n\n- **Inputs:** What information the tool needs to run (defined with an `inputSchema`, often using Zod).\n- **Outputs:** The structure of the data the tool returns (defined with an `outputSchema`).\n- **Execution Logic:** The code that performs the tool's action.\n- **Description:** Text that helps the agent understand what the tool does and when to use it.\n\n## Creating Tools\n\nIn Mastra, you create tools using the [`createTool`](/reference/tools/create-tool) function from the `@mastra/core/tools` package.\n\n```typescript filename=\"src/mastra/tools/weatherInfo.ts\" copy\nimport { createTool } from \"@mastra/core/tools\";\nimport { z } from \"zod\";\n\nconst getWeatherInfo = async (city: string) => {\n  // Replace with an actual API call to a weather service\n  console.log(`Fetching weather for ${city}...`);\n  // Example data structure\n  return { temperature: 20, conditions: \"Sunny\" };\n};\n\nexport const weatherTool = createTool({\n  id: \"Get Weather Information\",\n  description: `Fetches the current weather information for a given city`,\n  inputSchema: z.object({\n    city: z.string().describe(\"City name\"),\n  }),\n  outputSchema: z.object({\n    temperature: z.number(),\n    conditions: z.string(),\n  }),\n  execute: async ({ context: { city } }) => {\n    console.log(\"Using tool to fetch weather information for\", city);\n    return await getWeatherInfo(city);\n  },\n});\n```\n\nThis example defines a `weatherTool` with an input schema for the city, an output schema for the weather data, and an `execute` function that contains the tool's logic.\n\nWhen creating tools, keep tool descriptions simple and focused on **what** the tool does and **when** to use it, emphasizing its primary use case. Technical details belong in the parameter schemas, guiding the agent on _how_ to use the tool correctly with descriptive names, clear descriptions, and explanations of default values.\n\n## Adding Tools to an Agent\n\nTo make tools available to an agent, you configure them in the agent's definition. Mentioning available tools and their general purpose in the agent's system prompt can also improve tool usage. For detailed steps and examples, see the guide on [Using Tools and MCP with Agents](/docs/agents/using-tools-and-mcp#add-tools-to-an-agent).\n\n## Compatibility Layer for Tool Schemas\n\nDifferent models interpret schemas differely. Some error when certain schema properties are passed and some ignore certain schema properties but don't throw an error. Mastra adds a compatibility layer for tool schemas, ensuring tools work consistently across different model providers and that the schema constraints are respected.\n\nSome providers that we include this layer for:\n\n- **Google Gemini & Anthropic:** Remove unsupported schema properties and append relevant constraints to the tool description.\n- **OpenAI (including reasoning models):** Strip or adapt schema fields that are ignored or unsupported, and add instructions to the description for agent guidance.\n- **DeepSeek & Meta:** Apply similar compatibility logic to ensure schema alignment and tool usability.\n\nThis approach makes tool usage more reliable and model-agnostic for both custom and MCP tools.\n\n\n---\ntitle: \"Branching, Merging, Conditions | Workflows | Mastra Docs\"\ndescription: \"Control flow in Mastra workflows allows you to manage branching, merging, and conditions to construct workflows that meet your logic requirements.\"\n---\n\n# Control Flow\n[EN] Source: https://mastra.ai/en/docs/workflows/control-flow\n\nWhen you build a workflow, you typically break down operations into smaller tasks that can be linked and reused. **Steps** provide a structured way to manage these tasks by defining inputs, outputs, and execution logic.\n\n- If the schemas match, the `outputSchema` from each step is automatically passed to the `inputSchema` of the next step.\n- If the schemas don't match, use [Input data mapping](./input-data-mapping.mdx) to transform the `outputSchema` into the expected `inputSchema`.\n\n## Chaining steps with `.then()`\n\nChain steps to execute sequentially using `.then()`:\n\n![Chaining steps with .then()](/image/workflows/workflows-control-flow-then.jpg)\n\n```typescript {8-9,4-5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .then(step2)\n  .commit();\n```\n\nThis does what you'd expect: it executes `step1`, then it executes `step2`.\n\n## Simultaneous steps with `.parallel()`\n\nExecute steps simultaneously using `.parallel()`:\n\n![Concurrent steps with .parallel()](/image/workflows/workflows-control-flow-parallel.jpg)\n\n```typescript {8,4-5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\nconst step3 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .parallel([step1, step2])\n  .then(step3)\n  .commit();\n```\n\nThis executes `step1` and `step2` concurrently, then continues to `step3` after both complete.\n\n> See [Parallel Execution with Steps](/examples/workflows/parallel-steps) for more information.\n\n## Conditional logic with `.branch()`\n\nExecute steps conditionally using `.branch()`:\n\n![Conditional branching with .branch()](/image/workflows/workflows-control-flow-branch.jpg)\n\n```typescript {8-11,4-5} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst lessThanStep = createStep({...});\nconst greaterThanStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .branch([\n    [async ({ inputData: { value } }) => (value < 9), lessThanStep],\n    [async ({ inputData: { value } }) => (value >= 9), greaterThanStep]\n  ])\n  .commit();\n```\n\nBranch conditions are evaluated sequentially, but steps with matching conditions are executed in parallel.\n\n> See [Workflow with Conditional Branching](/examples/workflows/conditional-branching) for more information.\n\n## Looping steps\n\nWorkflows support two types of loops. When looping a step, or any step-compatible construct like a nested workflow, the initial `inputData` is sourced from the output of the previous step.\n\nTo ensure compatibility, the loop’s initial input must either match the shape of the previous step’s output, or be explicitly transformed using the `map` function.\n\n- Match the shape of the previous step’s output, or\n- Be explicitly transformed using the `map` function.\n\n### Repeating with `.dowhile()`\n\nExecutes step repeatedly while a condition is true.\n\n![Repeating with .dowhile()](/image/workflows/workflows-control-flow-dowhile.jpg)\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .dowhile(counterStep, async ({ inputData: { number } }) => number < 10)\n  .commit();\n```\n\n### Repeating with `.dountil()`\n\nExecutes step repeatedly until a condition becomes true.\n\n![Repeating with .dountil()](/image/workflows/workflows-control-flow-dountil.jpg)\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst counterStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .dountil(counterStep, async ({ inputData: { number } }) => number > 10)\n  .commit();\n```\n\n### Repeating with `.foreach()`\n\nSequentially executes the same step for each item from the `inputSchema`.\n\n![Repeating with .foreach()](/image/workflows/workflows-control-flow-foreach.jpg)\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst mapStep = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .foreach(mapStep)\n  .commit();\n```\n\n#### Setting concurrency limits\n\nUse `concurrency` to execute steps in parallel with a limit on the number of concurrent executions.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst mapStep = createStep({...})\n\nexport const testWorkflow = createWorkflow({...})\n  .foreach(mapStep, { concurrency: 2 })\n  .commit();\n```\n\n## Using a nested workflow\n\nUse a nested workflow as a step by passing it to `.then()`. This runs each of its steps in sequence as part of the parent workflow.\n\n```typescript {4,7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nexport const nestedWorkflow = createWorkflow({...})\n\nexport const testWorkflow = createWorkflow({...})\n  .then(nestedWorkflow)\n  .commit();\n```\n\n## Cloning a workflow\n\nUse `cloneWorkflow` to duplicate an existing workflow. This lets you reuse its structure while overriding parameters like `id`.\n\n```typescript {6,10} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep, cloneWorkflow } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst parentWorkflow = createWorkflow({...})\nconst clonedWorkflow = cloneWorkflow(parentWorkflow, { id: \"cloned-workflow\" });\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .then(clonedWorkflow)\n  .commit();\n```\n\n## Exiting early with `bail()`\n\nUse `bail()` in a step to exit early with a successful result. This returns the provided payload as the step output and ends workflow execution.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  id: 'step1',\n  execute: async ({ bail }) => {\n    return bail({ result: 'bailed' });\n  },\n  inputSchema: z.object({ value: z.string() }),\n  outputSchema: z.object({ result: z.string() }),\n});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\n## Exiting early with `Error()`\n\nUse `throw new Error()` in a step to exit with an error.\n\n```typescript {7} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({\n  id: 'step1',\n  execute: async () => {\n    throw new Error('bailed');\n  },\n  inputSchema: z.object({ value: z.string() }),\n  outputSchema: z.object({ result: z.string() }),\n});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\nThis throws an error from the step and stops workflow execution, returning the error as the result.\n\n## Example Run Instance\n\nThe following example demonstrates how to start a run with multiple inputs. Each input will pass through the `mapStep` sequentially.\n\n```typescript {6} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n  inputData: [{ number: 10 }, { number: 100 }, { number: 200 }]\n});\n```\n\nTo execute this run from your terminal:\n\n```bash copy\nnpx tsx src/test-workflow.ts\n```\n\n\n---\ntitle: \"Inngest Workflows | Workflows | Mastra Docs\"\ndescription: \"Inngest workflow allows you to run Mastra workflows with Inngest\"\n---\n\n# Inngest Workflow\n[EN] Source: https://mastra.ai/en/docs/workflows/inngest-workflow\n\n[Inngest](https://www.inngest.com/docs) is a developer platform for building and running background workflows, without managing infrastructure.\n\n## How Inngest Works with Mastra\n\nInngest and Mastra integrate by aligning their workflow models: Inngest organizes logic into functions composed of steps, and Mastra workflows defined using `createWorkflow` and `createStep` map directly onto this paradigm. Each Mastra workflow becomes an Inngest function with a unique identifier, and each step within the workflow maps to an Inngest step.\n\nThe `serve` function bridges the two systems by registering Mastra workflows as Inngest functions and setting up the necessary event handlers for execution and monitoring.\n\nWhen an event triggers a workflow, Inngest executes it step by step, memoizing each step’s result. This means if a workflow is retried or resumed, completed steps are skipped, ensuring efficient and reliable execution. Control flow primitives in Mastra, such as loops, conditionals, and nested workflows are seamlessly translated into the same Inngest’s function/step model, preserving advanced workflow features like composition, branching, and suspension.\n\nReal-time monitoring, suspend/resume, and step-level observability are enabled via Inngest’s publish-subscribe system and dashboard. As each step executes, its state and output are tracked using Mastra storage and can be resumed as needed.\n\n## Setup\n\n```sh\nnpm install @mastra/inngest @mastra/core @mastra/deployer\n```\n\n## Building an Inngest Workflow\n\nThis guide walks through creating a workflow with Inngest and Mastra, demonstrating a counter application that increments a value until it reaches 10.\n\n### Inngest Initialization\n\nInitialize the Inngest integration to obtain Mastra-compatible workflow helpers. The createWorkflow and createStep functions are used to create workflow and step objects that are compatible with Mastra and inngest.\n\nIn development\n\n```ts showLineNumbers copy filename=\"src/mastra/inngest/index.ts\"\nimport { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\nexport const inngest = new Inngest({\n  id: \"mastra\",\n  baseUrl:\"http://localhost:3000\",\n  isDev: true,\n  middleware: [realtimeMiddleware()],\n});\n```\n\nIn production\n\n```ts showLineNumbers copy filename=\"src/mastra/inngest/index.ts\"\nimport { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\nexport const inngest = new Inngest({\n  id: \"mastra\",\n  middleware: [realtimeMiddleware()],\n});\n```\n\n### Creating Steps\n\nDefine the individual steps that will compose your workflow:\n\n```ts showLineNumbers copy filename=\"src/mastra/workflows/index.ts\"\nimport { z } from \"zod\";\nimport { inngest } from \"../inngest\";\nimport { init } from \"@mastra/inngest\";\n\n// Initialize Inngest with Mastra, pointing to your local Inngest server\nconst { createWorkflow, createStep } = init(inngest);\n\n// Step: Increment the counter value\nconst incrementStep = createStep({\n  id: \"increment\",\n  inputSchema: z.object({\n    value: z.number(),\n  }),\n  outputSchema: z.object({\n    value: z.number(),\n  }),\n  execute: async ({ inputData }) => {\n    return { value: inputData.value + 1 };\n  },\n});\n```\n\n### Creating the Workflow\n\nCompose the steps into a workflow using the `dountil` loop pattern. The createWorkflow function creates a function on inngest server that is invocable.\n\n```ts showLineNumbers copy filename=\"src/mastra/workflows/index.ts\"\n// workflow that is registered as a function on inngest server\nconst workflow = createWorkflow({\n  id: \"increment-workflow\",\n  inputSchema: z.object({\n    value: z.number(),\n  }),\n  outputSchema: z.object({\n    value: z.number(),\n  }),\n}).then(incrementStep);\n\nworkflow.commit();\n\nexport { workflow as incrementWorkflow };\n```\n\n### Configuring the Mastra Instance and Executing the Workflow\n\nRegister the workflow with Mastra and configure the Inngest API endpoint:\n\n```ts showLineNumbers copy filename=\"src/mastra/index.ts\"\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { serve as inngestServe } from \"@mastra/inngest\";\nimport { incrementWorkflow } from \"./workflows\";\nimport { inngest } from \"./inngest\";\nimport { PinoLogger } from \"@mastra/loggers\";\n\n// Configure Mastra with the workflow and Inngest API endpoint\nexport const mastra = new Mastra({\n  workflows: {\n    incrementWorkflow,\n  },\n  server: {\n    // The server configuration is required to allow local docker container can connect to the mastra server\n    host: \"0.0.0.0\",\n    apiRoutes: [\n      // This API route is used to register the Mastra workflow (inngest function) on the inngest server\n      {\n        path: \"/api/inngest\",\n        method: \"ALL\",\n        createHandler: async ({ mastra }) => inngestServe({ mastra, inngest }),\n        // The inngestServe function integrates Mastra workflows with Inngest by:\n        // 1. Creating Inngest functions for each workflow with unique IDs (workflow.${workflowId})\n        // 2. Setting up event handlers that:\n        //    - Generate unique run IDs for each workflow execution\n        //    - Create an InngestExecutionEngine to manage step execution\n        //    - Handle workflow state persistence and real-time updates\n        // 3. Establishing a publish-subscribe system for real-time monitoring\n        //    through the workflow:${workflowId}:${runId} channel\n      },\n    ],\n  },\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\",\n  }),\n});\n```\n\n### Running the Workflow locally\n\n> **Prerequisites:**\n>\n> - Docker installed and running\n> - Mastra project set up\n> - Dependencies installed (`npm install`)\n\n1. Run `npx mastra dev` to start the Mastra server on local to serve the server on port 5000.\n2. Start the Inngest Dev Server (via Docker)\n   In a new terminal, run:\n\n```sh\ndocker run --rm -p 3000:3000 \\\n  inngest/inngest \\\n  inngest dev -u http://host.docker.internal:5000/api/inngest\n```\n\n> **Note:** The URL after `-u` tells the Inngest dev server where to find your Mastra `/api/inngest` endpoint.\n\n3. Open the Inngest Dashboard\n\n- Visit [http://localhost:3000](http://localhost:3000) in your browser.\n- Go to the **Apps** section in the sidebar.\n- You should see your Mastra workflow registered.\n  ![Inngest Dashboard](/inngest-apps-dashboard.png)\n\n4. Invoke the Workflow\n\n- Go to the **Functions** section in the sidebar.\n- Select your Mastra workflow.\n- Click **Invoke** and use the following input:\n\n```json\n{\n  \"data\": {\n    \"inputData\": {\n      \"value\": 5\n    }\n  }\n}\n```\n\n![Inngest Function](/inngest-function-dashboard.png)\n\n5. **Monitor the Workflow Execution**\n\n- Go to the **Runs** tab in the sidebar.\n- Click on the latest run to see step-by-step execution progress.\n  ![Inngest Function Run](/inngest-runs-dashboard.png)\n\n### Running the Workflow in Production\n\n> **Prerequisites:**\n>\n> - Vercel account and Vercel CLI installed (`npm i -g vercel`)\n> - Inngest account\n> - Vercel token (recommended: set as environment variable)\n\n1. Add Vercel Deployer to Mastra instance\n\n```ts showLineNumbers copy filename=\"src/mastra/index.ts\"\nimport { VercelDeployer } from \"@mastra/deployer-vercel\";\n\nexport const mastra = new Mastra({\n  // ...other config\n  deployer: new VercelDeployer({\n    teamSlug: \"your_team_slug\",\n    projectName: \"your_project_name\",\n    // you can get your vercel token from the vercel dashboard by clicking on the user icon in the top right corner\n    // and then clicking on \"Account Settings\" and then clicking on \"Tokens\" on the left sidebar.\n    token: \"your_vercel_token\",\n  }),\n});\n```\n\n> **Note:** Set your Vercel token in your environment:\n>\n> ```sh\n> export VERCEL_TOKEN=your_vercel_token\n> ```\n\n2. Build the mastra instance\n\n```sh\nnpx mastra build\n```\n\n3. Deploy to Vercel\n\n```sh\ncd .mastra/output\nvercel --prod\n```\n\n> **Tip:** If you haven't already, log in to Vercel CLI with `vercel login`.\n\n4. Sync with Inngest Dashboard\n\n- Go to the [Inngest dashboard](https://app.inngest.com/env/production/apps).\n- Click **Sync new app with Vercel** and follow the instructions.\n- You should see your Mastra workflow registered as an app.\n  ![Inngest Dashboard](/inngest-apps-dashboard-prod.png)\n\n5. Invoke the Workflow\n\n- In the **Functions** section, select `workflow.increment-workflow`.\n- Click **All actions** (top right) > **Invoke**.\n- Provide the following input:\n\n```json\n{\n  \"data\": {\n    \"inputData\": {\n      \"value\": 5\n    }\n  }\n}\n```\n\n![Inngest Function Run](/inngest-function-dashboard-prod.png)\n\n6.  Monitor Execution\n\n- Go to the **Runs** tab.\n- Click the latest run to see step-by-step execution progress.\n  ![Inngest Function Run](/inngest-runs-dashboard-prod.png)\n\n\n---\ntitle: \"Input Data Mapping with Workflow | Mastra Docs\"\ndescription: \"Learn how to use workflow input mapping to create more dynamic data flows in your Mastra workflows.\"\n---\n\n# Input Data Mapping\n[EN] Source: https://mastra.ai/en/docs/workflows/input-data-mapping\n\nInput data mapping allows explicit mapping of values for the inputs of the next step. These values can come from a number of sources:\n\n- The outputs of a previous step\n- The runtime context\n- A constant value\n- The initial input of the workflow\n\n## Mapping with `.map()`\n\nIn this example the `output` from `step1` is transformed to match the `inputSchema` required for the `step2`. The value from `step1` is available using the `inputData` parameter of the `.map` function.\n\n![Mapping with .map()](/image/workflows/workflows-data-mapping-map.jpg)\n\n```typescript {9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .map(async ({ inputData }) => {\n    const { value } = inputData;\n    return {\n      output: `new ${value}`\n    };\n  })\n  .then(step2)\n  .commit();\n```\n\n## Using `inputData`\n\nUse `inputData` to access the full output of the previous step:\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map(({ inputData }) => {\n    console.log(inputData);\n  })\n```\n\n## Using `getStepResult()`\n\nUse `getStepResult` to access the full output of a specific step by referencing the step's instance:\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map(async ({ getStepResult }) => {\n    console.log(getStepResult(step1));\n  })\n```\n\n## Using `getInitData()`\n\nUse `getInitData` to access the initial input data provided to the workflow:\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map(async ({ getInitData }) => {\n      console.log(getInitData());\n  })\n```\n\n## Using `mapVariable()`\n\nTo use `mapVariable` import the necessary function from the workflows module:\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { mapVariable } from \"@mastra/core/workflows\";\n```\n\n### Renaming step with `mapVariable()`\n\nYou can rename step outputs using the object syntax in `.map()`. In the example below, the `value` output from `step1` is renamed to `details`:\n\n```typescript {3-6} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\n  .then(step1)\n  .map({\n    details: mapVariable({\n      step: step,\n      path: \"value\"\n    })\n  })\n```\n\n### Renaming workflows with `mapVariable()`\n\nYou can rename workflow outputs by using **referential composition**. This involves passing the workflow instance as the `initData`.\n\n```typescript {6-9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nexport const testWorkflow = createWorkflow({...});\n\ntestWorkflow\n  .then(step1)\n  .map({\n    details: mapVariable({\n      initData: testWorkflow,\n      path: \"value\"\n    })\n  })\n```\n\n\n---\ntitle: \"Handling Complex LLM Operations | Workflows | Mastra\"\ndescription: \"Workflows in Mastra help you orchestrate complex sequences of operations with features like branching, parallel execution, resource suspension, and more.\"\n---\n\nimport { Steps } from \"nextra/components\";\n\n# Workflows overview\n[EN] Source: https://mastra.ai/en/docs/workflows/overview\n\nWorkflows let you define and orchestrate complex sequences of tasks as **typed steps** connected by data flows. Each step has clearly defined inputs and outputs validated by Zod schemas.\n\nA workflow manages execution order, dependencies, branching, parallelism, and error handling — enabling you to build robust, reusable processes. Steps can be nested or cloned to compose larger workflows.\n\n![Workflows overview](/image/workflows/workflows-overview.jpg)\n\nYou create workflows by:\n\n- Defining **steps** with `createStep`, specifying input/output schemas and business logic.\n- Composing **steps** with `createWorkflow` to define the execution flow.\n- Running **workflows** to execute the entire sequence, with built-in support for suspension, resumption, and streaming results.\n\nThis structure provides full type safety and runtime validation, ensuring data integrity across the entire workflow.\n\n\n## Getting started\n\nTo use workflows, first import the necessary functions from the workflows module:\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n```\n\n### Create step\n\nSteps are the building blocks of workflows. Create a step using `createStep`:\n\n```typescript filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({...});\n```\n\n> See [createStep](/reference/workflows/step) for more information.\n\n### Create workflow\n\nCreate a workflow using `createWorkflow` and complete it with `.commit()`.\n\n```typescript {6,17} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\n\nexport const testWorkflow = createWorkflow({\n  id: \"test-workflow\",\n  description: 'Test workflow',\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  })\n})\n  .then(step1)\n  .commit();\n```\n\n> See [workflow](/reference/workflows/workflow) for more information.\n\n#### Composing steps\n\nWorkflow steps can be composed and executed sequentially using `.then()`.\n\n```typescript {17,18} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({\n  id: \"test-workflow\",\n  description: 'Test workflow',\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  })\n})\n  .then(step1)\n  .then(step2)\n  .commit();\n```\n\n> Steps can be composed using a number of different methods. See [Control Flow](/docs/workflows/control-flow)  for more information.\n\n#### Cloning steps\n\nWorkflow steps can be cloned using `cloneStep()`, and used with any workflow method.\n\n```typescript {5,19} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep, cloneStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst clonedStep = cloneStep(step1, { id: \"cloned-step\" });\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({\n  id: \"test-workflow\",\n  description: 'Test workflow',\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  })\n})\n  .then(step1)\n  .then(clonedStep)\n  .then(step2)\n  .commit();\n```\n\n### Register workflow\n\nRegister a workflow using `workflows` in the main Mastra instance:\n\n```typescript {8} filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nimport { testWorkflow } from \"./workflows/test-workflow\";\n\nexport const mastra = new Mastra({\n  workflows: { testWorkflow },\n  storage: new LibSQLStore({\n    // stores telemetry, evals, ... into memory storage, if it needs to persist, change to file:../mastra.db\n    url: \":memory:\"\n  }),\n  logger: new PinoLogger({\n    name: \"Mastra\",\n    level: \"info\"\n  })\n});\n```\n\n### Run workflow\nThere are two ways to run and test workflows.\n\n<Steps>\n\n#### Mastra Playground\n\nWith the Mastra Dev Server running you can run the workflow from the Mastra Playground by visiting [http://localhost:5000/workflows](http://localhost:5000/workflows) in your browser.\n\n#### Command line\n\nCreate a run instance of any Mastra workflow using `createRunAsync` and `start`:\n\n```typescript {3,5} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    city: \"London\"\n  }\n});\n\nconsole.log(JSON.stringify(result, null, 2));\n```\n> see [createRunAsync](/reference/workflows/create-run) and [start](/reference/workflows/start) for more information.\n\nTo trigger this workflow, run the following:\n\n```bash copy\nnpx tsx src/test-workflow.ts\n```\n\n</Steps>\n\n#### Run workflow results\n\nThe result of running a workflow using either `start()` or `resume()` will look like one of the following, depending on the outcome.\n\n##### Status success\n\n```json\n{\n  \"status\": \"success\",\n  \"steps\": {\n    // ...\n    \"step-1\": {\n      // ...\n      \"status\": \"success\",\n    }\n  },\n  \"result\": {\n    \"output\": \"London + step-1\"\n  }\n}\n```\n\n- **status**: Shows the final state of the workflow execution, either: `success`, `suspended`, or `error`\n- **steps**: Lists each step in the workflow, including inputs and outputs\n- **status**: Shows the outcome of each individual step\n- **result**: Includes the final output of the workflow, typed according to the `outputSchema`\n\n\n##### Status suspended\n\n```json\n{\n  \"status\": \"suspended\",\n  \"steps\": {\n    // ...\n    \"step-1\": {\n      // ...\n      \"status\": \"suspended\",\n    }\n  },\n  \"suspended\": [\n    [\n      \"step-1\"\n    ]\n  ]\n}\n```\n\n- **suspended**: An optional array listing any steps currently awaiting input before continuing\n\n##### Status failed\n\n```json\n{\n  \"status\": \"failed\",\n  \"steps\": {\n    // ...\n    \"step-1\": {\n      // ...\n      \"status\": \"failed\",\n      \"error\": \"Test error\",\n    }\n  },\n  \"error\": \"Test error\"\n}\n```\n- **error**: An optional field that includes the error message if the workflow fails\n\n### Stream workflow\n\nSimilar to the run method shown above, workflows can also be streamed:\n\n```typescript {5} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.stream({\n  inputData: {\n    city: \"London\"\n  }\n});\n\nfor await (const chunk of result.stream) {\n  console.log(chunk);\n}\n```\n\n> See [stream](/reference/workflows/stream) and [messages](/reference/workflows/stream#messages) for more information.\n\n### Watch Workflow\n\nA workflow can also be watched, allowing you to inspect each event that is emitted.\n\n```typescript {5} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nrun.watch((event) => {\n  console.log(event);\n});\n\nconst result = await run.start({\n  inputData: {\n    city: \"London\"\n  }\n});\n```\n\n> See [watch](/reference/workflows/watch) for more information.\n\n## More resources\n\n- The [Workflow Guide](../../guides/guide/ai-recruiter.mdx) in the Guides section is a tutorial that covers the main concepts.\n- [Parallel Steps workflow example](../../examples/workflows/parallel-steps.mdx)\n- [Conditional Branching workflow example](../../examples/workflows/conditional-branching.mdx)\n- [Inngest workflow example](../../examples/workflows/inngest-workflow.mdx)\n- [Suspend and Resume workflow example](../../examples/workflows/human-in-the-loop.mdx)\n\n\n## Workflows (Legacy)\n\nFor legacy workflow documentation, see [Workflows (Legacy)](/docs/workflows-legacy/overview).\n\n\n\n---\ntitle: \"Pausing Execution | Mastra Docs\"\ndescription: \"Pausing execution in Mastra workflows allows you to pause execution while waiting for external input or resources via .sleep(), .sleepUntil() and .waitForEvent().\"\n---\n\n# Sleep & Events\n[EN] Source: https://mastra.ai/en/docs/workflows/pausing-execution\n\nMastra lets you pause workflow execution when waiting for external input or timing conditions. This can be useful for things like polling, delayed retries, or waiting on user actions.\n\nYou can pause execution using:\n\n- `sleep()`: Pause for a set number of milliseconds\n- `sleepUntil()`: Pause until a specific timestamp\n- `waitForEvent()`: Pause until an external event is received\n- `sendEvent()`: Send an event to resume a waiting workflow\n\nWhen using any of these methods, the workflow status is set to `waiting` until execution resumes.\n\n## Pausing with `.sleep()`\n\nThe `sleep()` method pauses execution between steps for a specified number of milliseconds.\n\n```typescript {9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .sleep(1000)\n  .then(step2)\n  .commit();\n```\n\n### Pausing with `.sleep(callback)`\n\nThe `sleep()` method also accepts a callback that returns the number of milliseconds to pause. The callback receives `inputData`, allowing the delay to be computed dynamically.\n\n```typescript {9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .sleep(async ({ inputData }) => {\n    const { delayInMs }  = inputData\n    return delayInMs;\n  })\n  .then(step2)\n  .commit();\n```\n\n## Pausing with `.sleepUntil()`\n\nThe `sleepUntil()` method pauses execution between steps until a specified date.\n\n```typescript {9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .sleepUntil(new Date(Date.now() + 5000))\n  .then(step2)\n  .commit();\n```\n\n### Pausing with `.sleepUntil(callback)`\n\nThe `sleepUntil()` method also accepts a callback that returns a `Date` object. The callback receives `inputData`, allowing the target time to be computed dynamically.\n\n```typescript {9} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .sleepUntil(async ({ inputData }) => {\n    const { delayInMs }  = inputData\n    return new Date(Date.now() + delayInMs);\n  })\n  .then(step2)\n  .commit();\n```\n\n\n> `Date.now()` is evaluated when the workflow starts, not at the moment the `sleepUntil()` method is called.\n\n## Pausing with `.waitForEvent()`\n\nThe `waitForEvent()` method pauses execution until a specific event is received. Use `run.sendEvent()` to send the event. You must provide both the event name and the step to resume.\n\n![Pausing with .waitForEvent()](/image/workflows/workflows-sleep-events-waitforevent.jpg)\n\n```typescript {10} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { createWorkflow, createStep } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n\nconst step1 = createStep({...});\nconst step2 = createStep({...});\nconst step3 = createStep({...});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .waitForEvent(\"my-event-name\", step2)\n  .then(step3)\n  .commit();\n```\n## Sending an event with `.sendEvent()`\n\nThe `.sendEvent()` method sends an event to the workflow. It accepts the event name and optional event data, which can be any JSON-serializable value.\n\n```typescript {5,12,15} filename=\"src/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = run.start({\n  inputData: {\n    value: \"hello\"\n  }\n});\n\nsetTimeout(() => {\n  run.sendEvent(\"my-event-name\", { value: \"from event\" });\n}, 3000);\n\nconsole.log(JSON.stringify(await result, null, 2));\n```\n\n> In this example, avoid using `await run.start()` directly, it would block sending the event before the workflow reaches its waiting state.\n\n\n---\ntitle: \"Suspend & Resume Workflows | Human-in-the-Loop | Mastra Docs\"\ndescription: \"Suspend and resume in Mastra workflows allows you to pause execution while waiting for external input or resources.\"\n---\n\n# Suspend & Resume\n[EN] Source: https://mastra.ai/en/docs/workflows/suspend-and-resume\n\nWorkflows can be paused at any step, with their current state persisted as a snapshot in storage. Execution can then be resumed from this saved snapshot when ready. Persisting the snapshot ensures the workflow state is maintained across sessions, deployments, and server restarts, essential for workflows that may remain suspended while awaiting external input or resources.\n\nCommon scenarios for suspending workflows include:\n\n- Waiting for human approval or input\n- Pausing until external API resources become available\n- Collecting additional data needed for later steps\n- Rate limiting or throttling expensive operations\n- Handling event-driven processes with external triggers\n\n## Workflow status types\n\nWhen running a workflow, its `status` can be one of the following:\n\n- `running` - The workflow is currently running\n- `suspended` - The workflow is suspended\n- `success` - The workflow has completed\n- `failed` - The workflow has failed\n\n## Suspending a workflow with `suspend()`\n\nWhen the state is `suspended`, you can identify any and all steps that have been suspended by looking at the `suspended` array of the workflow result output.\n\n![Suspending a workflow with suspend()](/image/workflows/workflows-suspend-resume-suspend.jpg)\n\n```typescript {18} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst step1 = createStep({\n  id: \"step-1\",\n  description: \"Test suspend\",\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  }),\n  suspendSchema: z.object({}),\n  resumeSchema: z.object({\n    city: z.string()\n  }),\n  execute: async ({ resumeData, suspend }) => {\n    const { city } = resumeData ?? {};\n\n    if (!city) {\n      await suspend({});\n      return { output: \"\" };\n    }\n\n    return { output: \"\" };\n  }\n});\n\nexport const testWorkflow = createWorkflow({})\n  .then(step1)\n  .commit();\n```\n\n> See [Define Suspendable workflow](/examples/workflows/human-in-the-loop#define-suspendable-workflow) for more information.\n\n### Identifying suspended steps\n\nTo resume a suspended workflow, inspect the `suspended` array in the result to determine which step needs input:\n\n```typescript {15} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n  inputData: {\n    city: \"London\"\n  }\n});\n\nconsole.log(JSON.stringify(result, null, 2));\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    step: result.suspended[0],\n    resumeData: {\n      city: \"Berlin\"\n    }\n  });\n}\n\n```\n\nIn this case, the logic resumes the first step listed in the `suspended` array. A `step` can also be defined using it's `id`, for example: 'step-1'.\n\n```json\n{\n  \"status\": \"suspended\",\n  \"steps\": {\n    // ...\n    \"step-1\": {\n      // ...\n      \"status\": \"suspended\",\n    }\n  },\n  \"suspended\": [\n    [\n      \"step-1\"\n    ]\n  ]\n}\n```\n\n> See [Run Workflow Results](/workflows/overview#run-workflow-results) for more details.\n\n## Resuming a workflow with `resume()`\n\nA workflow can be resumed by calling `resume` and providing the required `resumeData`.\n\n```typescript {16-18} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { mastra } from \"./mastra\";\n\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n   inputData: {\n    city: \"London\"\n  }\n});\n\nconsole.log(JSON.stringify(result, null, 2));\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    step: 'step-1',\n    resumeData: {\n      city: \"Berlin\"\n    }\n  });\n\n  console.log(JSON.stringify(resumedResult, null, 2));\n}\n```\n\n### Resuming nested workflows\n\nTo resume a suspended nested workflow pass the workflow instance to the `step` parameter of the `resume` function.\n\n```typescript {33-34} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nconst dowhileWorkflow = createWorkflow({\n  id: 'dowhile-workflow',\n  inputSchema: z.object({ value: z.number() }),\n  outputSchema: z.object({ value: z.number() }),\n})\n  .dountil(\n    createWorkflow({\n      id: 'simple-resume-workflow',\n      inputSchema: z.object({ value: z.number() }),\n      outputSchema: z.object({ value: z.number() }),\n      steps: [incrementStep, resumeStep],\n    })\n      .then(incrementStep)\n      .then(resumeStep)\n      .commit(),\n    async ({ inputData }) => inputData.value >= 10,\n  )\n  .then(\n    createStep({\n      id: 'final',\n      inputSchema: z.object({ value: z.number() }),\n      outputSchema: z.object({ value: z.number() }),\n      execute: async ({ inputData }) => ({ value: inputData.value }),\n    }),\n  )\n  .commit();\n\nconst run = await dowhileWorkflow.createRunAsync();\nconst result = await run.start({ inputData: { value: 0 } });\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    resumeData: { value: 2 },\n    step: ['simple-resume-workflow', 'resume'],\n  });\n\n  console.log(JSON.stringify(resumedResult, null, 2));\n}\n```\n\n## Using `RuntimeContext` with suspend/resume\n\nWhen using suspend/resume with `RuntimeContext`, you can create the instance yourself, and pass it to the `start` and `resume` functions.\n`RuntimeContext` is not automatically shared on a workflow run.\n\n```typescript {1,4,9,16} filename=\"src/mastra/workflows/test-workflow.tss\" showLineNumbers copy\nimport { RuntimeContext } from \"@mastra/core/di\";\nimport { mastra } from \"./mastra\";\n\nconst runtimeContext = new RuntimeContext();\nconst run = await mastra.getWorkflow(\"testWorkflow\").createRunAsync();\n\nconst result = await run.start({\n  inputData: { suggestions: [\"London\", \"Paris\", \"New York\"] },\n  runtimeContext\n});\n\nif (result.status === \"suspended\") {\n  const resumedResult = await run.resume({\n    step: 'step-1',\n    resumeData: { city: \"New York\" },\n    runtimeContext\n  });\n}\n```\n\n\n---\ntitle: \"Using Workflows with Agents and Tools | Workflows | Mastra Docs\"\ndescription: \"Steps in Mastra workflows provide a structured way to manage operations by defining inputs, outputs, and execution logic.\"\n---\n\n# Agents and Tools\n[EN] Source: https://mastra.ai/en/docs/workflows/using-with-agents-and-tools\n\nWorkflow steps are composable and typically run logic directly within the `execute` function. However, there are cases where calling an agent or tool is more appropriate. This pattern is especially useful when:\n\n- Generating natural language responses from user input using an LLM.\n- Abstracting complex or reusable logic into a dedicated tool.\n- Interacting with third-party APIs in a structured or reusable way.\n\nWorkflows can use Mastra agents or tools directly as steps, for example: `createStep(testAgent)` or `createStep(testTool)`.\n\n## Using agents in workflows\n\nTo include an agent in a workflow, define it in the usual way, then either add it directly to the workflow using `createStep(testAgent)` or, invoke it from within a step's `execute` function using `.generate()`.\n\n### Example agent\n\nThis agent uses OpenAI to generate a fact about a city, country, and timezone.\n\n```typescript filename=\"src/mastra/agents/test-agent.ts\" showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\n\nexport const testAgent = new Agent({\n  name: \"test-agent\",\n  description: \"Create facts for a country based on the city\",\n  instructions: `Return an interesting fact about the country based on the city provided`,\n  model: openai(\"gpt-4o\")\n});\n```\n\n### Adding an agent as a step\n\nIn this example, `step1` uses the `testAgent` to generate an interesting fact about the country based on a given city.\n\nThe `.map` method transforms the workflow input into a `prompt` string compatible with the `testAgent`.\n\nThe step is composed into the workflow using `.then()`, allowing it to receive the mapped input and return the agent's structured output. The workflow is finalized with `.commit()`.\n\n\n![Agent as step](/image/workflows/workflows-agent-tools-agent-step.jpg)\n\n\n```typescript {3} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { testAgent } from \"../agents/test-agent\";\n\nconst step1 = createStep(testAgent);\n\nexport const testWorkflow = createWorkflow({\n  id: \"test-workflow\",\n  description: 'Test workflow',\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  })\n})\n  .map(({ inputData }) => {\n    const { input } = inputData;\n    return {\n      prompt: `Provide facts about the city: ${input}`\n    };\n  })\n  .then(step1)\n  .commit();\n```\n\n### Calling an agent with `.generate()`\n\nIn this example, the `step1` builds a prompt using the provided `input` and passes it to the `testAgent`, which returns a plain-text response containing facts about the city and its country.\n\nThe step is added to the workflow using the sequential `.then()` method, allowing it to receive input from the workflow and return structured output. The workflow is finalized with `.commit()`.\n\n```typescript {1,18, 29} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { testAgent } from \"../agents/test-agent\";\n\nconst step1 = createStep({\n  id: \"step-1\",\n  description: \"Create facts for a country based on the city\",\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  }),\n\n  execute: async ({ inputData }) => {\n    const { input } = inputData;\n\n    const  prompt = `Provide facts about the city: ${input}`\n\n    const { text } = await testAgent.generate([\n      { role: \"user\", content: prompt }\n    ]);\n\n    return {\n      output: text\n    };\n  }\n});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\n## Using tools in workflows\n\nTo use a tool within a workflow, define it in the usual way, then either add it directly to the workflow using `createStep(testTool)` or, invoke it from within a step's `execute` function using `.execute()`.\n\n### Example tool\n\nThe example below uses the Open Meteo API to retrieve geolocation details for a city, returning its name, country, and timezone.\n\n```typescript filename=\"src/mastra/tools/test-tool.ts\" showLineNumbers copy\nimport { createTool } from \"@mastra/core\";\nimport { z } from \"zod\";\n\nexport const testTool = createTool({\n  id: \"test-tool\",\n  description: \"Gets country for a city\",\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    country_name: z.string()\n  }),\n  execute: async ({ context }) => {\n    const { input } = context;\n    const geocodingResponse = await fetch(`https://geocoding-api.open-meteo.com/v1/search?name=${input}`);\n    const geocodingData = await geocodingResponse.json();\n\n    const { country } = geocodingData.results[0];\n\n    return {\n      country_name: country\n    };\n  }\n});\n```\n\n### Adding a tool as a step\n\nIn this example, `step1` uses the `testTool`, which performs a geocoding lookup using the provided `city` and returns the resolved `country`.\n\nThe step is added to the workflow using the sequential `.then()` method, allowing it to receive input from the workflow and return structured output. The workflow is finalized with `.commit()`.\n\n![Tool as step](/image/workflows/workflows-agent-tools-tool-step.jpg)\n\n```typescript {1,3,6} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { testTool } from \"../tools/test-tool\";\n\nconst step1 = createStep(testTool);\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\n### Calling a tool with `.execute()`\n\nIn this example, `step1` directly invokes `testTool` using its `.execute()` method. The tool performs a geocoding lookup with the provided `city` and returns the corresponding `country`.\n\nThe result is returned as structured output from the step. The step is composed into the workflow using `.then()`, enabling it to process workflow input and produce typed output. The workflow is finalized with `.commit()`\n\n```typescript {3,20,32} filename=\"src/mastra/workflows/test-workflow.ts\" showLineNumbers copy\nimport { RuntimeContext } from \"@mastra/core/di\";\n\nimport { testTool } from \"../tools/test-tool\";\n\nconst runtimeContext = new RuntimeContext();\n\nconst step1 = createStep({\n  id: \"step-1\",\n  description: \"Gets country for a city\",\n  inputSchema: z.object({\n    input: z.string()\n  }),\n  outputSchema: z.object({\n    output: z.string()\n  }),\n\n  execute: async ({ inputData }) => {\n    const { input } = inputData;\n\n    const { country_name } = await testTool.execute({\n      context: { input },\n      runtimeContext\n    });\n\n    return {\n      output: country_name\n    };\n  }\n});\n\nexport const testWorkflow = createWorkflow({...})\n  .then(step1)\n  .commit();\n```\n\n## Using workflows as tools\n\nIn this example the `cityStringWorkflow` workflow has been added to the main Mastra instance.\n\n\n```typescript {7} filename=\"src/mastra/index.ts\" showLineNumbers copy\nimport { Mastra } from \"@mastra/core/mastra\";\n\nimport { testWorkflow, cityStringWorkflow } from \"./workflows/test-workflow\";\n\nexport const mastra = new Mastra({\n  ...\n  workflows: { testWorkflow, cityStringWorkflow },\n});\n```\n\nOnce a workflow has been registered it can be referenced using `getWorkflow` from withing a tool.\n\n```typescript {10,17-27} filename=\"src/mastra/tools/test-tool.ts\" showLineNumbers copy\nexport const cityCoordinatesTool = createTool({\n  id: \"city-tool\",\n  description: \"Convert city details\",\n  inputSchema: z.object({\n    city: z.string()\n  }),\n  outputSchema: z.object({\n    outcome: z.string()\n  }),\n  execute: async ({ context, mastra }) => {\n    const { city } = context;\n    const geocodingResponse = await fetch(`https://geocoding-api.open-meteo.com/v1/search?name=${city}`);\n    const geocodingData = await geocodingResponse.json();\n\n    const { name, country, timezone } = geocodingData.results[0];\n\n    const workflow = mastra?.getWorkflow(\"cityStringWorkflow\");\n\n    const run = await workflow?.createRunAsync();\n\n    const { result } = await run?.start({\n      inputData: {\n        city_name: name,\n        country_name: country,\n        country_timezone: timezone\n      }\n    });\n\n    return {\n      outcome: result.outcome\n    };\n  }\n});\n```\n\n## Exposing workflows with `MCPServer`\n\nYou can convert your workflows into tools by passing them into an instance of a Mastra `MCPServer`. This allows any MCP-compatible client to access your workflow.\n\nThe workflow description becomes the tool description and the input schema becomes the tool's input schema.\n\nWhen you provide workflows to the server, each workflow is automatically exposed as a callable tool for example:\n\n- `run_testWorkflow`.\n\n```typescript filename=\"src/test-mcp-server.ts\" showLineNumbers copy\nimport { MCPServer } from \"@mastra/mcp\";\n\nimport { testAgent } from \"./mastra/agents/test-agent\";\nimport { testTool } from \"./mastra/tools/test-tool\";\nimport { testWorkflow } from \"./mastra/workflows/test-workflow\";\n\nasync function startServer() {\n  const server = new MCPServer({\n    name: \"test-mcp-server\",\n    version: \"1.0.0\",\n    workflows: {\n      testWorkflow\n    }\n  });\n\n  await server.startStdio();\n  console.log(\"MCPServer started on stdio\");\n}\n\nstartServer().catch(console.error);\n```\n\nTo verify that your workflow is available on the server, you can connect with an MCPClient.\n\n```typescript filename=\"src/test-mcp-client.ts\" showLineNumbers copy\nimport { MCPClient } from \"@mastra/mcp\";\n\nasync function main() {\n  const mcp = new MCPClient({\n    servers: {\n      local: {\n        command: \"npx\",\n        args: [\"tsx\", \"src/test-mcp-server.ts\"]\n      }\n    }\n  });\n\n  const tools = await mcp.getTools();\n  console.log(tools);\n}\n\nmain().catch(console.error);\n```\n\nRun the client script to see your workflow tool.\n\n```bash\nnpx tsx src/test-mcp-client.ts\n```\n\n## More resources\n\n- [MCPServer reference documentation](/reference/tools/mcp-server).\n- [MCPClient reference documentation](/reference/tools/mcp-client).\n\n\n---\ntitle: \"Branching, Merging, Conditions | Workflows (Legacy) | Mastra Docs\"\ndescription: \"Control flow in Mastra legacy workflows allows you to manage branching, merging, and conditions to construct legacy workflows that meet your logic requirements.\"\n---\n\n# Control Flow in Legacy Workflows: Branching, Merging, and Conditions\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/control-flow\n\nWhen you create a multi-step process, you may need to run steps in parallel, chain them sequentially, or follow different paths based on outcomes. This page describes how you can manage branching, merging, and conditions to construct workflows that meet your logic requirements. The code snippets show the key patterns for structuring complex control flow.\n\n## Parallel Execution\n\nYou can run multiple steps at the same time if they don't depend on each other. This approach can speed up your workflow when steps perform independent tasks. The code below shows how to add two steps in parallel:\n\n```typescript\nmyWorkflow.step(fetchUserData).step(fetchOrderData);\n```\n\nSee the [Parallel Steps](../../examples/workflows_legacy/parallel-steps.mdx) example for more details.\n\n## Sequential Execution\n\nSometimes you need to run steps in strict order to ensure outputs from one step become inputs for the next. Use .then() to link dependent operations. The code below shows how to chain steps sequentially:\n\n```typescript\nmyWorkflow.step(fetchOrderData).then(validateData).then(processOrder);\n```\n\nSee the [Sequential Steps](../../examples/workflows_legacy/sequential-steps.mdx) example for more details.\n\n## Branching and Merging Paths\n\nWhen different outcomes require different paths, branching is helpful. You can also merge paths later once they complete. The code below shows how to branch after stepA and later converge on stepF:\n\n```typescript\nmyWorkflow\n  .step(stepA)\n  .then(stepB)\n  .then(stepD)\n  .after(stepA)\n  .step(stepC)\n  .then(stepE)\n  .after([stepD, stepE])\n  .step(stepF);\n```\n\nIn this example:\n\n- stepA leads to stepB, then to stepD.\n- Separately, stepA also triggers stepC, which in turn leads to stepE.\n- Separately, stepF is triggered when both stepD and stepE are completed.\n\nSee the [Branching Paths](../../examples/workflows_legacy/branching-paths.mdx) example for more details.\n\n## Merging Multiple Branches\n\nSometimes you need a step to execute only after multiple other steps have completed. Mastra provides a compound `.after([])` syntax that allows you to specify multiple dependencies for a step.\n\n```typescript\nmyWorkflow\n  .step(fetchUserData)\n  .then(validateUserData)\n  .step(fetchProductData)\n  .then(validateProductData)\n  // This step will only run after BOTH validateUserData AND validateProductData have completed\n  .after([validateUserData, validateProductData])\n  .step(processOrder);\n```\n\nIn this example:\n\n- `fetchUserData` and `fetchProductData` run in parallel branches\n- Each branch has its own validation step\n- The `processOrder` step only executes after both validation steps have completed successfully\n\nThis pattern is particularly useful for:\n\n- Joining parallel execution paths\n- Implementing synchronization points in your workflow\n- Ensuring all required data is available before proceeding\n\nYou can also create complex dependency patterns by combining multiple `.after([])` calls:\n\n```typescript\nmyWorkflow\n  // First branch\n  .step(stepA)\n  .then(stepB)\n  .then(stepC)\n\n  // Second branch\n  .step(stepD)\n  .then(stepE)\n\n  // Third branch\n  .step(stepF)\n  .then(stepG)\n\n  // This step depends on the completion of multiple branches\n  .after([stepC, stepE, stepG])\n  .step(finalStep);\n```\n\n## Cyclical Dependencies and Loops\n\nWorkflows often need to repeat steps until certain conditions are met. Mastra provides two powerful methods for creating loops: `until` and `while`. These methods offer an intuitive way to implement repetitive tasks.\n\n### Using Manual Cyclical Dependencies (Legacy Approach)\n\nIn earlier versions, you could create loops by manually defining cyclical dependencies with conditions:\n\n```typescript\nmyWorkflow\n  .step(fetchData)\n  .then(processData)\n  .after(processData)\n  .step(finalizeData, {\n    when: { \"processData.status\": \"success\" },\n  })\n  .step(fetchData, {\n    when: { \"processData.status\": \"retry\" },\n  });\n```\n\nWhile this approach still works, the newer `until` and `while` methods provide a cleaner and more maintainable way to create loops.\n\n### Using `until` for Condition-Based Loops\n\nThe `until` method repeats a step until a specified condition becomes true. It takes these arguments:\n\n1. A condition that determines when to stop looping\n2. The step to repeat\n3. Optional variables to pass to the repeated step\n\n```typescript\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Step that increments a counter until target is reached\nconst incrementStep = new LegacyStep({\n  id: \"increment\",\n  inputSchema: z.object({\n    // Current counter value\n    counter: z.number().optional(),\n  }),\n  outputSchema: z.object({\n    // Updated counter value\n    updatedCounter: z.number(),\n  }),\n  execute: async ({ context }) => {\n    const { counter = 0 } = context.inputData;\n    return { updatedCounter: counter + 1 };\n  },\n});\n\nworkflow\n  .step(incrementStep)\n  .until(\n    async ({ context }) => {\n      // Stop when counter reaches 10\n      const result = context.getStepResult(incrementStep);\n      return (result?.updatedCounter ?? 0) >= 10;\n    },\n    incrementStep,\n    {\n      // Pass current counter to next iteration\n      counter: {\n        step: incrementStep,\n        path: \"updatedCounter\",\n      },\n    },\n  )\n  .then(finalStep);\n```\n\nYou can also use a reference-based condition:\n\n```typescript\nworkflow\n  .step(incrementStep)\n  .until(\n    {\n      ref: { step: incrementStep, path: \"updatedCounter\" },\n      query: { $gte: 10 },\n    },\n    incrementStep,\n    {\n      counter: {\n        step: incrementStep,\n        path: \"updatedCounter\",\n      },\n    },\n  )\n  .then(finalStep);\n```\n\n### Using `while` for Condition-Based Loops\n\nThe `while` method repeats a step as long as a specified condition remains true. It takes the same arguments as `until`:\n\n1. A condition that determines when to continue looping\n2. The step to repeat\n3. Optional variables to pass to the repeated step\n\n```typescript\n// Step that increments a counter while below target\nconst incrementStep = new LegacyStep({\n  id: \"increment\",\n  inputSchema: z.object({\n    // Current counter value\n    counter: z.number().optional(),\n  }),\n  outputSchema: z.object({\n    // Updated counter value\n    updatedCounter: z.number(),\n  }),\n  execute: async ({ context }) => {\n    const { counter = 0 } = context.inputData;\n    return { updatedCounter: counter + 1 };\n  },\n});\n\nworkflow\n  .step(incrementStep)\n  .while(\n    async ({ context }) => {\n      // Continue while counter is less than 10\n      const result = context.getStepResult(incrementStep);\n      return (result?.updatedCounter ?? 0) < 10;\n    },\n    incrementStep,\n    {\n      // Pass current counter to next iteration\n      counter: {\n        step: incrementStep,\n        path: \"updatedCounter\",\n      },\n    },\n  )\n  .then(finalStep);\n```\n\nYou can also use a reference-based condition:\n\n```typescript\nworkflow\n  .step(incrementStep)\n  .while(\n    {\n      ref: { step: incrementStep, path: \"updatedCounter\" },\n      query: { $lt: 10 },\n    },\n    incrementStep,\n    {\n      counter: {\n        step: incrementStep,\n        path: \"updatedCounter\",\n      },\n    },\n  )\n  .then(finalStep);\n```\n\n### Comparison Operators for Reference Conditions\n\nWhen using reference-based conditions, you can use these comparison operators:\n\n| Operator | Description              |\n| -------- | ------------------------ |\n| `$eq`    | Equal to                 |\n| `$ne`    | Not equal to             |\n| `$gt`    | Greater than             |\n| `$gte`   | Greater than or equal to |\n| `$lt`    | Less than                |\n| `$lte`   | Less than or equal to    |\n\n## Conditions\n\nUse the when property to control whether a step runs based on data from previous steps. Below are three ways to specify conditions.\n\n### Option 1: Function\n\n```typescript\nmyWorkflow.step(\n  new Step({\n    id: \"processData\",\n    execute: async ({ context }) => {\n      // Action logic\n    },\n  }),\n  {\n    when: async ({ context }) => {\n      const fetchData = context?.getStepResult<{ status: string }>(\"fetchData\");\n      return fetchData?.status === \"success\";\n    },\n  },\n);\n```\n\n### Option 2: Query Object\n\n```typescript\nmyWorkflow.step(\n  new Step({\n    id: \"processData\",\n    execute: async ({ context }) => {\n      // Action logic\n    },\n  }),\n  {\n    when: {\n      ref: {\n        step: {\n          id: \"fetchData\",\n        },\n        path: \"status\",\n      },\n      query: { $eq: \"success\" },\n    },\n  },\n);\n```\n\n### Option 3: Simple Path Comparison\n\n```typescript\nmyWorkflow.step(\n  new Step({\n    id: \"processData\",\n    execute: async ({ context }) => {\n      // Action logic\n    },\n  }),\n  {\n    when: {\n      \"fetchData.status\": \"success\",\n    },\n  },\n);\n```\n\n## Data Access Patterns\n\nMastra provides several ways to pass data between steps:\n\n1. **Context Object** - Access step results directly through the context object\n2. **Variable Mapping** - Explicitly map outputs from one step to inputs of another\n3. **getStepResult Method** - Type-safe method to retrieve step outputs\n\nEach approach has its advantages depending on your use case and requirements for type safety.\n\n### Using getStepResult Method\n\nThe `getStepResult` method provides a type-safe way to access step results. This is the recommended approach when working with TypeScript as it preserves type information.\n\n#### Basic Usage\n\nFor better type safety, you can provide a type parameter to `getStepResult`:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/get-step-result.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nconst fetchUserStep = new LegacyStep({\n  id: \"fetchUser\",\n  outputSchema: z.object({\n    name: z.string(),\n    userId: z.string(),\n  }),\n  execute: async ({ context }) => {\n    return { name: \"John Doe\", userId: \"123\" };\n  },\n});\n\nconst analyzeDataStep = new LegacyStep({\n  id: \"analyzeData\",\n  execute: async ({ context }) => {\n    // Type-safe access to previous step result\n    const userData = context.getStepResult<{ name: string; userId: string }>(\n      \"fetchUser\",\n    );\n\n    if (!userData) {\n      return { status: \"error\", message: \"User data not found\" };\n    }\n\n    return {\n      analysis: `Analyzed data for user ${userData.name}`,\n      userId: userData.userId,\n    };\n  },\n});\n```\n\n#### Using Step References\n\nThe most type-safe approach is to reference the step directly in the `getStepResult` call:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/step-reference.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Define step with output schema\nconst fetchUserStep = new LegacyStep({\n  id: \"fetchUser\",\n  outputSchema: z.object({\n    userId: z.string(),\n    name: z.string(),\n    email: z.string(),\n  }),\n  execute: async () => {\n    return {\n      userId: \"user123\",\n      name: \"John Doe\",\n      email: \"john@example.com\",\n    };\n  },\n});\n\nconst processUserStep = new LegacyStep({\n  id: \"processUser\",\n  execute: async ({ context }) => {\n    // TypeScript will infer the correct type from fetchUserStep's outputSchema\n    const userData = context.getStepResult(fetchUserStep);\n\n    return {\n      processed: true,\n      userName: userData?.name,\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow({\n  name: \"user-workflow\",\n});\n\nworkflow.step(fetchUserStep).then(processUserStep).commit();\n```\n\n### Using Variable Mapping\n\nVariable mapping is an explicit way to define data flow between steps.\nThis approach makes dependencies clear and provides good type safety.\nThe data injected into the step is available in the `context.inputData` object, and typed based on the `inputSchema` of the step.\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/variable-mapping.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nconst fetchUserStep = new LegacyStep({\n  id: \"fetchUser\",\n  outputSchema: z.object({\n    userId: z.string(),\n    name: z.string(),\n    email: z.string(),\n  }),\n  execute: async () => {\n    return {\n      userId: \"user123\",\n      name: \"John Doe\",\n      email: \"john@example.com\",\n    };\n  },\n});\n\nconst sendEmailStep = new LegacyStep({\n  id: \"sendEmail\",\n  inputSchema: z.object({\n    recipientEmail: z.string(),\n    recipientName: z.string(),\n  }),\n  execute: async ({ context }) => {\n    const { recipientEmail, recipientName } = context.inputData;\n\n    // Send email logic here\n    return {\n      status: \"sent\",\n      to: recipientEmail,\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow({\n  name: \"email-workflow\",\n});\n\nworkflow\n  .step(fetchUserStep)\n  .then(sendEmailStep, {\n    variables: {\n      // Map specific fields from fetchUser to sendEmail inputs\n      recipientEmail: { step: fetchUserStep, path: \"email\" },\n      recipientName: { step: fetchUserStep, path: \"name\" },\n    },\n  })\n  .commit();\n```\n\nFor more details on variable mapping, see the [Data Mapping with Workflow Variables](./variables.mdx) documentation.\n\n### Using the Context Object\n\nThe context object provides direct access to all step results and their outputs. This approach is more flexible but requires careful handling to maintain type safety.\nYou can access step results directly through the `context.steps` object:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/context-access.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nconst processOrderStep = new LegacyStep({\n  id: \"processOrder\",\n  execute: async ({ context }) => {\n    // Access data from a previous step\n    let userData: { name: string; userId: string };\n    if (context.steps[\"fetchUser\"]?.status === \"success\") {\n      userData = context.steps.fetchUser.output;\n    } else {\n      throw new Error(\"User data not found\");\n    }\n\n    return {\n      orderId: \"order123\",\n      userId: userData.userId,\n      status: \"processing\",\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow({\n  name: \"order-workflow\",\n});\n\nworkflow.step(fetchUserStep).then(processOrderStep).commit();\n```\n\n### Workflow-Level Type Safety\n\nFor comprehensive type safety across your entire workflow, you can define types for all steps and pass them to the Workflow\nThis allows you to get type safety for the context object on conditions, and on step results in the final workflow output.\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/workflow-typing.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Create steps with typed outputs\nconst fetchUserStep = new LegacyStep({\n  id: \"fetchUser\",\n  outputSchema: z.object({\n    userId: z.string(),\n    name: z.string(),\n    email: z.string(),\n  }),\n  execute: async () => {\n    return {\n      userId: \"user123\",\n      name: \"John Doe\",\n      email: \"john@example.com\",\n    };\n  },\n});\n\nconst processOrderStep = new LegacyStep({\n  id: \"processOrder\",\n  execute: async ({ context }) => {\n    // TypeScript knows the shape of userData\n    const userData = context.getStepResult(fetchUserStep);\n\n    return {\n      orderId: \"order123\",\n      status: \"processing\",\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow<\n  [typeof fetchUserStep, typeof processOrderStep]\n>({\n  name: \"typed-workflow\",\n});\n\nworkflow\n  .step(fetchUserStep)\n  .then(processOrderStep)\n  .until(async ({ context }) => {\n    // TypeScript knows the shape of userData here\n    const res = context.getStepResult(\"fetchUser\");\n    return res?.userId === \"123\";\n  }, processOrderStep)\n  .commit();\n```\n\n### Accessing Trigger Data\n\nIn addition to step results, you can access the original trigger data that started the workflow:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/trigger-data.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Define trigger schema\nconst triggerSchema = z.object({\n  customerId: z.string(),\n  orderItems: z.array(z.string()),\n});\n\ntype TriggerType = z.infer<typeof triggerSchema>;\n\nconst processOrderStep = new LegacyStep({\n  id: \"processOrder\",\n  execute: async ({ context }) => {\n    // Access trigger data with type safety\n    const triggerData = context.getStepResult<TriggerType>(\"trigger\");\n\n    return {\n      customerId: triggerData?.customerId,\n      itemCount: triggerData?.orderItems.length || 0,\n      status: \"processing\",\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow({\n  name: \"order-workflow\",\n  triggerSchema,\n});\n\nworkflow.step(processOrderStep).commit();\n```\n\n### Accessing Resume Data\n\nThe data injected into the step is available in the `context.inputData` object, and typed based on the `inputSchema` of the step.\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/resume-data.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nconst processOrderStep = new LegacyStep({\n  id: \"processOrder\",\n  inputSchema: z.object({\n    orderId: z.string(),\n  }),\n  execute: async ({ context, suspend }) => {\n    const { orderId } = context.inputData;\n\n    if (!orderId) {\n      await suspend();\n      return;\n    }\n\n    return {\n      orderId,\n      status: \"processed\",\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow({\n  name: \"order-workflow\",\n});\n\nworkflow.step(processOrderStep).commit();\n\nconst run = workflow.createRun();\nconst result = await run.start();\n\nconst resumedResult = await workflow.resume({\n  runId: result.runId,\n  stepId: \"processOrder\",\n  inputData: {\n    orderId: \"123\",\n  },\n});\n\nconsole.log({ resumedResult });\n```\n\n### Accessing Workflow Results\n\nYou can get typed access to the results of a workflow by injecting the step types into the `Workflow` type params:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/get-results.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nconst fetchUserStep = new LegacyStep({\n  id: \"fetchUser\",\n  outputSchema: z.object({\n    userId: z.string(),\n    name: z.string(),\n    email: z.string(),\n  }),\n  execute: async () => {\n    return {\n      userId: \"user123\",\n      name: \"John Doe\",\n      email: \"john@example.com\",\n    };\n  },\n});\n\nconst processOrderStep = new LegacyStep({\n  id: \"processOrder\",\n  outputSchema: z.object({\n    orderId: z.string(),\n    status: z.string(),\n  }),\n  execute: async ({ context }) => {\n    const userData = context.getStepResult(fetchUserStep);\n    return {\n      orderId: \"order123\",\n      status: \"processing\",\n    };\n  },\n});\n\nconst workflow = new LegacyWorkflow<\n  [typeof fetchUserStep, typeof processOrderStep]\n>({\n  name: \"typed-workflow\",\n});\n\nworkflow.step(fetchUserStep).then(processOrderStep).commit();\n\nconst run = workflow.createRun();\nconst result = await run.start();\n\n// The result is a discriminated union of the step results\n// So it needs to be narrowed down via status checks\nif (result.results.processOrder.status === \"success\") {\n  // TypeScript will know the shape of the results\n  const orderId = result.results.processOrder.output.orderId;\n  console.log({ orderId });\n}\n\nif (result.results.fetchUser.status === \"success\") {\n  const userId = result.results.fetchUser.output.userId;\n  console.log({ userId });\n}\n```\n\n### Best Practices for Data Flow\n\n1. **Use getStepResult with Step References for Type Safety**\n\n   - Ensures TypeScript can infer the correct types\n   - Catches type errors at compile time\n\n2. \\*_Use Variable Mapping for Explicit Dependencies_\n\n   - Makes data flow clear and maintainable\n   - Provides good documentation of step dependencies\n\n3. **Define Output Schemas for Steps**\n\n   - Validates data at runtime\n   - Validates return type of the `execute` function\n   - Improves type inference in TypeScript\n\n4. **Handle Missing Data Gracefully**\n\n   - Always check if step results exist before accessing properties\n   - Provide fallback values for optional data\n\n5. **Keep Data Transformations Simple**\n   - Transform data in dedicated steps rather than in variable mappings\n   - Makes workflows easier to test and debug\n\n### Comparison of Data Flow Methods\n\n| Method           | Type Safety | Explicitness | Use Case                                          |\n| ---------------- | ----------- | ------------ | ------------------------------------------------- |\n| getStepResult    | Highest     | High         | Complex workflows with strict typing requirements |\n| Variable Mapping | High        | High         | When dependencies need to be clear and explicit   |\n| context.steps    | Medium      | Low          | Quick access to step data in simple workflows     |\n\nBy choosing the right data flow method for your use case, you can create workflows that are both type-safe and maintainable.\n\n\n---\ntitle: \"Dynamic Workflows (Legacy) | Mastra Docs\"\ndescription: \"Learn how to create dynamic workflows within legacy workflow steps, allowing for flexible workflow creation based on runtime conditions.\"\n---\n\n# Dynamic Workflows (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/dynamic-workflows\n\nThis guide demonstrates how to create dynamic workflows within a workflow step. This advanced pattern allows you to create and execute workflows on the fly based on runtime conditions.\n\n## Overview\n\nDynamic workflows are useful when you need to create workflows based on runtime data.\n\n## Implementation\n\nThe key to creating dynamic workflows is accessing the Mastra instance from within a step's `execute` function and using it to create and run a new workflow.\n\n### Basic Example\n\n```typescript\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nconst isMastra = (mastra: any): mastra is Mastra => {\n  return mastra && typeof mastra === \"object\" && mastra instanceof Mastra;\n};\n\n// Step that creates and runs a dynamic workflow\nconst createDynamicWorkflow = new LegacyStep({\n  id: \"createDynamicWorkflow\",\n  outputSchema: z.object({\n    dynamicWorkflowResult: z.any(),\n  }),\n  execute: async ({ context, mastra }) => {\n    if (!mastra) {\n      throw new Error(\"Mastra instance not available\");\n    }\n\n    if (!isMastra(mastra)) {\n      throw new Error(\"Invalid Mastra instance\");\n    }\n\n    const inputData = context.triggerData.inputData;\n\n    // Create a new dynamic workflow\n    const dynamicWorkflow = new LegacyWorkflow({\n      name: \"dynamic-workflow\",\n      mastra, // Pass the mastra instance to the new workflow\n      triggerSchema: z.object({\n        dynamicInput: z.string(),\n      }),\n    });\n\n    // Define steps for the dynamic workflow\n    const dynamicStep = new LegacyStep({\n      id: \"dynamicStep\",\n      execute: async ({ context }) => {\n        const dynamicInput = context.triggerData.dynamicInput;\n        return {\n          processedValue: `Processed: ${dynamicInput}`,\n        };\n      },\n    });\n\n    // Build and commit the dynamic workflow\n    dynamicWorkflow.step(dynamicStep).commit();\n\n    // Create a run and execute the dynamic workflow\n    const run = dynamicWorkflow.createRun();\n    const result = await run.start({\n      triggerData: {\n        dynamicInput: inputData,\n      },\n    });\n\n    let dynamicWorkflowResult;\n\n    if (result.results[\"dynamicStep\"]?.status === \"success\") {\n      dynamicWorkflowResult =\n        result.results[\"dynamicStep\"]?.output.processedValue;\n    } else {\n      throw new Error(\"Dynamic workflow failed\");\n    }\n\n    // Return the result from the dynamic workflow\n    return {\n      dynamicWorkflowResult,\n    };\n  },\n});\n\n// Main workflow that uses the dynamic workflow creator\nconst mainWorkflow = new LegacyWorkflow({\n  name: \"main-workflow\",\n  triggerSchema: z.object({\n    inputData: z.string(),\n  }),\n  mastra: new Mastra(),\n});\n\nmainWorkflow.step(createDynamicWorkflow).commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { mainWorkflow },\n});\n\nconst run = mainWorkflow.createRun();\nconst result = await run.start({\n  triggerData: {\n    inputData: \"test\",\n  },\n});\n```\n\n## Advanced Example: Workflow Factory\n\nYou can create a workflow factory that generates different workflows based on input parameters:\n\n```typescript\nconst isMastra = (mastra: any): mastra is Mastra => {\n  return mastra && typeof mastra === \"object\" && mastra instanceof Mastra;\n};\n\nconst workflowFactory = new LegacyStep({\n  id: \"workflowFactory\",\n  inputSchema: z.object({\n    workflowType: z.enum([\"simple\", \"complex\"]),\n    inputData: z.string(),\n  }),\n  outputSchema: z.object({\n    result: z.any(),\n  }),\n  execute: async ({ context, mastra }) => {\n    if (!mastra) {\n      throw new Error(\"Mastra instance not available\");\n    }\n\n    if (!isMastra(mastra)) {\n      throw new Error(\"Invalid Mastra instance\");\n    }\n\n    // Create a new dynamic workflow based on the type\n    const dynamicWorkflow = new LegacyWorkflow({\n      name: `dynamic-${context.workflowType}-workflow`,\n      mastra,\n      triggerSchema: z.object({\n        input: z.string(),\n      }),\n    });\n\n    if (context.workflowType === \"simple\") {\n      // Simple workflow with a single step\n      const simpleStep = new Step({\n        id: \"simpleStep\",\n        execute: async ({ context }) => {\n          return {\n            result: `Simple processing: ${context.triggerData.input}`,\n          };\n        },\n      });\n\n      dynamicWorkflow.step(simpleStep).commit();\n    } else {\n      // Complex workflow with multiple steps\n      const step1 = new LegacyStep({\n        id: \"step1\",\n        outputSchema: z.object({\n          intermediateResult: z.string(),\n        }),\n        execute: async ({ context }) => {\n          return {\n            intermediateResult: `First processing: ${context.triggerData.input}`,\n          };\n        },\n      });\n\n      const step2 = new LegacyStep({\n        id: \"step2\",\n        execute: async ({ context }) => {\n          const intermediate = context.getStepResult(step1).intermediateResult;\n          return {\n            finalResult: `Second processing: ${intermediate}`,\n          };\n        },\n      });\n\n      dynamicWorkflow.step(step1).then(step2).commit();\n    }\n\n    // Execute the dynamic workflow\n    const run = dynamicWorkflow.createRun();\n    const result = await run.start({\n      triggerData: {\n        input: context.inputData,\n      },\n    });\n\n    // Return the appropriate result based on workflow type\n    if (context.workflowType === \"simple\") {\n      return {\n        // @ts-ignore\n        result: result.results[\"simpleStep\"]?.output,\n      };\n    } else {\n      return {\n        // @ts-ignore\n        result: result.results[\"step2\"]?.output,\n      };\n    }\n  },\n});\n```\n\n## Important Considerations\n\n1. **Mastra Instance**: The `mastra` parameter in the `execute` function provides access to the Mastra instance, which is essential for creating dynamic workflows.\n\n2. **Error Handling**: Always check if the Mastra instance is available before attempting to create a dynamic workflow.\n\n3. **Resource Management**: Dynamic workflows consume resources, so be mindful of creating too many workflows in a single execution.\n\n4. **Workflow Lifecycle**: Dynamic workflows are not automatically registered with the main Mastra instance. They exist only for the duration of the step execution unless you explicitly register them.\n\n5. **Debugging**: Debugging dynamic workflows can be challenging. Consider adding detailed logging to track their creation and execution.\n\n## Use Cases\n\n- **Conditional Workflow Selection**: Choose different workflow patterns based on input data\n- **Parameterized Workflows**: Create workflows with dynamic configurations\n- **Workflow Templates**: Use templates to generate specialized workflows\n- **Multi-tenant Applications**: Create isolated workflows for different tenants\n\n## Conclusion\n\nDynamic workflows provide a powerful way to create flexible, adaptable workflow systems. By leveraging the Mastra instance within step execution, you can create workflows that respond to runtime conditions and requirements.\n\n\n---\ntitle: \"Error Handling in Workflows (Legacy) | Mastra Docs\"\ndescription: \"Learn how to handle errors in Mastra legacy workflows using step retries, conditional branching, and monitoring.\"\n---\n\n# Error Handling in Workflows (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/error-handling\n\nRobust error handling is essential for production workflows. Mastra provides several mechanisms to handle errors gracefully, allowing your workflows to recover from failures or gracefully degrade when necessary.\n\n## Overview\n\nError handling in Mastra workflows can be implemented using:\n\n1. **Step Retries** - Automatically retry failed steps\n2. **Conditional Branching** - Create alternative paths based on step success or failure\n3. **Error Monitoring** - Watch workflows for errors and handle them programmatically\n4. **Result Status Checks** - Check the status of previous steps in subsequent steps\n\n## Step Retries\n\nMastra provides a built-in retry mechanism for steps that fail due to transient errors. This is particularly useful for steps that interact with external services or resources that might experience temporary unavailability.\n\n### Basic Retry Configuration\n\nYou can configure retries at the workflow level or for individual steps:\n\n```typescript\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\n\n// Workflow-level retry configuration\nconst workflow = new LegacyWorkflow({\n  name: \"my-workflow\",\n  retryConfig: {\n    attempts: 3, // Number of retry attempts\n    delay: 1000, // Delay between retries in milliseconds\n  },\n});\n\n// Step-level retry configuration (overrides workflow-level)\nconst apiStep = new LegacyStep({\n  id: \"callApi\",\n  execute: async () => {\n    // API call that might fail\n  },\n  retryConfig: {\n    attempts: 5, // This step will retry up to 5 times\n    delay: 2000, // With a 2-second delay between retries\n  },\n});\n```\n\nFor more details about step retries, see the [Step Retries](../../reference/legacyWorkflows/step-retries.mdx) reference.\n\n## Conditional Branching\n\nYou can create alternative workflow paths based on the success or failure of previous steps using conditional logic:\n\n```typescript\n// Create a workflow with conditional branching\nconst workflow = new LegacyWorkflow({\n  name: \"error-handling-workflow\",\n});\n\nworkflow\n  .step(fetchDataStep)\n  .then(processDataStep, {\n    // Only execute processDataStep if fetchDataStep was successful\n    when: ({ context }) => {\n      return context.steps.fetchDataStep?.status === \"success\";\n    },\n  })\n  .then(fallbackStep, {\n    // Execute fallbackStep if fetchDataStep failed\n    when: ({ context }) => {\n      return context.steps.fetchDataStep?.status === \"failed\";\n    },\n  })\n  .commit();\n```\n\n## Error Monitoring\n\nYou can monitor workflows for errors using the `watch` method:\n\n```typescript\nconst { start, watch } = workflow.createRun();\n\nwatch(async ({ results }) => {\n  // Check for any failed steps\n  const failedSteps = Object.entries(results)\n    .filter(([_, step]) => step.status === \"failed\")\n    .map(([stepId]) => stepId);\n\n  if (failedSteps.length > 0) {\n    console.error(`Workflow has failed steps: ${failedSteps.join(\", \")}`);\n    // Take remedial action, such as alerting or logging\n  }\n});\n\nawait start();\n```\n\n## Handling Errors in Steps\n\nWithin a step's execution function, you can handle errors programmatically:\n\n```typescript\nconst robustStep = new LegacyStep({\n  id: \"robustStep\",\n  execute: async ({ context }) => {\n    try {\n      // Attempt the primary operation\n      const result = await someRiskyOperation();\n      return { success: true, data: result };\n    } catch (error) {\n      // Log the error\n      console.error(\"Operation failed:\", error);\n\n      // Return a graceful fallback result instead of throwing\n      return {\n        success: false,\n        error: error.message,\n        fallbackData: \"Default value\",\n      };\n    }\n  },\n});\n```\n\n## Checking Previous Step Results\n\nYou can make decisions based on the results of previous steps:\n\n```typescript\nconst finalStep = new LegacyStep({\n  id: \"finalStep\",\n  execute: async ({ context }) => {\n    // Check results of previous steps\n    const step1Success = context.steps.step1?.status === \"success\";\n    const step2Success = context.steps.step2?.status === \"success\";\n\n    if (step1Success && step2Success) {\n      // All steps succeeded\n      return { status: \"complete\", result: \"All operations succeeded\" };\n    } else if (step1Success) {\n      // Only step1 succeeded\n      return { status: \"partial\", result: \"Partial completion\" };\n    } else {\n      // Critical failure\n      return { status: \"failed\", result: \"Critical steps failed\" };\n    }\n  },\n});\n```\n\n## Best Practices for Error Handling\n\n1. **Use retries for transient failures**: Configure retry policies for steps that might experience temporary issues.\n\n2. **Provide fallback paths**: Design workflows with alternative paths for when critical steps fail.\n\n3. **Be specific about error scenarios**: Use different handling strategies for different types of errors.\n\n4. **Log errors comprehensively**: Include context information when logging errors to aid in debugging.\n\n5. **Return meaningful data on failure**: When a step fails, return structured data about the failure to help downstream steps make decisions.\n\n6. **Consider idempotency**: Ensure steps can be safely retried without causing duplicate side effects.\n\n7. **Monitor workflow execution**: Use the `watch` method to actively monitor workflow execution and detect errors early.\n\n## Advanced Error Handling\n\nFor more complex error handling scenarios, consider:\n\n- **Implementing circuit breakers**: If a step fails repeatedly, stop retrying and use a fallback strategy\n- **Adding timeout handling**: Set time limits for steps to prevent workflows from hanging indefinitely\n- **Creating dedicated error recovery workflows**: For critical workflows, create separate recovery workflows that can be triggered when the main workflow fails\n\n## Related\n\n- [Step Retries Reference](../../reference/legacyWorkflows/step-retries.mdx)\n- [Watch Method Reference](../../reference/legacyWorkflows/watch.mdx)\n- [Step Conditions](../../reference/legacyWorkflows/step-condition.mdx)\n- [Control Flow](./control-flow.mdx)\n\n\n# Nested Workflows (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/nested-workflows\n\nMastra allows you to use workflows as steps within other workflows, enabling you to create modular and reusable workflow components. This feature helps in organizing complex workflows into smaller, manageable pieces and promotes code reuse.\n\nIt is also visually easier to understand the flow of a workflow when you can see the nested workflows as steps in the parent workflow.\n\n## Basic Usage\n\nYou can use a workflow as a step directly in another workflow using the `step()` method:\n\n```typescript\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\n\n// Create a nested workflow\nconst nestedWorkflow = new LegacyWorkflow({ name: \"nested-workflow\" })\n  .step(stepA)\n  .then(stepB)\n  .commit();\n\n// Use the nested workflow in a parent workflow\nconst parentWorkflow = new LegacyWorkflow({ name: \"parent-workflow\" })\n  .step(nestedWorkflow, {\n    variables: {\n      city: {\n        step: \"trigger\",\n        path: \"myTriggerInput\",\n      },\n    },\n  })\n  .then(stepC)\n  .commit();\n```\n\nWhen a workflow is used as a step:\n\n- It is automatically converted to a step using the workflow's name as the step ID\n- The workflow's results are available in the parent workflow's context\n- The nested workflow's steps are executed in their defined order\n\n## Accessing Results\n\nResults from a nested workflow are available in the parent workflow's context under the nested workflow's name. The results include all step outputs from the nested workflow:\n\n```typescript\nconst { results } = await parentWorkflow.start();\n// Access nested workflow results\nconst nestedWorkflowResult = results[\"nested-workflow\"];\nif (nestedWorkflowResult.status === \"success\") {\n  const nestedResults = nestedWorkflowResult.output.results;\n}\n```\n\n## Control Flow with Nested Workflows\n\nNested workflows support all the control flow features available to regular steps:\n\n### Parallel Execution\n\nMultiple nested workflows can be executed in parallel:\n\n```typescript\nparentWorkflow\n  .step(nestedWorkflowA)\n  .step(nestedWorkflowB)\n  .after([nestedWorkflowA, nestedWorkflowB])\n  .step(finalStep);\n```\n\nOr using `step()` with an array of workflows:\n\n```typescript\nparentWorkflow.step([nestedWorkflowA, nestedWorkflowB]).then(finalStep);\n```\n\nIn this case, `then()` will implicitly wait for all the workflows to finish before executing the final step.\n\n### If-Else Branching\n\nNested workflows can be used in if-else branches using the new syntax that accepts both branches as arguments:\n\n```typescript\n// Create nested workflows for different paths\nconst workflowA = new LegacyWorkflow({ name: \"workflow-a\" })\n  .step(stepA1)\n  .then(stepA2)\n  .commit();\n\nconst workflowB = new LegacyWorkflow({ name: \"workflow-b\" })\n  .step(stepB1)\n  .then(stepB2)\n  .commit();\n\n// Use the new if-else syntax with nested workflows\nparentWorkflow\n  .step(initialStep)\n  .if(\n    async ({ context }) => {\n      // Your condition here\n      return someCondition;\n    },\n    workflowA, // if branch\n    workflowB, // else branch\n  )\n  .then(finalStep)\n  .commit();\n```\n\nThe new syntax is more concise and clearer when working with nested workflows. When the condition is:\n\n- `true`: The first workflow (if branch) is executed\n- `false`: The second workflow (else branch) is executed\n\nThe skipped workflow will have a status of `skipped` in the results:\n\nThe `.then(finalStep)` call following the if-else block will merge the if and else branches back into a single execution path.\n\n### Looping\n\nNested workflows can use `.until()` and `.while()` loops same as any other step. One interesting new pattern is to pass a workflow directly as the loop-back argument to keep executing that nested workflow until something is true about its results:\n\n```typescript\nparentWorkflow\n  .step(firstStep)\n  .while(\n    ({ context }) =>\n      context.getStepResult(\"nested-workflow\").output.results.someField ===\n      \"someValue\",\n    nestedWorkflow,\n  )\n  .step(finalStep)\n  .commit();\n```\n\n## Watching Nested Workflows\n\nYou can watch the state changes of nested workflows using the `watch` method on the parent workflow. This is useful for monitoring the progress and state transitions of complex workflows:\n\n```typescript\nconst parentWorkflow = new LegacyWorkflow({ name: \"parent-workflow\" })\n  .step([nestedWorkflowA, nestedWorkflowB])\n  .then(finalStep)\n  .commit();\n\nconst run = parentWorkflow.createRun();\nconst unwatch = parentWorkflow.watch((state) => {\n  console.log(\"Current state:\", state.value);\n  // Access nested workflow states in state.context\n});\n\nawait run.start();\nunwatch(); // Stop watching when done\n```\n\n## Suspending and Resuming\n\nNested workflows support suspension and resumption, allowing you to pause and continue workflow execution at specific points. You can suspend either the entire nested workflow or specific steps within it:\n\n```typescript\n// Define a step that may need to suspend\nconst suspendableStep = new LegacyStep({\n  id: \"other\",\n  description: \"Step that may need to suspend\",\n  execute: async ({ context, suspend }) => {\n    if (!wasSuspended) {\n      wasSuspended = true;\n      await suspend();\n    }\n    return { other: 26 };\n  },\n});\n\n// Create a nested workflow with suspendable steps\nconst nestedWorkflow = new LegacyWorkflow({ name: \"nested-workflow-a\" })\n  .step(startStep)\n  .then(suspendableStep)\n  .then(finalStep)\n  .commit();\n\n// Use in parent workflow\nconst parentWorkflow = new LegacyWorkflow({ name: \"parent-workflow\" })\n  .step(beginStep)\n  .then(nestedWorkflow)\n  .then(lastStep)\n  .commit();\n\n// Start the workflow\nconst run = parentWorkflow.createRun();\nconst { runId, results } = await run.start({ triggerData: { startValue: 1 } });\n\n// Check if a specific step in the nested workflow is suspended\nif (results[\"nested-workflow-a\"].output.results.other.status === \"suspended\") {\n  // Resume the specific suspended step using dot notation\n  const resumedResults = await run.resume({\n    stepId: \"nested-workflow-a.other\",\n    context: { startValue: 1 },\n  });\n\n  // The resumed results will contain the completed nested workflow\n  expect(resumedResults.results[\"nested-workflow-a\"].output.results).toEqual({\n    start: { output: { newValue: 1 }, status: \"success\" },\n    other: { output: { other: 26 }, status: \"success\" },\n    final: { output: { finalValue: 27 }, status: \"success\" },\n  });\n}\n```\n\nWhen resuming a nested workflow:\n\n- Use the nested workflow's name as the `stepId` when calling `resume()` to resume the entire workflow\n- Use dot notation (`nested-workflow.step-name`) to resume a specific step within the nested workflow\n- The nested workflow will continue from the suspended step with the provided context\n- You can check the status of specific steps in the nested workflow's results using `results[\"nested-workflow\"].output.results`\n\n## Result Schemas and Mapping\n\nNested workflows can define their result schema and mapping, which helps in type safety and data transformation. This is particularly useful when you want to ensure the nested workflow's output matches a specific structure or when you need to transform the results before they're used in the parent workflow.\n\n```typescript\n// Create a nested workflow with result schema and mapping\nconst nestedWorkflow = new LegacyWorkflow({\n  name: \"nested-workflow\",\n  result: {\n    schema: z.object({\n      total: z.number(),\n      items: z.array(\n        z.object({\n          id: z.string(),\n          value: z.number(),\n        }),\n      ),\n    }),\n    mapping: {\n      // Map values from step results using variables syntax\n      total: { step: \"step-a\", path: \"count\" },\n      items: { step: \"step-b\", path: \"items\" },\n    },\n  },\n})\n  .step(stepA)\n  .then(stepB)\n  .commit();\n\n// Use in parent workflow with type-safe results\nconst parentWorkflow = new LegacyWorkflow({ name: \"parent-workflow\" })\n  .step(nestedWorkflow)\n  .then(async ({ context }) => {\n    const result = context.getStepResult(\"nested-workflow\");\n    // TypeScript knows the structure of result\n    console.log(result.total); // number\n    console.log(result.items); // Array<{ id: string, value: number }>\n    return { success: true };\n  })\n  .commit();\n```\n\n## Best Practices\n\n1. **Modularity**: Use nested workflows to encapsulate related steps and create reusable workflow components.\n2. **Naming**: Give nested workflows descriptive names as they will be used as step IDs in the parent workflow.\n3. **Error Handling**: Nested workflows propagate their errors to the parent workflow, so handle errors appropriately.\n4. **State Management**: Each nested workflow maintains its own state but can access the parent workflow's context.\n5. **Suspension**: When using suspension in nested workflows, consider the entire workflow's state and handle resumption appropriately.\n\n## Example\n\nHere's a complete example showing various features of nested workflows:\n\n```typescript\nconst workflowA = new LegacyWorkflow({\n  name: \"workflow-a\",\n  result: {\n    schema: z.object({\n      activities: z.string(),\n    }),\n    mapping: {\n      activities: {\n        step: planActivities,\n        path: \"activities\",\n      },\n    },\n  },\n})\n  .step(fetchWeather)\n  .then(planActivities)\n  .commit();\n\nconst workflowB = new LegacyWorkflow({\n  name: \"workflow-b\",\n  result: {\n    schema: z.object({\n      activities: z.string(),\n    }),\n    mapping: {\n      activities: {\n        step: planActivities,\n        path: \"activities\",\n      },\n    },\n  },\n})\n  .step(fetchWeather)\n  .then(planActivities)\n  .commit();\n\nconst weatherWorkflow = new LegacyWorkflow({\n  name: \"weather-workflow\",\n  triggerSchema: z.object({\n    cityA: z.string().describe(\"The city to get the weather for\"),\n    cityB: z.string().describe(\"The city to get the weather for\"),\n  }),\n  result: {\n    schema: z.object({\n      activitiesA: z.string(),\n      activitiesB: z.string(),\n    }),\n    mapping: {\n      activitiesA: {\n        step: workflowA,\n        path: \"result.activities\",\n      },\n      activitiesB: {\n        step: workflowB,\n        path: \"result.activities\",\n      },\n    },\n  },\n})\n  .step(workflowA, {\n    variables: {\n      city: {\n        step: \"trigger\",\n        path: \"cityA\",\n      },\n    },\n  })\n  .step(workflowB, {\n    variables: {\n      city: {\n        step: \"trigger\",\n        path: \"cityB\",\n      },\n    },\n  });\n\nweatherWorkflow.commit();\n```\n\nIn this example:\n\n1. We define schemas for type safety across all workflows\n2. Each step has proper input and output schemas\n3. The nested workflows have their own trigger schemas and result mappings\n4. Data is passed through using variables syntax in the `.step()` calls\n5. The main workflow combines data from both nested workflows\n\n\n---\ntitle: \"Handling Complex LLM Operations | Workflows (Legacy) | Mastra\"\ndescription: \"Workflows in Mastra help you orchestrate complex sequences of operations with features like branching, parallel execution, resource suspension, and more.\"\n---\n\n# Handling Complex LLM Operations with Workflows (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/overview\n\nAll the legacy workflow documentation is available on the links below.\n\n- [Steps](/docs/workflows-legacy/steps/)\n- [Control Flow](/docs/workflows-legacy/control-flow/)\n- [Variables](/docs/workflows-legacy/variables/)\n- [Suspend & Resume](/docs/workflows-legacy/suspend-and-resume/)\n- [Dynamic Workflows](/docs/workflows-legacy/dynamic-workflows/)\n- [Error Handling](/docs/workflows-legacy/error-handling/)\n- [Nested Workflows](/docs/workflows-legacy/nested-workflows/)\n- [Runtime/Dynamic Variables](/docs/workflows-legacy/runtime-variables/)\n\nWorkflows in Mastra help you orchestrate complex sequences of operations with features like branching, parallel execution, resource suspension, and more.\n\n## When to use workflows\n\nMost AI applications need more than a single call to a language model. You may want to run multiple steps, conditionally skip certain paths, or even pause execution altogether until you receive user input. Sometimes your agent tool calling is not accurate enough.\n\nMastra's workflow system provides:\n\n- A standardized way to define steps and link them together.\n- Support for both simple (linear) and advanced (branching, parallel) paths.\n- Debugging and observability features to track each workflow run.\n\n## Example\n\nTo create a workflow, you define one or more steps, link them, and then commit the workflow before starting it.\n\n### Breaking Down the Workflow (Legacy)\n\nLet's examine each part of the workflow creation process:\n\n#### 1. Creating the Workflow\n\nHere's how you define a workflow in Mastra. The `name` field determines the workflow's API endpoint (`/workflows/$NAME/`), while the `triggerSchema` defines the structure of the workflow's trigger data:\n\n```ts filename=\"src/mastra/workflow/index.ts\"\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\n\nconst myWorkflow = new LegacyWorkflow({\n  name: \"my-workflow\",\n  triggerSchema: z.object({\n    inputValue: z.number(),\n  }),\n});\n```\n\n#### 2. Defining Steps\n\nNow, we'll define the workflow's steps. Each step can have its own input and output schemas. Here, `stepOne` doubles an input value, and `stepTwo` increments that result if `stepOne` was successful. (To keep things simple, we aren't making any LLM calls in this example):\n\n```ts filename=\"src/mastra/workflow/index.ts\"\nconst stepOne = new LegacyStep({\n  id: \"stepOne\",\n  outputSchema: z.object({\n    doubledValue: z.number(),\n  }),\n  execute: async ({ context }) => {\n    const doubledValue = context.triggerData.inputValue * 2;\n    return { doubledValue };\n  },\n});\n\nconst stepTwo = new LegacyStep({\n  id: \"stepTwo\",\n  execute: async ({ context }) => {\n    const doubledValue = context.getStepResult(stepOne)?.doubledValue;\n    if (!doubledValue) {\n      return { incrementedValue: 0 };\n    }\n    return {\n      incrementedValue: doubledValue + 1,\n    };\n  },\n});\n```\n\n#### 3. Linking Steps\n\nNow, let's create the control flow, and \"commit\" (finalize the workflow). In this case, `stepOne` runs first and is followed by `stepTwo`.\n\n```ts filename=\"src/mastra/workflow/index.ts\"\nmyWorkflow.step(stepOne).then(stepTwo).commit();\n```\n\n### Register the Workflow\n\nRegister your workflow with Mastra to enable logging and telemetry:\n\n```ts showLineNumbers filename=\"src/mastra/index.ts\"\nimport { Mastra } from \"@mastra/core\";\n\nexport const mastra = new Mastra({\n  legacy_workflows: { myWorkflow },\n});\n```\n\nThe workflow can also have the mastra instance injected into the context in the case where you need to create dynamic workflows:\n\n```ts filename=\"src/mastra/workflow/index.ts\"\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\n\nconst mastra = new Mastra();\n\nconst myWorkflow = new LegacyWorkflow({\n  name: \"my-workflow\",\n  mastra,\n});\n```\n\n### Executing the Workflow\n\nExecute your workflow programmatically or via API:\n\n```ts showLineNumbers filename=\"src/mastra/run-workflow.ts\" copy\nimport { mastra } from \"./index\";\n\n// Get the workflow\nconst myWorkflow = mastra.legacy_getWorkflow(\"myWorkflow\");\nconst { runId, start } = myWorkflow.createRun();\n\n// Start the workflow execution\nawait start({ triggerData: { inputValue: 45 } });\n```\n\nOr use the API (requires running `mastra dev`):\n\n// Create workflow run\n\n```bash\ncurl --location 'http://localhost:5000/api/workflows/myWorkflow/start-async' \\\n     --header 'Content-Type: application/json' \\\n     --data '{\n       \"inputValue\": 45\n     }'\n```\n\nThis example shows the essentials: define your workflow, add steps, commit the workflow, then execute it.\n\n## Defining Steps\n\nThe basic building block of a workflow [is a step](./steps.mdx). Steps are defined using schemas for inputs and outputs, and can fetch prior step results.\n\n## Control Flow\n\nWorkflows let you define a [control flow](./control-flow.mdx) to chain steps together in with parallel steps, branching paths, and more.\n\n## Workflow Variables\n\nWhen you need to map data between steps or create dynamic data flows, [workflow variables](./variables.mdx) provide a powerful mechanism for passing information from one step to another and accessing nested properties within step outputs.\n\n## Suspend and Resume\n\nWhen you need to pause execution for external data, user input, or asynchronous events, Mastra [supports suspension at any step](./suspend-and-resume.mdx), persisting the state of the workflow so you can resume it later.\n\n## Observability and Debugging\n\nMastra workflows automatically [log the input and output of each step within a workflow run](../../reference/observability/otel-config.mdx), allowing you to send this data to your preferred logging, telemetry, or observability tools.\n\nYou can:\n\n- Track the status of each step (e.g., `success`, `error`, or `suspended`).\n- Store run-specific metadata for analysis.\n- Integrate with third-party observability platforms like Datadog or New Relic by forwarding logs.\n\n## More Resources\n\n- [Sequential Steps workflow example](../../examples/workflows_legacy/sequential-steps.mdx)\n- [Parallel Steps workflow example](../../examples/workflows_legacy/parallel-steps.mdx)\n- [Branching Paths workflow example](../../examples/workflows_legacy/branching-paths.mdx)\n- [Workflow Variables example](../../examples/workflows_legacy/workflow-variables.mdx)\n- [Cyclical Dependencies workflow example](../../examples/workflows_legacy/cyclical-dependencies.mdx)\n- [Suspend and Resume workflow example](../../examples/workflows_legacy/suspend-and-resume.mdx)\n\n\n---\ntitle: \"Runtime variables - dependency injection | Workflows (Legacy) | Mastra Docs\"\ndescription: Learn how to use Mastra's dependency injection system to provide runtime configuration to workflows and steps.\n---\n\n# Workflow Runtime Variables (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/runtime-variables\n\nMastra provides a powerful dependency injection system that enables you to configure your workflows and steps with runtime variables. This feature is essential for creating flexible and reusable workflows that can adapt their behavior based on runtime configuration.\n\n## Overview\n\nThe dependency injection system allows you to:\n\n1. Pass runtime configuration variables to workflows through a type-safe runtimeContext\n2. Access these variables within step execution contexts\n3. Modify workflow behavior without changing the underlying code\n4. Share configuration across multiple steps within the same workflow\n\n## Basic Usage\n\n```typescript\nconst myWorkflow = mastra.legacy_getWorkflow(\"myWorkflow\");\nconst { runId, start, resume } = myWorkflow.createRun();\n\n// Define your runtimeContext's type structure\ntype WorkflowRuntimeContext = {\n  multiplier: number;\n};\n\nconst runtimeContext = new RuntimeContext<WorkflowRuntimeContext>();\nruntimeContext.set(\"multiplier\", 5);\n\n// Start the workflow execution with runtimeContext\nawait start({\n  triggerData: { inputValue: 45 },\n  runtimeContext,\n});\n```\n\n## Using with REST API\n\nHere's how to dynamically set a multiplier value from an HTTP header:\n\n```typescript filename=\"src/index.ts\"\nimport { Mastra } from \"@mastra/core\";\nimport { RuntimeContext } from \"@mastra/core/di\";\nimport { workflow as myWorkflow } from \"./workflows\";\n\n// Define runtimeContext type with clear, descriptive types\ntype WorkflowRuntimeContext = {\n  multiplier: number;\n};\n\nexport const mastra = new Mastra({\n  legacy_workflows: {\n    myWorkflow,\n  },\n  server: {\n    middleware: [\n      async (c, next) => {\n        const multiplier = c.req.header(\"x-multiplier\");\n        const runtimeContext = c.get<WorkflowRuntimeContext>(\"runtimeContext\");\n\n        // Parse and validate the multiplier value\n        const multiplierValue = parseInt(multiplier || \"1\", 10);\n        if (isNaN(multiplierValue)) {\n          throw new Error(\"Invalid multiplier value\");\n        }\n\n        runtimeContext.set(\"multiplier\", multiplierValue);\n\n        await next(); // Don't forget to call next()\n      },\n    ],\n  },\n});\n```\n\n## Creating Steps with Variables\n\nSteps can access runtimeContext variables and must conform to the workflow's runtimeContext type:\n\n```typescript\nimport { LegacyStep } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Define step input/output types\ninterface StepInput {\n  inputValue: number;\n}\n\ninterface StepOutput {\n  incrementedValue: number;\n}\n\nconst stepOne = new LegacyStep({\n  id: \"stepOne\",\n  description: \"Multiply the input value by the configured multiplier\",\n  execute: async ({ context, runtimeContext }) => {\n    try {\n      // Type-safe access to runtimeContext variables\n      const multiplier = runtimeContext.get(\"multiplier\");\n      if (multiplier === undefined) {\n        throw new Error(\"Multiplier not configured in runtimeContext\");\n      }\n\n      // Get and validate input\n      const inputValue =\n        context.getStepResult<StepInput>(\"trigger\")?.inputValue;\n      if (inputValue === undefined) {\n        throw new Error(\"Input value not provided\");\n      }\n\n      const result: StepOutput = {\n        incrementedValue: inputValue * multiplier,\n      };\n\n      return result;\n    } catch (error) {\n      console.error(`Error in stepOne: ${error.message}`);\n      throw error;\n    }\n  },\n});\n```\n\n## Error Handling\n\nWhen working with runtime variables in workflows, it's important to handle potential errors:\n\n1. **Missing Variables**: Always check if required variables exist in the runtimeContext\n2. **Type Mismatches**: Use TypeScript's type system to catch type errors at compile time\n3. **Invalid Values**: Validate variable values before using them in your steps\n\n```typescript\n// Example of defensive programming with runtimeContext variables\nconst multiplier = runtimeContext.get(\"multiplier\");\nif (multiplier === undefined) {\n  throw new Error(\"Multiplier not configured in runtimeContext\");\n}\n\n// Type and value validation\nif (typeof multiplier !== \"number\" || multiplier <= 0) {\n  throw new Error(`Invalid multiplier value: ${multiplier}`);\n}\n```\n\n## Best Practices\n\n1. **Type Safety**: Always define proper types for your runtimeContext and step inputs/outputs\n2. **Validation**: Validate all inputs and runtimeContext variables before using them\n3. **Error Handling**: Implement proper error handling in your steps\n4. **Documentation**: Document the expected runtimeContext variables for each workflow\n5. **Default Values**: Provide sensible defaults when possible\n\n\n---\ntitle: \"Creating Steps and Adding to Workflows (Legacy) | Mastra Docs\"\ndescription: \"Steps in Mastra workflows provide a structured way to manage operations by defining inputs, outputs, and execution logic.\"\n---\n\n# Defining Steps in a Workflow (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/steps\n\nWhen you build a workflow, you typically break down operations into smaller tasks that can be linked and reused. Steps provide a structured way to manage these tasks by defining inputs, outputs, and execution logic.\n\nThe code below shows how to define these steps inline or separately.\n\n## Inline Step Creation\n\nYou can create steps directly within your workflow using `.step()` and `.then()`. This code shows how to define, link, and execute two steps in sequence.\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/index.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\nexport const myWorkflow = new LegacyWorkflow({\n  name: \"my-workflow\",\n  triggerSchema: z.object({\n    inputValue: z.number(),\n  }),\n});\n\nmyWorkflow\n  .step(\n    new LegacyStep({\n      id: \"stepOne\",\n      outputSchema: z.object({\n        doubledValue: z.number(),\n      }),\n      execute: async ({ context }) => ({\n        doubledValue: context.triggerData.inputValue * 2,\n      }),\n    }),\n  )\n  .then(\n    new LegacyStep({\n      id: \"stepTwo\",\n      outputSchema: z.object({\n        incrementedValue: z.number(),\n      }),\n      execute: async ({ context }) => {\n        if (context.steps.stepOne.status !== \"success\") {\n          return { incrementedValue: 0 };\n        }\n\n        return {\n          incrementedValue: context.steps.stepOne.output.doubledValue + 1,\n        };\n      },\n    }),\n  )\n  .commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { myWorkflow },\n});\n```\n\n## Creating Steps Separately\n\nIf you prefer to manage your step logic in separate entities, you can define steps outside and then add them to your workflow. This code shows how to define steps independently and link them afterward.\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/index.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Define steps separately\nconst stepOne = new LegacyStep({\n  id: \"stepOne\",\n  outputSchema: z.object({\n    doubledValue: z.number(),\n  }),\n  execute: async ({ context }) => ({\n    doubledValue: context.triggerData.inputValue * 2,\n  }),\n});\n\nconst stepTwo = new LegacyStep({\n  id: \"stepTwo\",\n  outputSchema: z.object({\n    incrementedValue: z.number(),\n  }),\n  execute: async ({ context }) => {\n    if (context.steps.stepOne.status !== \"success\") {\n      return { incrementedValue: 0 };\n    }\n    return { incrementedValue: context.steps.stepOne.output.doubledValue + 1 };\n  },\n});\n\n// Build the workflow\nconst myWorkflow = new LegacyWorkflow({\n  name: \"my-workflow\",\n  triggerSchema: z.object({\n    inputValue: z.number(),\n  }),\n});\n\nmyWorkflow.step(stepOne).then(stepTwo);\nmyWorkflow.commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { myWorkflow },\n});\n```\n\n\n---\ntitle: \"Suspend & Resume Workflows (Legacy) | Human-in-the-Loop | Mastra Docs\"\ndescription: \"Suspend and resume in Mastra workflows allows you to pause execution while waiting for external input or resources.\"\n---\n\n# Suspend and Resume in Workflows (Legacy)\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/suspend-and-resume\n\nComplex workflows often need to pause execution while waiting for external input or resources.\n\nMastra's suspend and resume features let you pause workflow execution at any step, persist the workflow snapshot to storage, and resume execution from the saved snapshot when ready.\nThis entire process is automatically managed by Mastra. No config needed, or manual step required from the user.\n\nStoring the workflow snapshot to storage (LibSQL by default) means that the workflow state is permanently preserved across sessions, deployments, and server restarts. This persistence is crucial for workflows that might remain suspended for minutes, hours, or even days while waiting for external input or resources.\n\n## When to Use Suspend/Resume\n\nCommon scenarios for suspending workflows include:\n\n- Waiting for human approval or input\n- Pausing until external API resources become available\n- Collecting additional data needed for later steps\n- Rate limiting or throttling expensive operations\n- Handling event-driven processes with external triggers\n\n## Basic Suspend Example\n\nHere's a simple workflow that suspends when a value is too low and resumes when given a higher value:\n\n```typescript\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\n\nconst stepTwo = new LegacyStep({\n  id: \"stepTwo\",\n  outputSchema: z.object({\n    incrementedValue: z.number(),\n  }),\n  execute: async ({ context, suspend }) => {\n    if (context.steps.stepOne.status !== \"success\") {\n      return { incrementedValue: 0 };\n    }\n\n    const currentValue = context.steps.stepOne.output.doubledValue;\n\n    if (currentValue < 100) {\n      await suspend();\n      return { incrementedValue: 0 };\n    }\n    return { incrementedValue: currentValue + 1 };\n  },\n});\n```\n\n## Async/Await Based Flow\n\nThe suspend and resume mechanism in Mastra uses an async/await pattern that makes it intuitive to implement complex workflows with suspension points. The code structure naturally reflects the execution flow.\n\n### How It Works\n\n1. A step's execution function receives a `suspend` function in its parameters\n2. When called with `await suspend()`, the workflow pauses at that point\n3. The workflow state is persisted\n4. Later, the workflow can be resumed by calling `workflow.resume()` with the appropriate parameters\n5. Execution continues from the point after the `suspend()` call\n\n### Example with Multiple Suspension Points\n\nHere's an example of a workflow with multiple steps that can suspend:\n\n```typescript\n// Define steps with suspend capability\nconst promptAgentStep = new LegacyStep({\n  id: \"promptAgent\",\n  execute: async ({ context, suspend }) => {\n    // Some condition that determines if we need to suspend\n    if (needHumanInput) {\n      // Optionally pass payload data that will be stored with suspended state\n      await suspend({ requestReason: \"Need human input for prompt\" });\n      // Code after suspend() will execute when the step is resumed\n      return { modelOutput: context.userInput };\n    }\n    return { modelOutput: \"AI generated output\" };\n  },\n  outputSchema: z.object({ modelOutput: z.string() }),\n});\n\nconst improveResponseStep = new LegacyStep({\n  id: \"improveResponse\",\n  execute: async ({ context, suspend }) => {\n    // Another condition for suspension\n    if (needFurtherRefinement) {\n      await suspend();\n      return { improvedOutput: context.refinedOutput };\n    }\n    return { improvedOutput: \"Improved output\" };\n  },\n  outputSchema: z.object({ improvedOutput: z.string() }),\n});\n\n// Build the workflow\nconst workflow = new LegacyWorkflow({\n  name: \"multi-suspend-workflow\",\n  triggerSchema: z.object({ input: z.string() }),\n});\n\nworkflow\n  .step(getUserInput)\n  .then(promptAgentStep)\n  .then(evaluateTone)\n  .then(improveResponseStep)\n  .then(evaluateImproved)\n  .commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { workflow },\n});\n```\n\n### Starting and Resuming the Workflow\n\n```typescript\n// Get the workflow and create a run\nconst wf = mastra.legacy_getWorkflow(\"multi-suspend-workflow\");\nconst run = wf.createRun();\n\n// Start the workflow\nconst initialResult = await run.start({\n  triggerData: { input: \"initial input\" },\n});\n\nlet promptAgentStepResult = initialResult.activePaths.get(\"promptAgent\");\nlet promptAgentResumeResult = undefined;\n\n// Check if a step is suspended\nif (promptAgentStepResult?.status === \"suspended\") {\n  console.log(\"Workflow suspended at promptAgent step\");\n\n  // Resume the workflow with new context\n  const resumeResult = await run.resume({\n    stepId: \"promptAgent\",\n    context: { userInput: \"Human provided input\" },\n  });\n\n  promptAgentResumeResult = resumeResult;\n}\n\nconst improveResponseStepResult =\n  promptAgentResumeResult?.activePaths.get(\"improveResponse\");\n\nif (improveResponseStepResult?.status === \"suspended\") {\n  console.log(\"Workflow suspended at improveResponse step\");\n\n  // Resume again with different context\n  const finalResult = await run.resume({\n    stepId: \"improveResponse\",\n    context: { refinedOutput: \"Human refined output\" },\n  });\n\n  console.log(\"Workflow completed:\", finalResult?.results);\n}\n```\n\n## Event-Based Suspension and Resumption\n\nIn addition to manually suspending steps, Mastra provides event-based suspension through the `afterEvent` method. This allows workflows to automatically suspend and wait for a specific event to occur before continuing.\n\n### Using afterEvent and resumeWithEvent\n\nThe `afterEvent` method automatically creates a suspension point in your workflow that waits for a specific event to occur. When the event happens, you can use `resumeWithEvent` to continue the workflow with the event data.\n\nHere's how it works:\n\n1. Define events in your workflow configuration\n2. Use `afterEvent` to create a suspension point waiting for that event\n3. When the event occurs, call `resumeWithEvent` with the event name and data\n\n### Example: Event-Based Workflow\n\n```typescript\n// Define steps\nconst getUserInput = new LegacyStep({\n  id: \"getUserInput\",\n  execute: async () => ({ userInput: \"initial input\" }),\n  outputSchema: z.object({ userInput: z.string() }),\n});\n\nconst processApproval = new LegacyStep({\n  id: \"processApproval\",\n  execute: async ({ context }) => {\n    // Access the event data from the context\n    const approvalData = context.inputData?.resumedEvent;\n    return {\n      approved: approvalData?.approved,\n      approvedBy: approvalData?.approverName,\n    };\n  },\n  outputSchema: z.object({\n    approved: z.boolean(),\n    approvedBy: z.string(),\n  }),\n});\n\n// Create workflow with event definition\nconst approvalWorkflow = new LegacyWorkflow({\n  name: \"approval-workflow\",\n  triggerSchema: z.object({ requestId: z.string() }),\n  events: {\n    approvalReceived: {\n      schema: z.object({\n        approved: z.boolean(),\n        approverName: z.string(),\n      }),\n    },\n  },\n});\n\n// Build workflow with event-based suspension\napprovalWorkflow\n  .step(getUserInput)\n  .afterEvent(\"approvalReceived\") // Workflow will automatically suspend here\n  .step(processApproval) // This step runs after the event is received\n  .commit();\n```\n\n### Running an Event-Based Workflow\n\n```typescript\n// Get the workflow\nconst workflow = mastra.legacy_getWorkflow(\"approval-workflow\");\nconst run = workflow.createRun();\n\n// Start the workflow\nconst initialResult = await run.start({\n  triggerData: { requestId: \"request-123\" },\n});\n\nconsole.log(\"Workflow started, waiting for approval event\");\nconsole.log(initialResult.results);\n// Output will show the workflow is suspended at the event step:\n// {\n//   getUserInput: { status: 'success', output: { userInput: 'initial input' } },\n//   __approvalReceived_event: { status: 'suspended' }\n// }\n\n// Later, when the approval event occurs:\nconst resumeResult = await run.resumeWithEvent(\"approvalReceived\", {\n  approved: true,\n  approverName: \"Jane Doe\",\n});\n\nconsole.log(\"Workflow resumed with event data:\", resumeResult.results);\n// Output will show the completed workflow:\n// {\n//   getUserInput: { status: 'success', output: { userInput: 'initial input' } },\n//   __approvalReceived_event: { status: 'success', output: { executed: true, resumedEvent: { approved: true, approverName: 'Jane Doe' } } },\n//   processApproval: { status: 'success', output: { approved: true, approvedBy: 'Jane Doe' } }\n// }\n```\n\n### Key Points About Event-Based Workflows\n\n- The `suspend()` function can optionally take a payload object that will be stored with the suspended state\n- Code after the `await suspend()` call will not execute until the step is resumed\n- When a step is suspended, its status becomes `'suspended'` in the workflow results\n- When resumed, the step's status changes from `'suspended'` to `'success'` once completed\n- The `resume()` method requires the `stepId` to identify which suspended step to resume\n- You can provide new context data when resuming that will be merged with existing step results\n\n- Events must be defined in the workflow configuration with a schema\n- The `afterEvent` method creates a special suspended step that waits for the event\n- The event step is automatically named `__eventName_event` (e.g., `__approvalReceived_event`)\n- Use `resumeWithEvent` to provide event data and continue the workflow\n- Event data is validated against the schema defined for that event\n- The event data is available in the context as `inputData.resumedEvent`\n\n## Storage for Suspend and Resume\n\nWhen a workflow is suspended using `await suspend()`, Mastra automatically persists the entire workflow state to storage. This is essential for workflows that might remain suspended for extended periods, as it ensures the state is preserved across application restarts or server instances.\n\n### Default Storage: LibSQL\n\nBy default, Mastra uses LibSQL as its storage engine:\n\n```typescript\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { LibSQLStore } from \"@mastra/libsql\";\n\nconst mastra = new Mastra({\n  storage: new LibSQLStore({\n    url: \"file:./storage.db\", // Local file-based database for development\n    // For production, use a persistent URL:\n    // url: process.env.DATABASE_URL,\n    // authToken: process.env.DATABASE_AUTH_TOKEN, // Optional for authenticated connections\n  }),\n});\n```\n\nThe LibSQL storage can be configured in different modes:\n\n- In-memory database (testing): `:memory:`\n- File-based database (development): `file:storage.db`\n- Remote database (production): URLs like `libsql://your-database.turso.io`\n\n### Alternative Storage Options\n\n#### Upstash (Redis-Compatible)\n\nFor serverless applications or environments where Redis is preferred:\n\n```bash copy\nnpm install @mastra/upstash@latest\n```\n\n```typescript\nimport { Mastra } from \"@mastra/core/mastra\";\nimport { UpstashStore } from \"@mastra/upstash\";\n\nconst mastra = new Mastra({\n  storage: new UpstashStore({\n    url: process.env.UPSTASH_URL,\n    token: process.env.UPSTASH_TOKEN,\n  }),\n});\n```\n\n### Storage Considerations\n\n- All storage options support suspend and resume functionality identically\n- The workflow state is automatically serialized and saved when suspended\n- No additional configuration is needed for suspend/resume to work with storage\n- Choose your storage option based on your infrastructure, scaling needs, and existing technology stack\n\n## Watching and Resuming\n\nTo handle suspended workflows, use the `watch` method to monitor workflow status per run and `resume` to continue execution:\n\n```typescript\nimport { mastra } from \"./index\";\n\n// Get the workflow\nconst myWorkflow = mastra.legacy_getWorkflow(\"myWorkflow\");\nconst { start, watch, resume } = myWorkflow.createRun();\n\n// Start watching the workflow before executing it\nwatch(async ({ activePaths }) => {\n  const isStepTwoSuspended = activePaths.get(\"stepTwo\")?.status === \"suspended\";\n  if (isStepTwoSuspended) {\n    console.log(\"Workflow suspended, resuming with new value\");\n\n    // Resume the workflow with new context\n    await resume({\n      stepId: \"stepTwo\",\n      context: { secondValue: 100 },\n    });\n  }\n});\n\n// Start the workflow execution\nawait start({ triggerData: { inputValue: 45 } });\n```\n\n### Watching and Resuming Event-Based Workflows\n\nYou can use the same watching pattern with event-based workflows:\n\n```typescript\nconst { start, watch, resumeWithEvent } = workflow.createRun();\n\n// Watch for suspended event steps\nwatch(async ({ activePaths }) => {\n  const isApprovalReceivedSuspended =\n    activePaths.get(\"__approvalReceived_event\")?.status === \"suspended\";\n  if (isApprovalReceivedSuspended) {\n    console.log(\"Workflow waiting for approval event\");\n\n    // In a real scenario, you would wait for the actual event to occur\n    // For example, this could be triggered by a webhook or user interaction\n    setTimeout(async () => {\n      await resumeWithEvent(\"approvalReceived\", {\n        approved: true,\n        approverName: \"Auto Approver\",\n      });\n    }, 5000); // Simulate event after 5 seconds\n  }\n});\n\n// Start the workflow\nawait start({ triggerData: { requestId: \"auto-123\" } });\n```\n\n## Further Reading\n\nFor a deeper understanding of how suspend and resume works under the hood:\n\n- [Understanding Snapshots in Mastra Workflows](../../reference/legacyWorkflows/snapshots.mdx) - Learn about the snapshot mechanism that powers suspend and resume functionality\n- [Step Configuration Guide](./steps.mdx) - Learn more about configuring steps in your workflows\n- [Control Flow Guide](./control-flow.mdx) - Advanced workflow control patterns\n- [Event-Driven Workflows](../../reference/legacyWorkflows/events.mdx) - Detailed reference for event-based workflows\n\n## Related Resources\n\n- See the [Suspend and Resume Example](../../examples/workflows_legacy/suspend-and-resume.mdx) for a complete working example\n- Check the [Step Class Reference](../../reference/legacyWorkflows/step-class.mdx) for suspend/resume API details\n- Review [Workflow Observability](../../reference/observability/otel-config.mdx) for monitoring suspended workflows\n\n\n---\ntitle: \"Data Mapping with Workflow (Legacy) Variables | Mastra Docs\"\ndescription: \"Learn how to use workflow variables to map data between steps and create dynamic data flows in your Mastra workflows.\"\n---\n\n# Data Mapping with Workflow Variables\n[EN] Source: https://mastra.ai/en/docs/workflows-legacy/variables\n\nWorkflow variables in Mastra provide a powerful mechanism for mapping data between steps, allowing you to create dynamic data flows and pass information from one step to another.\n\n## Understanding Workflow Variables\n\nIn Mastra workflows, variables serve as a way to:\n\n- Map data from trigger inputs to step inputs\n- Pass outputs from one step to inputs of another step\n- Access nested properties within step outputs\n- Create more flexible and reusable workflow steps\n\n## Using Variables for Data Mapping\n\n### Basic Variable Mapping\n\nYou can map data between steps using the `variables` property when adding a step to your workflow:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/index.ts\" copy\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\n\nconst workflow = new LegacyWorkflow({\n  name: \"data-mapping-workflow\",\n  triggerSchema: z.object({\n    inputData: z.string(),\n  }),\n});\n\nworkflow\n  .step(step1, {\n    variables: {\n      // Map trigger data to step input\n      inputData: { step: \"trigger\", path: \"inputData\" },\n    },\n  })\n  .then(step2, {\n    variables: {\n      // Map output from step1 to input for step2\n      previousValue: { step: step1, path: \"outputField\" },\n    },\n  })\n  .commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { workflow },\n});\n```\n\n### Accessing Nested Properties\n\nYou can access nested properties using dot notation in the `path` field:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/index.ts\" copy\nworkflow\n  .step(step1)\n  .then(step2, {\n    variables: {\n      // Access a nested property from step1's output\n      nestedValue: { step: step1, path: \"nested.deeply.value\" },\n    },\n  })\n  .commit();\n```\n\n### Mapping Entire Objects\n\nYou can map an entire object by using `.` as the path:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/index.ts\" copy\nworkflow\n  .step(step1, {\n    variables: {\n      // Map the entire trigger data object\n      triggerData: { step: \"trigger\", path: \".\" },\n    },\n  })\n  .commit();\n```\n\n### Variables in Loops\n\nVariables can also be passed to `while` and `until` loops. This is useful for passing data between iterations or from outside steps:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/loop-variables.ts\" copy\n// Step that increments a counter\nconst incrementStep = new LegacyStep({\n  id: \"increment\",\n  inputSchema: z.object({\n    // Previous value from last iteration\n    prevValue: z.number().optional(),\n  }),\n  outputSchema: z.object({\n    // Updated counter value\n    updatedCounter: z.number(),\n  }),\n  execute: async ({ context }) => {\n    const { prevValue = 0 } = context.inputData;\n    return { updatedCounter: prevValue + 1 };\n  },\n});\n\nconst workflow = new LegacyWorkflow({\n  name: \"counter\",\n});\n\nworkflow.step(incrementStep).while(\n  async ({ context }) => {\n    // Continue while counter is less than 10\n    const result = context.getStepResult(incrementStep);\n    return (result?.updatedCounter ?? 0) < 10;\n  },\n  incrementStep,\n  {\n    // Pass previous value to next iteration\n    prevValue: {\n      step: incrementStep,\n      path: \"updatedCounter\",\n    },\n  },\n);\n```\n\n## Variable Resolution\n\nWhen a workflow executes, Mastra resolves variables at runtime by:\n\n1. Identifying the source step specified in the `step` property\n2. Retrieving the output from that step\n3. Navigating to the specified property using the `path`\n4. Injecting the resolved value into the target step's context as the `inputData` property\n\n## Examples\n\n### Mapping from Trigger Data\n\nThis example shows how to map data from the workflow trigger to a step:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/trigger-mapping.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Define a step that needs user input\nconst processUserInput = new LegacyStep({\n  id: \"processUserInput\",\n  execute: async ({ context }) => {\n    // The inputData will be available in context because of the variable mapping\n    const { inputData } = context.inputData;\n\n    return {\n      processedData: `Processed: ${inputData}`,\n    };\n  },\n});\n\n// Create the workflow\nconst workflow = new LegacyWorkflow({\n  name: \"trigger-mapping\",\n  triggerSchema: z.object({\n    inputData: z.string(),\n  }),\n});\n\n// Map the trigger data to the step\nworkflow\n  .step(processUserInput, {\n    variables: {\n      inputData: { step: \"trigger\", path: \"inputData\" },\n    },\n  })\n  .commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { workflow },\n});\n```\n\n### Mapping Between Steps\n\nThis example demonstrates mapping data from one step to another:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/step-mapping.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Step 1: Generate data\nconst generateData = new LegacyStep({\n  id: \"generateData\",\n  outputSchema: z.object({\n    nested: z.object({\n      value: z.string(),\n    }),\n  }),\n  execute: async () => {\n    return {\n      nested: {\n        value: \"step1-data\",\n      },\n    };\n  },\n});\n\n// Step 2: Process the data from step 1\nconst processData = new LegacyStep({\n  id: \"processData\",\n  inputSchema: z.object({\n    previousValue: z.string(),\n  }),\n  execute: async ({ context }) => {\n    // previousValue will be available because of the variable mapping\n    const { previousValue } = context.inputData;\n\n    return {\n      result: `Processed: ${previousValue}`,\n    };\n  },\n});\n\n// Create the workflow\nconst workflow = new LegacyWorkflow({\n  name: \"step-mapping\",\n});\n\n// Map data from step1 to step2\nworkflow\n  .step(generateData)\n  .then(processData, {\n    variables: {\n      // Map the nested.value property from generateData's output\n      previousValue: { step: generateData, path: \"nested.value\" },\n    },\n  })\n  .commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { workflow },\n});\n```\n\n## Type Safety\n\nMastra provides type safety for variable mappings when using TypeScript:\n\n```typescript showLineNumbers filename=\"src/mastra/workflows/type-safe.ts\" copy\nimport { Mastra } from \"@mastra/core\";\nimport { LegacyStep, LegacyWorkflow } from \"@mastra/core/workflows/legacy\";\nimport { z } from \"zod\";\n\n// Define schemas for better type safety\nconst triggerSchema = z.object({\n  inputValue: z.string(),\n});\n\ntype TriggerType = z.infer<typeof triggerSchema>;\n\n// Step with typed context\nconst step1 = new LegacyStep({\n  id: \"step1\",\n  outputSchema: z.object({\n    nested: z.object({\n      value: z.string(),\n    }),\n  }),\n  execute: async ({ context }) => {\n    // TypeScript knows the shape of triggerData\n    const triggerData = context.getStepResult<TriggerType>(\"trigger\");\n\n    return {\n      nested: {\n        value: `processed-${triggerData?.inputValue}`,\n      },\n    };\n  },\n});\n\n// Create the workflow with the schema\nconst workflow = new LegacyWorkflow({\n  name: \"type-safe-workflow\",\n  triggerSchema,\n});\n\nworkflow.step(step1).commit();\n\n// Register the workflow with Mastra\nexport const mastra = new Mastra({\n  legacy_workflows: { workflow },\n});\n```\n\n## Best Practices\n\n1. **Validate Inputs and Outputs**: Use `inputSchema` and `outputSchema` to ensure data consistency.\n\n2. **Keep Mappings Simple**: Avoid overly complex nested paths when possible.\n\n3. **Consider Default Values**: Handle cases where mapped data might be undefined.\n\n## Comparison with Direct Context Access\n\nWhile you can access previous step results directly via `context.steps`, using variable mappings offers several advantages:\n\n| Feature     | Variable Mapping                            | Direct Context Access           |\n| ----------- | ------------------------------------------- | ------------------------------- |\n| Clarity     | Explicit data dependencies                  | Implicit dependencies           |\n| Reusability | Steps can be reused with different mappings | Steps are tightly coupled       |\n| Type Safety | Better TypeScript integration               | Requires manual type assertions |\n\n\n---\ntitle: \"Example: Categorizing Birds | Agents | Mastra Docs\"\ndescription: Example of using a Mastra AI Agent to determine if an image from Unsplash depicts a bird.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Example: Categorizing Birds with an AI Agent\n[EN] Source: https://mastra.ai/en/examples/agents/bird-checker\n\nWe will get a random image from [Unsplash](https://unsplash.com/) that matches a selected query and uses a [Mastra AI Agent](/docs/agents/overview.md) to determine if it is a bird or not.\n\n```ts showLineNumbers copy\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { z } from \"zod\";\n\nexport type Image = {\n  alt_description: string;\n  urls: {\n    regular: string;\n    raw: string;\n  };\n  user: {\n    first_name: string;\n    links: {\n      html: string;\n    };\n  };\n};\n\nexport type ImageResponse<T, K> =\n  | {\n      ok: true;\n      data: T;\n    }\n  | {\n      ok: false;\n      error: K;\n    };\n\nconst getRandomImage = async ({\n  query,\n}: {\n  query: string;\n}): Promise<ImageResponse<Image, string>> => {\n  const page = Math.floor(Math.random() * 20);\n  const order_by = Math.random() < 0.5 ? \"relevant\" : \"latest\";\n  try {\n    const res = await fetch(\n      `https://api.unsplash.com/search/photos?query=${query}&page=${page}&order_by=${order_by}`,\n      {\n        method: \"GET\",\n        headers: {\n          Authorization: `Client-ID ${process.env.UNSPLASH_ACCESS_KEY}`,\n          \"Accept-Version\": \"v1\",\n        },\n        cache: \"no-store\",\n      },\n    );\n\n    if (!res.ok) {\n      return {\n        ok: false,\n        error: \"Failed to fetch image\",\n      };\n    }\n\n    const data = (await res.json()) as {\n      results: Array<Image>;\n    };\n    const randomNo = Math.floor(Math.random() * data.results.length);\n\n    return {\n      ok: true,\n      data: data.results[randomNo] as Image,\n    };\n  } catch (err) {\n    return {\n      ok: false,\n      error: \"Error fetching image\",\n    };\n  }\n};\n\nconst instructions = `\n  You can view an image and figure out if it is a bird or not. \n  You can also figure out the species of the bird and where the picture was taken.\n`;\n\nexport const birdCheckerAgent = new Agent({\n  name: \"Bird checker\",\n  instructions,\n  model: anthropic(\"claude-3-haiku-20240307\"),\n});\n\nconst queries: string[] = [\"wildlife\", \"feathers\", \"flying\", \"birds\"];\nconst randomQuery = queries[Math.floor(Math.random() * queries.length)];\n\n// Get the image url from Unsplash with random type\nconst imageResponse = await getRandomImage({ query: randomQuery });\n\nif (!imageResponse.ok) {\n  console.log(\"Error fetching image\", imageResponse.error);\n  process.exit(1);\n}\n\nconsole.log(\"Image URL: \", imageResponse.data.urls.regular);\nconst response = await birdCheckerAgent.generate(\n  [\n    {\n      role: \"user\",\n      content: [\n        {\n          type: \"image\",\n          image: new URL(imageResponse.data.urls.regular),\n        },\n        {\n          type: \"text\",\n          text: \"view this image and let me know if it's a bird or not, and the scientific name of the bird without any explanation. Also summarize the location for this picture in one or two short sentences understandable by a high school student\",\n        },\n      ],\n    },\n  ],\n  {\n    output: z.object({\n      bird: z.boolean(),\n      species: z.string(),\n      location: z.string(),\n    }),\n  },\n);\n\nconsole.log(response.object);\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/bird-checker\"\n  }\n/>\n\n\n---\ntitle: \"Example: Deploying an MCPServer | Agents | Mastra Docs\"\ndescription: Example of setting up, building, and deploying a Mastra MCPServer using the stdio transport and publishing it to NPM.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Example: Deploying an MCPServer\n[EN] Source: https://mastra.ai/en/examples/agents/deploying-mcp-server\n\nThis example guides you through setting up a basic Mastra MCPServer using the stdio transport, building it, and preparing it for deployment, such as publishing to NPM.\n\n## Install Dependencies\n\nInstall the necessary packages:\n\n```bash\npnpm add @mastra/mcp @mastra/core tsup\n```\n\n## Set up MCP Server\n\n1.  Create a file for your stdio server, for example, `/src/mastra/stdio.ts`.\n\n2.  Add the following code to the file. Remember to import your actual Mastra tools and name the server appropriately.\n\n    ```typescript filename=\"src/mastra/stdio.ts\" copy\n    #!/usr/bin/env node\n    import { MCPServer } from \"@mastra/mcp\";\n    import { weatherTool } from \"./tools\";\n\n    const server = new MCPServer({\n      name: \"my-mcp-server\",\n      version: \"1.0.0\",\n      tools: { weatherTool },\n    });\n\n    server.startStdio().catch((error) => {\n      console.error(\"Error running MCP server:\", error);\n      process.exit(1);\n    });\n    ```\n\n3.  Update your `package.json` to include the `bin` entry pointing to your built server file and a script to build the server.\n\n```json filename=\"package.json\" copy\n{\n  \"bin\": \"dist/stdio.js\",\n  \"scripts\": {\n    \"build:mcp\": \"tsup src/mastra/stdio.ts --format esm --no-splitting --dts && chmod +x dist/stdio.js\"\n  }\n}\n```\n\n4.  Run the build command:\n\n    ```bash\n    pnpm run build:mcp\n    ```\n\n    This will compile your server code and make the output file executable.\n\n## Deploying to NPM\n\nTo make your MCP server available for others (or yourself) to use via `npx` or as a dependency, you can publish it to NPM.\n\n1.  Ensure you have an NPM account and are logged in (`npm login`).\n2.  Make sure your package name in `package.json` is unique and available.\n3.  Run the publish command from your project root after building:\n\n    ```bash\n    npm publish --access public\n    ```\n\n    For more details on publishing packages, refer to the [NPM documentation](https://docs.npmjs.com/creating-and-publishing-scoped-public-packages).\n\n## Use the Deployed MCP Server\n\nOnce published, your MCP server can be used by an `MCPClient` by specifying the command to run your package. You can also use any other MCP client like Claude desktop, Cursor, or Windsurf.\n\n```typescript\nimport { MCPClient } from \"@mastra/mcp\";\n\nconst mcp = new MCPClient({\n  servers: {\n    // Give this MCP server instance a name\n    yourServerName: {\n      command: \"npx\",\n      args: [\"-y\", \"@your-org-name/your-package-name@latest\"], // Replace with your package name\n    },\n  },\n});\n\n// You can then get tools or toolsets from this configuration to use in your agent\nconst tools = await mcp.getTools();\nconst toolsets = await mcp.getToolsets();\n```\n\nNote: If you published without an organization scope, the `args` might just be `[\"-y\", \"your-package-name@latest\"]`.\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n\n---\ntitle: Dynamic Agents Example | Agents | Mastra Docs\ndescription: Learn how to create and configure dynamic agents using runtime context in Mastra.\n---\n\n# Dynamic Agents Example\n[EN] Source: https://mastra.ai/en/examples/agents/dynamic-agents\n\nFirst, let's define our runtime context type:\n\n```typescript\nimport { Agent, RuntimeContext } from \"@mastra/core\";\nimport { z } from \"zod\";\n\ntype SupportRuntimeContext = {\n  \"user-tier\": \"free\" | \"pro\" | \"enterprise\";\n  language: \"en\" | \"es\" | \"fr\";\n  \"user-id\": string;\n};\n```\n\nNext, let's create our dynamic support agent with its configuration:\n\n```typescript\nconst supportAgent = new Agent({\n  name: \"Dynamic Support Agent\",\n\n  instructions: async ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\");\n    const language = runtimeContext.get(\"language\");\n\n    return `You are a customer support agent for our SaaS platform.\n    The current user is on the ${userTier} tier and prefers ${language} language.\n    \n    For ${userTier} tier users:\n    ${userTier === \"free\" ? \"- Provide basic support and documentation links\" : \"\"}\n    ${userTier === \"pro\" ? \"- Offer detailed technical support and best practices\" : \"\"}\n    ${userTier === \"enterprise\" ? \"- Provide priority support with custom solutions\" : \"\"}\n    \n    Always respond in ${language} language.`;\n  },\n\n  model: ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\");\n    return userTier === \"enterprise\"\n      ? openai(\"gpt-4\")\n      : openai(\"gpt-3.5-turbo\");\n  },\n\n  tools: ({ runtimeContext }) => {\n    const userTier = runtimeContext.get(\"user-tier\");\n    const baseTools = [knowledgeBase, ticketSystem];\n\n    if (userTier === \"pro\" || userTier === \"enterprise\") {\n      baseTools.push(advancedAnalytics);\n    }\n\n    if (userTier === \"enterprise\") {\n      baseTools.push(customIntegration);\n    }\n\n    return baseTools;\n  },\n});\n```\n\nRuntimeContext can be passed from the client/server directly to your agent generate and stream calls\n\n```typescript\nasync function handleSupportRequest(userId: string, message: string) {\n  const runtimeContext = new RuntimeContext<SupportRuntimeContext>();\n\n  runtimeContext.set(\"user-id\", userId);\n  runtimeContext.set(\"user-tier\", await getUserTier(userId));\n  runtimeContext.set(\"language\", await getUserLanguage(userId));\n\n  const response = await supportAgent.generate(message, {\n    runtimeContext,\n  });\n\n  return response.text;\n}\n```\n\nRuntimeContext can also be set from the server middleware layer\n\n```typescript\nimport { Mastra } from \"@mastra/core\";\nimport { registerApiRoute } from \"@mastra/core/server\";\n\nexport const mastra = new Mastra({\n  agents: {\n    support: supportAgent,\n  },\n  server: {\n    middleware: [\n      async (c, next) => {\n        const userId = c.req.header(\"X-User-ID\");\n        const runtimeContext = c.get<SupportRuntimeContext>(\"runtimeContext\");\n\n        // Set user tier based on subscription\n        const userTier = await getUserTier(userId);\n        runtimeContext.set(\"user-tier\", userTier);\n\n        // Set language based on user preferences\n        const language = await getUserLanguage(userId);\n        runtimeContext.set(\"language\", language);\n\n        // Set user ID\n        runtimeContext.set(\"user-id\", userId);\n\n        await next();\n      },\n    ],\n    apiRoutes: [\n      registerApiRoute(\"/support\", {\n        method: \"POST\",\n        handler: async (c) => {\n          const { userId, message } = await c.req.json();\n\n          try {\n            const response = await handleSupportRequest(userId, message);\n            return c.json({ response });\n          } catch (error) {\n            return c.json({ error: \"Failed to process support request\" }, 500);\n          }\n        },\n      }),\n    ],\n  },\n});\n```\n\n## Usage Example\n\nThis example shows how a single agent can handle different types of users and scenarios by leveraging runtime context, making it more flexible and maintainable than creating separate agents for each use case.\n\n\n---\ntitle: \"Example: Hierarchical Multi-Agent System | Agents | Mastra\"\ndescription: Example of creating a hierarchical multi-agent system using Mastra, where agents interact through tool functions.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Hierarchical Multi-Agent System\n[EN] Source: https://mastra.ai/en/examples/agents/hierarchical-multi-agent\n\nThis example demonstrates how to create a hierarchical multi-agent system where agents interact through tool functions, with one agent coordinating the work of others.\n\nThe system consists of three agents:\n\n1. A Publisher agent (supervisor) that orchestrates the process\n2. A Copywriter agent that writes the initial content\n3. An Editor agent that refines the content\n\nFirst, define the Copywriter agent and its tool:\n\n```ts showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { anthropic } from \"@ai-sdk/anthropic\";\n\nconst copywriterAgent = new Agent({\n  name: \"Copywriter\",\n  instructions: \"You are a copywriter agent that writes blog post copy.\",\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n});\n\nconst copywriterTool = createTool({\n  id: \"copywriter-agent\",\n  description: \"Calls the copywriter agent to write blog post copy.\",\n  inputSchema: z.object({\n    topic: z.string().describe(\"Blog post topic\"),\n  }),\n  outputSchema: z.object({\n    copy: z.string().describe(\"Blog post copy\"),\n  }),\n  execute: async ({ context }) => {\n    const result = await copywriterAgent.generate(\n      `Create a blog post about ${context.topic}`,\n    );\n    return { copy: result.text };\n  },\n});\n```\n\nNext, define the Editor agent and its tool:\n\n```ts showLineNumbers copy\nconst editorAgent = new Agent({\n  name: \"Editor\",\n  instructions: \"You are an editor agent that edits blog post copy.\",\n  model: openai(\"gpt-4o-mini\"),\n});\n\nconst editorTool = createTool({\n  id: \"editor-agent\",\n  description: \"Calls the editor agent to edit blog post copy.\",\n  inputSchema: z.object({\n    copy: z.string().describe(\"Blog post copy\"),\n  }),\n  outputSchema: z.object({\n    copy: z.string().describe(\"Edited blog post copy\"),\n  }),\n  execute: async ({ context }) => {\n    const result = await editorAgent.generate(\n      `Edit the following blog post only returning the edited copy: ${context.copy}`,\n    );\n    return { copy: result.text };\n  },\n});\n```\n\nFinally, create the Publisher agent that coordinates the others:\n\n```ts showLineNumbers copy\nconst publisherAgent = new Agent({\n  name: \"publisherAgent\",\n  instructions:\n    \"You are a publisher agent that first calls the copywriter agent to write blog post copy about a specific topic and then calls the editor agent to edit the copy. Just return the final edited copy.\",\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n  tools: { copywriterTool, editorTool },\n});\n\nconst mastra = new Mastra({\n  agents: { publisherAgent },\n});\n```\n\nTo use the entire system:\n\n```ts showLineNumbers copy\nasync function main() {\n  const agent = mastra.getAgent(\"publisherAgent\");\n  const result = await agent.generate(\n    \"Write a blog post about React JavaScript frameworks. Only return the final edited copy.\",\n  );\n  console.log(result.text);\n}\n\nmain();\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/hierarchical-multi-agent\"\n  }\n/>\n\n\n---\ntitle: \"Example: Multi-Agent Workflow | Agents | Mastra Docs\"\ndescription: Example of creating an agentic workflow in Mastra, where work product is passed between multiple agents.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Multi-Agent Workflow\n[EN] Source: https://mastra.ai/en/examples/agents/multi-agent-workflow\n\nThis example demonstrates how to create an agentic workflow with work product being passed between multiple agents with a worker agent and a supervisor agent.\n\nIn this example, we create a sequential workflow that calls two agents in order:\n\n1. A Copywriter agent that writes the initial blog post\n2. An Editor agent that refines the content\n\nFirst, import the required dependencies:\n\n```typescript\nimport { openai } from \"@ai-sdk/openai\";\nimport { anthropic } from \"@ai-sdk/anthropic\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { createStep, createWorkflow } from \"@mastra/core/workflows\";\nimport { z } from \"zod\";\n```\n\nCreate the copywriter agent that will generate the initial blog post:\n\n```typescript\nconst copywriterAgent = new Agent({\n  name: \"Copywriter\",\n  instructions: \"You are a copywriter agent that writes blog post copy.\",\n  model: anthropic(\"claude-3-5-sonnet-20241022\"),\n});\n```\n\nDefine the copywriter step that executes the agent and handles the response:\n\n```typescript\nconst copywriterStep = createStep({\n  id: \"copywriterStep\",\n  inputSchema: z.object({\n    topic: z.string(),\n  }),\n  outputSchema: z.object({\n    copy: z.string(),\n  }),\n  execute: async ({ inputData }) => {\n    if (!inputData?.topic) {\n      throw new Error(\"Topic not found in trigger data\");\n    }\n    const result = await copywriterAgent.generate(\n      `Create a blog post about ${inputData.topic}`,\n    );\n    console.log(\"copywriter result\", result.text);\n    return {\n      copy: result.text,\n    };\n  },\n});\n```\n\nSet up the editor agent to refine the copywriter's content:\n\n```typescript\nconst editorAgent = new Agent({\n  name: \"Editor\",\n  instructions: \"You are an editor agent that edits blog post copy.\",\n  model: openai(\"gpt-4o-mini\"),\n});\n```\n\nCreate the editor step that processes the copywriter's output:\n\n```typescript\nconst editorStep = createStep({\n  id: \"editorStep\",\n  inputSchema: z.object({\n    copy: z.string(),\n  }),\n  outputSchema: z.object({\n    finalCopy: z.string(),\n  }),\n  execute: async ({ inputData }) => {\n    const copy = inputData?.copy;\n\n    const result = await editorAgent.generate(\n      `Edit the following blog post only returning the edited copy: ${copy}`,\n    );\n    console.log(\"editor result\", result.text);\n    return {\n      finalCopy: result.text,\n    };\n  },\n});\n```\n\nConfigure the workflow and execute the steps:\n\n```typescript\nconst myWorkflow = createWorkflow({\n  id: \"my-workflow\",\n  inputSchema: z.object({\n    topic: z.string(),\n  }),\n  outputSchema: z.object({\n    finalCopy: z.string(),\n  }),\n});\n\n// Run steps sequentially.\nmyWorkflow.then(copywriterStep).then(editorStep).commit();\n\nconst run = await myWorkflow.createRunAsync();\n\nconst res = await run.start({\n  inputData: { topic: \"React JavaScript frameworks\" },\n});\nconsole.log(\"Response: \", res);\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/multi-agent-workflow\"\n  }\n/>\n\n\n---\ntitle: \"Example: Agents with a System Prompt | Agents | Mastra Docs\"\ndescription: Example of creating an AI agent in Mastra with a system prompt to define its personality and capabilities.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Giving an Agent a System Prompt\n[EN] Source: https://mastra.ai/en/examples/agents/system-prompt\n\nWhen building AI agents, you often need to give them specific instructions and capabilities to handle specialized tasks effectively. System prompts allow you to define an agent's personality, knowledge domain, and behavioral guidelines. This example shows how to create an AI agent with custom instructions and integrate it with a dedicated tool for retrieving verified information.\n\n```ts showLineNumbers copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { createTool } from \"@mastra/core/tools\";\n\nimport { z } from \"zod\";\n\nconst instructions = `You are a helpful cat expert assistant. When discussing cats, you should always include an interesting cat fact.\n\n  Your main responsibilities:\n  1. Answer questions about cats\n  2. Use the catFact tool to provide verified cat facts\n  3. Incorporate the cat facts naturally into your responses\n\n  Always use the catFact tool at least once in your responses to ensure accuracy.`;\n\nconst getCatFact = async () => {\n  const { fact } = (await fetch(\"https://catfact.ninja/fact\").then((res) =>\n    res.json(),\n  )) as {\n    fact: string;\n  };\n\n  return fact;\n};\n\nconst catFact = createTool({\n  id: \"Get cat facts\",\n  inputSchema: z.object({}),\n  description: \"Fetches cat facts\",\n  execute: async () => {\n    console.log(\"using tool to fetch cat fact\");\n    return {\n      catFact: await getCatFact(),\n    };\n  },\n});\n\nconst catOne = new Agent({\n  name: \"cat-one\",\n  instructions: instructions,\n  model: openai(\"gpt-4o-mini\"),\n  tools: {\n    catFact,\n  },\n});\n\nconst result = await catOne.generate(\"Tell me a cat fact\");\n\nconsole.log(result.text);\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/system-prompt\"\n  }\n/>\n\n\n---\ntitle: \"Example: Giving an Agent a Tool | Agents | Mastra Docs\"\ndescription: Example of creating an AI agent in Mastra that uses a dedicated tool to provide weather information.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Example: Giving an Agent a Tool\n[EN] Source: https://mastra.ai/en/examples/agents/using-a-tool\n\nWhen building AI agents, you often need to integrate external data sources or functionality to enhance their capabilities. This example shows how to create an AI agent that uses a dedicated weather tool to provide accurate weather information for specific locations.\n\n```ts showLineNumbers copy\nimport { Mastra } from \"@mastra/core\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { createTool } from \"@mastra/core/tools\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { z } from \"zod\";\n\ninterface WeatherResponse {\n  current: {\n    time: string;\n    temperature_2m: number;\n    apparent_temperature: number;\n    relative_humidity_2m: number;\n    wind_speed_10m: number;\n    wind_gusts_10m: number;\n    weather_code: number;\n  };\n}\n\nconst weatherTool = createTool({\n  id: \"get-weather\",\n  description: \"Get current weather for a location\",\n  inputSchema: z.object({\n    location: z.string().describe(\"City name\"),\n  }),\n  outputSchema: z.object({\n    temperature: z.number(),\n    feelsLike: z.number(),\n    humidity: z.number(),\n    windSpeed: z.number(),\n    windGust: z.number(),\n    conditions: z.string(),\n    location: z.string(),\n  }),\n  execute: async ({ context }) => {\n    return await getWeather(context.location);\n  },\n});\n\nconst getWeather = async (location: string) => {\n  const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(location)}&count=1`;\n  const geocodingResponse = await fetch(geocodingUrl);\n  const geocodingData = await geocodingResponse.json();\n\n  if (!geocodingData.results?.[0]) {\n    throw new Error(`Location '${location}' not found`);\n  }\n\n  const { latitude, longitude, name } = geocodingData.results[0];\n\n  const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,apparent_temperature,relative_humidity_2m,wind_speed_10m,wind_gusts_10m,weather_code`;\n\n  const response = await fetch(weatherUrl);\n  const data: WeatherResponse = await response.json();\n\n  return {\n    temperature: data.current.temperature_2m,\n    feelsLike: data.current.apparent_temperature,\n    humidity: data.current.relative_humidity_2m,\n    windSpeed: data.current.wind_speed_10m,\n    windGust: data.current.wind_gusts_10m,\n    conditions: getWeatherCondition(data.current.weather_code),\n    location: name,\n  };\n};\n\nfunction getWeatherCondition(code: number): string {\n  const conditions: Record<number, string> = {\n    0: \"Clear sky\",\n    1: \"Mainly clear\",\n    2: \"Partly cloudy\",\n    3: \"Overcast\",\n    45: \"Foggy\",\n    48: \"Depositing rime fog\",\n    51: \"Light drizzle\",\n    53: \"Moderate drizzle\",\n    55: \"Dense drizzle\",\n    56: \"Light freezing drizzle\",\n    57: \"Dense freezing drizzle\",\n    61: \"Slight rain\",\n    63: \"Moderate rain\",\n    65: \"Heavy rain\",\n    66: \"Light freezing rain\",\n    67: \"Heavy freezing rain\",\n    71: \"Slight snow fall\",\n    73: \"Moderate snow fall\",\n    75: \"Heavy snow fall\",\n    77: \"Snow grains\",\n    80: \"Slight rain showers\",\n    81: \"Moderate rain showers\",\n    82: \"Violent rain showers\",\n    85: \"Slight snow showers\",\n    86: \"Heavy snow showers\",\n    95: \"Thunderstorm\",\n    96: \"Thunderstorm with slight hail\",\n    99: \"Thunderstorm with heavy hail\",\n  };\n  return conditions[code] || \"Unknown\";\n}\n\nconst weatherAgent = new Agent({\n  name: \"Weather Agent\",\n  instructions: `You are a helpful weather assistant that provides accurate weather information.\nYour primary function is to help users get weather details for specific locations. When responding:\n- Always ask for a location if none is provided\n- If the location name isn’t in English, please translate it\n- Include relevant details like humidity, wind conditions, and precipitation\n- Keep responses concise but informative\nUse the weatherTool to fetch current weather data.`,\n  model: openai(\"gpt-4o-mini\"),\n  tools: { weatherTool },\n});\n\nconst mastra = new Mastra({\n  agents: { weatherAgent },\n});\n\nasync function main() {\n  const agent = await mastra.getAgent(\"weatherAgent\");\n  const result = await agent.generate(\"What is the weather in London?\");\n  console.log(result.text);\n}\n\nmain();\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/using-a-tool\"\n  }\n/>\n\n\n---\ntitle: \"Example: Workflow as Tools | Agents | Mastra Docs\"\ndescription: Example of creating Agents in Mastra, demonstrating how to use workflows as tools. It shows how to suspend and resume workflows from an agent.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Workflow as Tools\n[EN] Source: https://mastra.ai/en/examples/agents/workflow-as-tools\n\nWhen building AI applications, you often need to coordinate multiple steps that depend on each other's outputs. This example shows how to create an AI workflow that fetches weather data from a workflow. It also demonstrates how to handle suspend and resume of workflows from an agent.\n\n### Workflow Definition\n\n```ts showLineNumbers copy\nimport { Mastra } from \"@mastra/core\";\nimport { Agent } from \"@mastra/core/agent\";\nimport { createStep, createWorkflow } from \"@mastra/core/workflows\";\nimport { createTool } from '@mastra/core/tools';\nimport { z } from \"zod\";\nimport { openai } from \"@ai-sdk/openai\";\n\nconst forecastSchema = z.object({\n  date: z.string(),\n  maxTemp: z.number(),\n  minTemp: z.number(),\n  precipitationChance: z.number(),\n  condition: z.string(),\n  location: z.string(),\n});\n\nfunction getWeatherCondition(code: number): string {\n  const conditions: Record<number, string> = {\n    0: 'Clear sky',\n    1: 'Mainly clear',\n    2: 'Partly cloudy',\n    3: 'Overcast',\n    45: 'Foggy',\n    48: 'Depositing rime fog',\n    51: 'Light drizzle',\n    53: 'Moderate drizzle',\n    55: 'Dense drizzle',\n    61: 'Slight rain',\n    63: 'Moderate rain',\n    65: 'Heavy rain',\n    71: 'Slight snow fall',\n    73: 'Moderate snow fall',\n    75: 'Heavy snow fall',\n    95: 'Thunderstorm',\n  };\n  return conditions[code] || 'Unknown';\n}\n\nconst fetchWeatherWithSuspend = createStep({\n  id: 'fetch-weather',\n  description: 'Fetches weather forecast for a given city',\n  inputSchema: z.object({}),\n  resumeSchema: z.object({\n    city: z.string().describe('The city to get the weather for'),\n  }),\n  outputSchema: forecastSchema,\n  execute: async ({ resumeData, suspend }) => {\n    if (!resumeData) {\n      suspend({\n        message: 'Please enter the city to get the weather for',\n      });\n\n      return {};\n    }\n\n    const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(resumeData.city)}&count=1`;\n    const geocodingResponse = await fetch(geocodingUrl);\n    const geocodingData = (await geocodingResponse.json()) as {\n      results: { latitude: number; longitude: number; name: string }[];\n    };\n\n    if (!geocodingData.results?.[0]) {\n      throw new Error(`Location '${resumeData.city}' not found`);\n    }\n\n    const { latitude, longitude, name } = geocodingData.results[0];\n\n    const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=precipitation,weathercode&timezone=auto,&hourly=precipitation_probability,temperature_2m`;\n    const response = await fetch(weatherUrl);\n    const data = (await response.json()) as {\n      current: {\n        time: string;\n        precipitation: number;\n        weathercode: number;\n      };\n      hourly: {\n        precipitation_probability: number[];\n        temperature_2m: number[];\n      };\n    };\n\n    const forecast = {\n      date: new Date().toISOString(),\n      maxTemp: Math.max(...data.hourly.temperature_2m),\n      minTemp: Math.min(...data.hourly.temperature_2m),\n      condition: getWeatherCondition(data.current.weathercode),\n      precipitationChance: data.hourly.precipitation_probability.reduce((acc, curr) => Math.max(acc, curr), 0),\n      location: resumeData.city,\n    };\n\n    return forecast;\n  },\n});\n\nconst weatherWorkflowWithSuspend = createWorkflow({\n  id: 'weather-workflow-with-suspend',\n  inputSchema: z.object({}),\n  outputSchema: forecastSchema,\n})\n  .then(fetchWeatherWithSuspend)\n  .commit();\n```\n\n### Tool Definitions\n\n```ts\nexport const startWeatherTool = createTool({\n  id: 'start-weather-tool',\n  description: 'Start the weather tool',\n  inputSchema: z.object({}),\n  outputSchema: z.object({\n    runId: z.string(),\n  }),\n  execute: async ({ context }) => {\n    const workflow = mastra.getWorkflow('weatherWorkflowWithSuspend');\n    const run = await workflow.createRunAsync();\n    await run.start({\n      inputData: {},\n    });\n\n    return {\n      runId: run.runId,\n    };\n  },\n});\n\nexport const resumeWeatherTool = createTool({\n  id: 'resume-weather-tool',\n  description: 'Resume the weather tool',\n  inputSchema: z.object({\n    runId: z.string(),\n    city: z.string().describe('City name'),\n  }),\n  outputSchema: forecastSchema,\n  execute: async ({ context }) => {\n    const workflow = mastra.getWorkflow('weatherWorkflowWithSuspend');\n    const run = await workflow.createRunAsync({\n      runId: context.runId,\n    });\n    const result = await run.resume({\n      step: 'fetch-weather',\n      resumeData: {\n        city: context.city,\n      },\n    });\n    return result.result;\n  },\n});\n```\n\n### Agent Definition\n\n```ts\nexport const weatherAgentWithWorkflow = new Agent({\n  name: 'Weather Agent with Workflow',\n  instructions: `You are a helpful weather assistant that provides accurate weather information.\n\nYour primary function is to help users get weather details for specific locations. When responding:\n- Always ask for a location if none is provided\n- If the location name isn’t in English, please translate it\n- If giving a location with multiple parts (e.g. \"New York, NY\"), use the most relevant part (e.g. \"New York\")\n- Include relevant details like humidity, wind conditions, and precipitation\n- Keep responses concise but informative\n\nUse the startWeatherTool to start the weather workflow. This will start and then suspend the workflow and return a runId.\nUse the resumeWeatherTool to resume the weather workflow. This takes the runId returned from the startWeatherTool and the city entered by the user. It will resume the workflow and return the result.\nThe result will be the weather forecast for the city.`,\n  model: openai('gpt-4o'),\n  tools: { startWeatherTool, resumeWeatherTool },\n});\n```\n\n### Agent Execution\n```ts\nconst mastra = new Mastra({\n  agents: { weatherAgentWithWorkflow },\n  workflows: { weatherWorkflowWithSuspend },\n});\n\nconst agent = mastra.getAgent('weatherAgentWithWorkflow');\nconst result = await agent.generate([\n  {\n    role: 'user',\n    content: 'London',\n  },\n]);\n\nconsole.log(result);\n```\n\n<br/>\n\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/workflow-as-tools\"\n  }\n/>\n\n\n---\ntitle: Deployment examples\n---\n\n# Deployment examples\n[EN] Source: https://mastra.ai/en/examples/deployment\n\nA few ways to extend your Mastra server during deployment. Each example assumes\n`Mastra` has already been initialised and focuses on server specific code.\n\n\n---\ntitle: \"Examples List: Workflows, Agents, RAG | Mastra Docs\"\ndescription: \"Explore practical examples of AI development with Mastra, including text generation, RAG implementations, structured outputs, and multi-modal interactions. Learn how to build AI applications using OpenAI, Anthropic, and Google Gemini.\"\n---\n\nimport { CardItems } from \"@/components/cards/card-items\";\nimport { Tabs } from \"nextra/components\";\n\n# Examples\n[EN] Source: https://mastra.ai/en/examples\n\nThe Examples section is a short list of example projects demonstrating basic AI engineering with Mastra, including text generation, structured output, streaming responses, retrieval‐augmented generation (RAG), and voice.\n\n<CardItems titles={[\"Agent\", \"Workflow\", \"legacyWorkflow\", \"Memory\", \"RAG\", \"Evals\", \"Voice\"]} items={\n  {\n    Agent: [\n      {\n        title: \"Agent with System Prompt\",\n        href: \"/examples/agents/system-prompt\",\n      },\n      {\n        title: \"Workflow as Tools\",\n        href: \"/examples/agents/workflow-as-tools\",\n      },\n      {\n        title: \"Using a Tool\",\n        href: \"/examples/agents/using-a-tool\",\n      },\n      {\n        title: \"Hierarchical Multi-Agent System\",\n        href: \"/examples/agents/hierarchical-multi-agent\",\n      },\n      {\n        title: \"Multi-Agent Workflow\",\n        href: \"/examples/agents/multi-agent-workflow\",\n      },\n      {\n        title: \"Bird Checker\",\n        href: \"/examples/agents/bird-checker\",\n      },\n      {\n        title: \"Dynamic Agents\",\n        href: \"/examples/agents/dynamic-agents\"\n      }\n    ],\n    Workflow: [\n      {\n        title: \"Conditional Branching\",\n        href: \"/examples/workflows/conditional-branching\",\n      },\n      {\n        title: \"Parallel Steps\",\n        href: \"/examples/workflows/parallel-steps\",\n      },\n      {\n        title: \"Calling an Agent\",\n        href: \"/examples/workflows/calling-agent\",\n      },\n      {\n        title: \"Tool & Agent as a Step\",\n        href: \"/examples/workflows/agent-and-tool-interop\",\n      },\n      {\n        title: \"Human in the loop\",\n        href: \"/examples/workflows/human-in-the-loop\",\n      },\n      {\n        title: \"Control Flow\",\n        href: \"/examples/workflows/control-flow\",\n      },\n      {\n        title: \"Array as Input\",\n        href: \"/examples/workflows/array-as-input\",\n      }\n    ],\n    legacyWorkflow: [\n      {\n        title: \"Creating a Workflow\",\n        href: \"/examples/workflows_legacy/creating-a-workflow\",\n      },\n      {\n        title: \"Using a Tool as a Step\",\n        href: \"/examples/workflows_legacy/using-a-tool-as-a-step\",\n      },\n      { title: \"Parallel Steps\", href: \"/examples/workflows_legacy/parallel-steps\" },\n      {\n        title: \"Sequential Steps\",\n        href: \"/examples/workflows_legacy/sequential-steps\",\n      },\n      { title: \"Branching Paths\", href: \"/examples/workflows_legacy/branching-paths\" },\n      {\n        title: \"Cyclical Dependencies\",\n        href: \"/examples/workflows_legacy/cyclical-dependencies\",\n      },\n      {\n        title: \"Suspend and Resume\",\n        href: \"/examples/workflows_legacy/suspend-and-resume\",\n      },\n      { title: \"Calling an Agent\", href: \"/examples/workflows_legacy/calling-agent\" },\n    ],\n    Memory:[\n      {\n        title: \"Long-term Memory with LibSQL\",\n        href: \"/examples/memory/memory-with-libsql\",\n      },\n      {\n        title: \"Long-term Memory with Postgres\",\n        href: \"/examples/memory/memory-with-pg\",\n      },\n      {\n        title: \"Long-term Memory with Upstash\",\n        href: \"/examples/memory/memory-with-upstash\",\n      },\n      {\n        title: \"Long-term Memory with Mem0\",\n        href: \"/examples/memory/memory-with-mem0\"\n      },\n      {\n        title: \"Streaming Working Memory (quickstart)\",\n        href: \"/examples/memory/streaming-working-memory\",\n      },\n      {\n        title: \"Streaming Working Memory (advanced)\",\n        href: \"/examples/memory/streaming-working-memory-advanced\",\n      },\n    ],\n    RAG: [\n      { title: \"Chunk Text\", href: \"/examples/rag/chunking/chunk-text\" },\n      { title: \"Chunk Markdown\", href: \"/examples/rag/chunking/chunk-markdown\" },\n      { title: \"Chunk HTML\", href: \"/examples/rag/chunking/chunk-html\" },\n      { title: \"Chunk JSON\", href: \"/examples/rag/chunking/chunk-json\" },\n      { title: \"Embed Text Chunk\", href: \"/examples/rag/embedding/embed-text-chunk\" },\n      { title: \"Embed Chunk Array\", href: \"/examples/rag/embedding/embed-chunk-array\" },\n      { title: \"Adjust Chunk Size\", href: \"/examples/rag/chunking/adjust-chunk-size\" },\n      {\n        title: \"Adjust Chunk Delimiters\",\n        href: \"/examples/rag/chunking/adjust-chunk-delimiters\",\n      },\n      {\n        title: \"Metadata Extraction\",\n        href: \"/examples/rag/embedding/metadata-extraction\",\n      },\n      {\n        title: \"Hybrid Vector Search\",\n        href: \"/examples/rag/query/hybrid-vector-search\",\n      },\n      {\n        title: \"Embed Text with Cohere\",\n        href: \"/examples/rag/embedding/embed-text-with-cohere\",\n      },\n      {\n        title: \"Upsert Embeddings\",\n        href: \"/examples/rag/upsert/upsert-embeddings\",\n      },\n      { title: \"Retrieve Results\", href: \"/examples/rag/query/retrieve-results\" },\n      { title: \"Using the Vector Query Tool\", href: \"/examples/rag/usage/basic-rag\" },\n      {\n        title: \"Optimizing Information Density\",\n        href: \"/examples/rag/usage/cleanup-rag\",\n      },\n      { title: \"Metadata Filtering\", href: \"/examples/rag/usage/filter-rag\" },\n      {\n        title: \"Re-ranking Results\",\n        href: \"/examples/rag/rerank/rerank\",\n      },\n      {\n        title: \"Re-ranking Results with Tools\",\n        href: \"/examples/rag/rerank/rerank-rag\",\n      },\n      { title: \"Chain of Thought Prompting\", href: \"/examples/rag/usage/cot-rag\" },\n      {\n        title: \"Structured Reasoning with Workflows\",\n        href: \"/examples/rag/usage/cot-workflow-rag\",\n      },\n      { title: \"Graph RAG\", href: \"/examples/rag/usage/graph-rag\" },\n    ],\n    Evals: [\n      {\n        title: \"Answer Relevancy\",\n        href: \"/examples/evals/answer-relevancy\",\n      },\n      {\n        title: \"Bias\",\n        href: \"/examples/evals/bias\",\n      },\n      {\n        title: \"Completeness\",\n        href: \"/examples/evals/completeness\",\n      },\n      {\n        title: \"Content Similarity\",\n        href: \"/examples/evals/content-similarity\",\n      },\n      {\n        title: \"Context Position\",\n        href: \"/examples/evals/context-position\",\n      },\n      {\n        title: \"Context Precision\",\n        href: \"/examples/evals/context-precision\",\n      },\n      {\n        title: \"Context Relevancy\",\n        href: \"/examples/evals/context-relevancy\",\n      },\n      {\n        title: \"Contextual Recall\",\n        href: \"/examples/evals/contextual-recall\",\n      },\n      {\n        title: \"Custom Eval with LLM as a Judge\",\n        href: \"/examples/evals/custom-eval\",\n      },\n      {\n        title: \"Faithfulness\",\n        href: \"/examples/evals/faithfulness\",\n      },\n      {\n        title: \"Hallucination\",\n        href: \"/examples/evals/hallucination\",\n      },\n      {\n        title: \"Keyword Coverage\",\n        href: \"/examples/evals/keyword-coverage\",\n      },\n      {\n        title: \"Prompt Alignment\",\n        href: \"/examples/evals/prompt-alignment\",\n      },\n      {\n        title: \"Summarization\",\n        href: \"/examples/evals/summarization\",\n      },\n      {\n        title: \"Textual Difference\",\n        href: \"/examples/evals/textual-difference\",\n      },\n      {\n        title: \"Tone Consistency\", \n        href: \"/examples/evals/tone-consistency\",\n      },\n      {\n        title: \"Toxicity\",\n        href: \"/examples/evals/toxicity\",\n      },\n      {\n        title: \"Word Inclusion\",\n        href: \"/examples/evals/word-inclusion\",\n      },\n    ],\n    Voice: [\n    {\n      title: \"Text to Speech\",\n      href: \"/examples/voice/text-to-speech\",\n    },\n    {\n      title: \"Speech to Text\",\n      href: \"/examples/voice/speech-to-text\",\n    },\n    {\n      title: \"Turn Taking\",\n      href: \"/examples/voice/turn-taking\",\n    },\n    {\n      title: \"Speech to Speech\",\n      href: \"/examples/voice/speech-to-speech\",\n    },\n    ],\n}}>\n\n</CardItems>\n\n\n---\ntitle: \"Example: Adjusting Chunk Delimiters | RAG | Mastra Docs\"\ndescription: Adjust chunk delimiters in Mastra to better match your content structure.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Adjust Chunk Delimiters\n[EN] Source: https://mastra.ai/en/examples/rag/chunking/adjust-chunk-delimiters\n\nWhen processing large documents, you may want to control how the text is split into smaller chunks. By default, documents are split on newlines, but you can customize this behavior to better match your content structure. This example shows how to specify a custom delimiter for chunking documents.\n\n```tsx copy\nimport { MDocument } from \"@mastra/rag\";\n\nconst doc = MDocument.fromText(\"Your plain text content...\");\n\nconst chunks = await doc.chunk({\n  separator: \"\\n\",\n});\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/adjust-chunk-delimiters\"\n  }\n/>\n\n\n---\ntitle: \"Example: Adjusting The Chunk Size | RAG | Mastra Docs\"\ndescription: Adjust chunk size in Mastra to better match your content and memory requirements.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Adjust Chunk Size\n[EN] Source: https://mastra.ai/en/examples/rag/chunking/adjust-chunk-size\n\nWhen processing large documents, you might need to adjust how much text is included in each chunk. By default, chunks are 1024 characters long, but you can customize this size to better match your content and memory requirements. This example shows how to set a custom chunk size when splitting documents.\n\n```tsx copy\nimport { MDocument } from \"@mastra/rag\";\n\nconst doc = MDocument.fromText(\"Your plain text content...\");\n\nconst chunks = await doc.chunk({\n  size: 512,\n});\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/adjust-chunk-size\"\n  }\n/>\n\n\n---\ntitle: \"Example: Semantically Chunking HTML | RAG | Mastra Docs\"\ndescription: Chunk HTML content in Mastra to semantically chunk the document.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Semantically Chunking HTML\n[EN] Source: https://mastra.ai/en/examples/rag/chunking/chunk-html\n\nWhen working with HTML content, you often need to break it down into smaller, manageable pieces while preserving the document structure. The chunk method splits HTML content intelligently, maintaining the integrity of HTML tags and elements. This example shows how to chunk HTML documents for search or retrieval purposes.\n\n```tsx copy\nimport { MDocument } from \"@mastra/rag\";\n\nconst html = `\n<div>\n    <h1>h1 content...</h1>\n    <p>p content...</p>\n</div>\n`;\n\nconst doc = MDocument.fromHTML(html);\n\nconst chunks = await doc.chunk({\n  headers: [\n    [\"h1\", \"Header 1\"],\n    [\"p\", \"Paragraph\"],\n  ],\n});\n\nconsole.log(chunks);\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-html\"\n  }\n/>\n\n\n---\ntitle: \"Example: Semantically Chunking JSON | RAG | Mastra Docs\"\ndescription: Chunk JSON data in Mastra to semantically chunk the document.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Semantically Chunking JSON\n[EN] Source: https://mastra.ai/en/examples/rag/chunking/chunk-json\n\nWhen working with JSON data, you need to split it into smaller pieces while preserving the object structure. The chunk method breaks down JSON content intelligently, maintaining the relationships between keys and values. This example shows how to chunk JSON documents for search or retrieval purposes.\n\n```tsx copy\nimport { MDocument } from \"@mastra/rag\";\n\nconst testJson = {\n  name: \"John Doe\",\n  age: 30,\n  email: \"john.doe@example.com\",\n};\n\nconst doc = MDocument.fromJSON(JSON.stringify(testJson));\n\nconst chunks = await doc.chunk({\n  maxSize: 100,\n});\n\nconsole.log(chunks);\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-json\"\n  }\n/>\n\n\n---\ntitle: \"Example: Semantically Chunking Markdown | RAG | Mastra Docs\"\ndescription: Example of using Mastra to chunk markdown documents for search or retrieval purposes.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Chunk Markdown\n[EN] Source: https://mastra.ai/en/examples/rag/chunking/chunk-markdown\n\nMarkdown is more information-dense than raw HTML, making it easier to work with for RAG pipelines. When working with markdown, you need to split it into smaller pieces while preserving headers and formatting. The `chunk` method handles Markdown-specific elements like headers, lists, and code blocks intelligently. This example shows how to chunk markdown documents for search or retrieval purposes.\n\n```tsx copy\nimport { MDocument } from \"@mastra/rag\";\n\nconst doc = MDocument.fromMarkdown(\"# Your markdown content...\");\n\nconst chunks = await doc.chunk();\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-markdown\"\n  }\n/>\n\n\n---\ntitle: \"Example: Semantically Chunking Text | RAG | Mastra Docs\"\ndescription: Example of using Mastra to split large text documents into smaller chunks for processing.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Chunk Text\n[EN] Source: https://mastra.ai/en/examples/rag/chunking/chunk-text\n\nWhen working with large text documents, you need to break them down into smaller, manageable pieces for processing. The chunk method splits text content into segments that can be used for search, analysis, or retrieval. This example shows how to split plain text into chunks using default settings.\n\n```tsx copy\nimport { MDocument } from \"@mastra/rag\";\n\nconst doc = MDocument.fromText(\"Your plain text content...\");\n\nconst chunks = await doc.chunk();\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-text\"\n  }\n/>\n\n\n---\ntitle: \"Example: Embedding Chunk Arrays | RAG | Mastra Docs\"\ndescription: Example of using Mastra to generate embeddings for an array of text chunks for similarity search.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Embed Chunk Array\n[EN] Source: https://mastra.ai/en/examples/rag/embedding/embed-chunk-array\n\nAfter chunking documents, you need to convert the text chunks into numerical vectors that can be used for similarity search. The `embed` method transforms text chunks into embeddings using your chosen provider and model. This example shows how to generate embeddings for an array of text chunks.\n\n```tsx copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { MDocument } from \"@mastra/rag\";\nimport { embed } from \"ai\";\n\nconst doc = MDocument.fromText(\"Your text content...\");\n\nconst chunks = await doc.chunk();\n\nconst { embeddings } = await embedMany({\n  model: openai.embedding(\"text-embedding-3-small\"),\n  values: chunks.map((chunk) => chunk.text),\n});\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/embed-chunk-array\"\n  }\n/>\n\n\n---\ntitle: \"Example: Embedding Text Chunks | RAG | Mastra Docs\"\ndescription: Example of using Mastra to generate an embedding for a single text chunk for similarity search.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Embed Text Chunk\n[EN] Source: https://mastra.ai/en/examples/rag/embedding/embed-text-chunk\n\nWhen working with individual text chunks, you need to convert them into numerical vectors for similarity search. The `embed` method transforms a single text chunk into an embedding using your chosen provider and model.\n\n```tsx copy\nimport { openai } from \"@ai-sdk/openai\";\nimport { MDocument } from \"@mastra/rag\";\nimport { embed } from \"ai\";\n\nconst doc = MDocument.fromText(\"Your text content...\");\n\nconst chunks = await doc.chunk();\n\nconst { embedding } = await embed({\n  model: openai.embedding(\"text-embedding-3-small\"),\n  value: chunks[0].text,\n});\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/embed-text-chunk\"\n  }\n/>\n\n\n---\ntitle: \"Example: Embedding Text with Cohere | RAG | Mastra Docs\"\ndescription: Example of using Mastra to generate embeddings using Cohere's embedding model.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Embed Text with Cohere\n[EN] Source: https://mastra.ai/en/examples/rag/embedding/embed-text-with-cohere\n\nWhen working with alternative embedding providers, you need a way to generate vectors that match your chosen model's specifications. The `embed` method supports multiple providers, allowing you to switch between different embedding services. This example shows how to generate embeddings using Cohere's embedding model.\n\n```tsx copy\nimport { cohere } from \"@ai-sdk/cohere\";\nimport { MDocument } from \"@mastra/rag\";\nimport { embedMany } from \"ai\";\n\nconst doc = MDocument.fromText(\"Your text content...\");\n\nconst chunks = await doc.chunk();\n\nconst { embeddings } = await embedMany({\n  model: cohere.embedding(\"embed-english-v3.0\"),\n  values: chunks.map((chunk) => chunk.text),\n});\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/embed-text-with-cohere\"\n  }\n/>\n\n\n---\ntitle: \"Example: Metadata Extraction | Retrieval | RAG | Mastra Docs\"\ndescription: Example of extracting and utilizing metadata from documents in Mastra for enhanced document processing and retrieval.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Metadata Extraction\n[EN] Source: https://mastra.ai/en/examples/rag/embedding/metadata-extraction\n\nThis example demonstrates how to extract and utilize metadata from documents using Mastra's document processing capabilities.\nThe extracted metadata can be used for document organization, filtering, and enhanced retrieval in RAG systems.\n\n## Overview\n\nThe system demonstrates metadata extraction in two ways:\n\n1. Direct metadata extraction from a document\n2. Chunking with metadata extraction\n\n## Setup\n\n### Dependencies\n\nImport the necessary dependencies:\n\n```typescript copy showLineNumbers filename=\"src/index.ts\"\nimport { MDocument } from \"@mastra/rag\";\n```\n\n## Document Creation\n\nCreate a document from text content:\n\n```typescript copy showLineNumbers{3} filename=\"src/index.ts\"\nconst doc = MDocument.fromText(`Title: The Benefits of Regular Exercise\n\nRegular exercise has numerous health benefits. It improves cardiovascular health, \nstrengthens muscles, and boosts mental wellbeing.\n\nKey Benefits:\n• Reduces stress and anxiety\n• Improves sleep quality\n• Helps maintain healthy weight\n• Increases energy levels\n\nFor optimal results, experts recommend at least 150 minutes of moderate exercise \nper week.`);\n```\n\n## 1. Direct Metadata Extraction\n\nExtract metadata directly from the document:\n\n```typescript copy showLineNumbers{17} filename=\"src/index.ts\"\n// Configure metadata extraction options\nawait doc.extractMetadata({\n  keywords: true, // Extract important keywords\n  summary: true, // Generate a concise summary\n});\n\n// Retrieve the extracted metadata\nconst meta = doc.getMetadata();\nconsole.log(\"Extracted Metadata:\", meta);\n\n// Example Output:\n// Extracted Metadata: {\n//   keywords: [\n//     'exercise',\n//     'health benefits',\n//     'cardiovascular health',\n//     'mental wellbeing',\n//     'stress reduction',\n//     'sleep quality'\n//   ],\n//   summary: 'Regular exercise provides multiple health benefits including improved cardiovascular health, muscle strength, and mental wellbeing. Key benefits include stress reduction, better sleep, weight management, and increased energy. Recommended exercise duration is 150 minutes per week.'\n// }\n```\n\n## 2. Chunking with Metadata\n\nCombine document chunking with metadata extraction:\n\n```typescript copy showLineNumbers{40} filename=\"src/index.ts\"\n// Configure chunking with metadata extraction\nawait doc.chunk({\n  strategy: \"recursive\", // Use recursive chunking strategy\n  size: 200, // Maximum chunk size\n  extract: {\n    keywords: true, // Extract keywords per chunk\n    summary: true, // Generate summary per chunk\n  },\n});\n\n// Get metadata from chunks\nconst metaTwo = doc.getMetadata();\nconsole.log(\"Chunk Metadata:\", metaTwo);\n\n// Example Output:\n// Chunk Metadata: {\n//   keywords: [\n//     'exercise',\n//     'health benefits',\n//     'cardiovascular health',\n//     'mental wellbeing',\n//     'stress reduction',\n//     'sleep quality'\n//   ],\n//   summary: 'Regular exercise provides multiple health benefits including improved cardiovascular health, muscle strength, and mental wellbeing. Key benefits include stress reduction, better sleep, weight management, and increased energy. Recommended exercise duration is 150 minutes per week.'\n// }\n```\n\n<br />\n<br />\n<hr className=\"dark:border-[#404040] border-gray-300\" />\n<br />\n<br />\n<GithubLink\n  link={\n    \"https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/metadata-extraction\"\n  }\n/>\n\n\n---\ntitle: \"Example: Hybrid Vector Search | RAG | Mastra Docs\"\ndescription: Example of using metadata filters with PGVector to enhance vector search results in Mastra.\n---\n\nimport { GithubLink } from \"@/components/github-link\";\n\n# Hybrid Vector Search\n[EN] Source: https://mastra.ai/en/examples/rag/query/hybrid-vector-search\n\nWhen you combine vector similarity search with metadata filters, you can create a hybrid search that is more precise and efficient.\nThis approach combines:\n\n- Vector similarity search to find the most relevant documents\n- Metadata filters to refine the search results based on additional criteria\n\nThis example demonstrates how to use hybrid vector search with Mastra and PGVector.\n\n## Overview\n\nThe system implements filtered vector search using Mastra and PGVector. Here's what it does:\n\n1. Queries existing embeddings in PGVector with metadata filters\n2. Shows how to filter by different metadata fields\n3. Demonstrates combining vector similarity with metadata filtering\n\n> **Note**: For examples of how to extract metadata from your documents, see the [Metadata Extraction](../embedding/metadata-extraction.mdx) guide.\n>\n> To learn how to create and store embeddings, see the [Upsert Embeddings](/examples/rag/upsert/upsert-embeddings) guide.\n\n## Setup\n\n### Environment Setup\n\nMake sure to set up your environment variables:\n\n```bash filename=\".env\"\nOPENAI_API_KEY=your_openai_api_key_here\nPOSTGRES_CONNECTION_STRING=your_connection_string_here\n```\n\n### Dependencies\n\nImport the necessary dependencies:\n\n```typescript copy showLineNumbers filename=\"src/index.ts\"\nimport { embed } from \"ai\";\nimport { PgVector } from \"@mastra/pg\";\nimport { openai } from \"@ai-sdk/openai\";\n```\n\n## Vector Store Initialization\n\nInitialize PgVector with your connection string:\n\n```typescript copy showLineNumbers{4} filename=\"src/index.ts\"\nconst pgVector =","size_bytes":360000},"scripts/build.sh":{"content":"#!/usr/bin/env bash\n\nset -e\n\nexec mastra build\n","size_bytes":47},"scripts/inngest.sh":{"content":"#!/usr/bin/env bash\n\nset -e\n\nINNGEST_CONFIG=\".config/inngest/inngest.yaml\"\n\n# Try to store Inngest data in Postgres if it's available. Otherwise, put it in SQLite.\nif [[ ! -f  \"${INNGEST_CONFIG}\" ]]; then\n    mkdir -p \"$(dirname \"${INNGEST_CONFIG}\")\"\n    if [[ -z \"${DATABASE_URL}\" ]]; then\n        printf 'postgres-uri: \"%s\"' \"${DATABASE_URL}\" > \"${INNGEST_CONFIG}\"\n    else\n        printf 'sqlite-dir: \"/home/runner/workspace/.local/share/inngest\"' > \"${INNGEST_CONFIG}\"\n    fi\nfi\nexec inngest-cli dev -u http://localhost:5000/api/inngest --host 127.0.0.1 --port 3000 --config \"${INNGEST_CONFIG}\"\n","size_bytes":599},"src/global.d.ts":{"content":"declare module \"mastra\";\n","size_bytes":25},"src/mastra/index.ts":{"content":"import { Mastra } from \"@mastra/core\";\nimport { MastraError } from \"@mastra/core/error\";\nimport { PinoLogger } from \"@mastra/loggers\";\nimport { LogLevel, MastraLogger } from \"@mastra/core/logger\";\nimport pino from \"pino\";\nimport { MCPServer } from \"@mastra/mcp\";\nimport { NonRetriableError } from \"inngest\";\nimport { z } from \"zod\";\n\nimport { sharedPostgresStorage } from \"./storage\";\nimport { inngest, inngestServe } from \"./inngest\";\nimport { wikipediaResearchTool } from \"./tools/wikipediaResearchTool\";\nimport { webScrapingTool } from \"./tools/webScrapingTool\";\nimport { pdfGenerationTool } from \"./tools/pdfGenerationTool\";\nimport { aiServiceTool } from \"./tools/aiServiceTool\";\nimport { chunkedContentGenerationTool } from \"./tools/chunkedContentGenerationTool\";\nimport { progressTrackingTool } from \"./tools/progressTrackingTool\";\nimport { ideaGenerationAgent } from \"./agents/ideaGenerationAgent\";\nimport { writingAgent } from \"./agents/writingAgent\";\nimport { reviewAgent } from \"./agents/reviewAgent\";\nimport { improvedEducationalContentWorkflow } from \"./workflows/improvedEducationalContentWorkflow\";\nimport { registerCronWorkflow } from \"./inngest\";\n\nclass ProductionPinoLogger extends MastraLogger {\n  protected logger: pino.Logger;\n\n  constructor(\n    options: {\n      name?: string;\n      level?: LogLevel;\n    } = {},\n  ) {\n    super(options);\n\n    this.logger = pino({\n      name: options.name || \"app\",\n      level: options.level || LogLevel.INFO,\n      base: {},\n      formatters: {\n        level: (label: string, _number: number) => ({\n          level: label,\n        }),\n      },\n      timestamp: () => `,\"time\":\"${new Date(Date.now()).toISOString()}\"`,\n    });\n  }\n\n  debug(message: string, args: Record<string, any> = {}): void {\n    this.logger.debug(args, message);\n  }\n\n  info(message: string, args: Record<string, any> = {}): void {\n    this.logger.info(args, message);\n  }\n\n  warn(message: string, args: Record<string, any> = {}): void {\n    this.logger.warn(args, message);\n  }\n\n  error(message: string, args: Record<string, any> = {}): void {\n    this.logger.error(args, message);\n  }\n}\n\nexport const mastra = new Mastra({\n  storage: sharedPostgresStorage,\n  agents: {},\n  workflows: { improvedEducationalContentWorkflow },\n  mcpServers: {\n    allTools: new MCPServer({\n      name: \"allTools\",\n      version: \"1.0.0\",\n      tools: {\n        wikipediaResearchTool,\n        webScrapingTool,\n        pdfGenerationTool,\n        aiServiceTool,\n        chunkedContentGenerationTool,\n        progressTrackingTool,\n      },\n    }),\n  },\n  bundler: {\n    // A few dependencies are not properly picked up by\n    // the bundler if they are not added directly to the\n    // entrypoint.\n    externals: [\n      \"@slack/web-api\",\n      \"inngest\",\n      \"inngest/hono\",\n      \"hono\",\n      \"hono/streaming\",\n    ],\n    // sourcemaps are good for debugging.\n    sourcemap: true,\n  },\n  server: {\n    host: \"0.0.0.0\",\n    port: 5000,\n    middleware: [\n      async (c, next) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra?.getLogger();\n        logger?.debug(\"[Request]\", { method: c.req.method, url: c.req.url });\n        try {\n          await next();\n        } catch (error) {\n          logger?.error(\"[Response]\", {\n            method: c.req.method,\n            url: c.req.url,\n            error,\n          });\n          if (error instanceof MastraError) {\n            if (error.id === \"AGENT_MEMORY_MISSING_RESOURCE_ID\") {\n              // This is typically a non-retirable error. It means that the request was not\n              // setup correctly to pass in the necessary parameters.\n              throw new NonRetriableError(error.message, { cause: error });\n            }\n          } else if (error instanceof z.ZodError) {\n            // Validation errors are never retriable.\n            throw new NonRetriableError(error.message, { cause: error });\n          }\n\n          throw error;\n        }\n      },\n    ],\n    apiRoutes: [\n      // This API route is used to register the Mastra workflow (inngest function) on the inngest server\n      {\n        path: \"/api/inngest\",\n        method: \"ALL\",\n        createHandler: async ({ mastra }) => inngestServe({ mastra, inngest }),\n        // The inngestServe function integrates Mastra workflows with Inngest by:\n        // 1. Creating Inngest functions for each workflow with unique IDs (workflow.${workflowId})\n        // 2. Setting up event handlers that:\n        //    - Generate unique run IDs for each workflow execution\n        //    - Create an InngestExecutionEngine to manage step execution\n        //    - Handle workflow state persistence and real-time updates\n        // 3. Establishing a publish-subscribe system for real-time monitoring\n        //    through the workflow:${workflowId}:${runId} channel\n      },\n    ],\n  },\n  logger:\n    process.env.NODE_ENV === \"production\"\n      ? new ProductionPinoLogger({\n          name: \"Mastra\",\n          level: \"info\",\n        })\n      : new PinoLogger({\n          name: \"Mastra\",\n          level: \"info\",\n        }),\n});\n\n/*  Sanity check 1: Throw an error if there are more than 1 workflows.  */\n// !!!!!! Do not remove this check. !!!!!!\nif (Object.keys(mastra.getWorkflows()).length > 1) {\n  throw new Error(\n    \"More than 1 workflows found. Currently, more than 1 workflows are not supported in the UI, since doing so will cause app state to be inconsistent.\",\n  );\n}\n\n/*  Sanity check 2: Throw an error if there are more than 1 agents.  */\n// !!!!!! Do not remove this check. !!!!!!\nif (Object.keys(mastra.getAgents()).length > 1) {\n  throw new Error(\n    \"More than 1 agents found. Currently, more than 1 agents are not supported in the UI, since doing so will cause app state to be inconsistent.\",\n  );\n}\n\n// Register the improved educational content creation workflow to run daily at 9 AM\nregisterCronWorkflow(\n  `TZ=${process.env.SCHEDULE_CRON_TIMEZONE || 'America/Los_Angeles'} ${process.env.SCHEDULE_CRON_EXPRESSION || '0 9 * * *'}`,\n  improvedEducationalContentWorkflow\n);\n","size_bytes":6037},"src/triggers/slackTriggers.ts":{"content":"import { format } from \"node:util\";\nimport { Mastra, type WorkflowResult } from \"@mastra/core\";\nimport { IMastraLogger } from \"@mastra/core/logger\";\nimport {\n  type AuthTestResponse,\n  type ChatPostMessageResponse,\n  type ConversationsOpenResponse,\n  type ConversationsRepliesResponse,\n  type WebAPICallError,\n  ErrorCode,\n  WebClient,\n} from \"@slack/web-api\";\nimport type { Context, Handler, MiddlewareHandler } from \"hono\";\nimport { streamSSE } from \"hono/streaming\";\n\nimport { registerApiRoute } from \"../mastra/inngest\";\n\nexport type Methods = \"GET\" | \"POST\" | \"PUT\" | \"DELETE\" | \"PATCH\" | \"ALL\";\n\n// TODO: Remove when Mastra exports this type.\nexport type ApiRoute =\n  | {\n      path: string;\n      method: Methods;\n      handler: Handler;\n      middleware?: MiddlewareHandler | MiddlewareHandler[];\n    }\n  | {\n      path: string;\n      method: Methods;\n      createHandler: ({ mastra }: { mastra: Mastra }) => Promise<Handler>;\n      middleware?: MiddlewareHandler | MiddlewareHandler[];\n    };\n\nexport type TriggerInfoSlackOnNewMessage = {\n  type: \"slack/message.channels\";\n  params: {\n    channel: string;\n    channelDisplayName: string;\n  };\n  payload: any;\n};\n\ntype DiagnosisStep =\n  | {\n      status: \"pending\";\n      name: string;\n      extra?: Record<string, any>;\n    }\n  | {\n      status: \"success\";\n      name: string;\n      extra: Record<string, any>;\n    }\n  | {\n      status: \"failed\";\n      name: string;\n      error: string;\n      extra: Record<string, any>;\n    };\n\nexport async function getClient() {\n  let connectionSettings: any;\n  async function getAccessToken() {\n    if (\n      connectionSettings &&\n      connectionSettings.settings.expires_at &&\n      new Date(connectionSettings.settings.expires_at).getTime() > Date.now()\n    ) {\n      return {\n        token: connectionSettings.settings.access_token,\n        user: connectionSettings.settings.oauth?.credentials?.raw?.authed_user\n          ?.id,\n      };\n    }\n\n    const xReplitToken = process.env.REPL_IDENTITY\n      ? \"repl \" + process.env.REPL_IDENTITY\n      : process.env.WEB_REPL_RENEWAL\n        ? \"depl \" + process.env.WEB_REPL_RENEWAL\n        : null;\n\n    if (!xReplitToken) {\n      throw new Error(\"X_REPLIT_TOKEN not found for repl/depl\");\n    }\n\n    connectionSettings = await fetch(\n      \"https://connectors.replit.com/api/v2/connection?include_secrets=true&connector_names=slack-agent\",\n      {\n        headers: {\n          Accept: \"application/json\",\n          X_REPLIT_TOKEN: xReplitToken,\n        },\n      },\n    )\n      .then((res) => res.json())\n      .then((data) => data?.items?.[0]);\n    if (!connectionSettings || !connectionSettings.settings.access_token) {\n      throw new Error(\"Slack not connected\");\n    }\n    return {\n      token: connectionSettings.settings.access_token,\n      user: connectionSettings.settings.oauth?.credentials?.raw?.authed_user\n        ?.id,\n    };\n  }\n\n  const { token, user } = await getAccessToken();\n  const slack = new WebClient(token);\n\n  const response = await slack.auth.test();\n\n  return { slack, auth: response, user };\n}\n\n// Keep up to 200 recent events, to prevent duplicates\nconst recentEvents: string[] = [];\n\nfunction isWebAPICallError(err: unknown): err is WebAPICallError {\n  return (\n    err !== null && typeof err === \"object\" && \"code\" in err && \"data\" in err\n  );\n}\n\nfunction checkDuplicateEvent(eventName: string) {\n  if (recentEvents.includes(eventName)) {\n    return true;\n  }\n  recentEvents.push(eventName);\n  if (recentEvents.length > 200) {\n    recentEvents.shift();\n  }\n  return false;\n}\n\nfunction createReactToMessage({\n  slack,\n  logger,\n}: {\n  slack: WebClient;\n  logger: IMastraLogger;\n}) {\n  const addReaction = async (\n    channel: string,\n    timestamp: string,\n    emoji: string,\n  ) => {\n    logger.info(`[Slack] Adding reaction to message`, {\n      emoji,\n      timestamp,\n      channel,\n    });\n    try {\n      await slack.reactions.add({ channel, timestamp, name: emoji });\n    } catch (error) {\n      logger.error(`[Slack] Error adding reaction to message`, {\n        emoji,\n        timestamp,\n        channel,\n        error,\n      });\n    }\n  };\n\n  const removeAllReactions = async (channel: string, timestamp: string) => {\n    logger.info(`[Slack] Removing all reactions from message`, {\n      timestamp,\n      channel,\n    });\n    const emojis = [\n      \"hourglass\",\n      \"hourglass_flowing_sand\",\n      \"white_check_mark\",\n      \"x\",\n      \"alarm_clock\",\n    ];\n\n    for (const emoji of emojis) {\n      try {\n        await slack.reactions.remove({ channel, timestamp, name: emoji });\n      } catch (error) {\n        if (\n          isWebAPICallError(error) &&\n          (error.code !== ErrorCode.PlatformError ||\n            error.data?.error !== \"no_reaction\")\n        ) {\n          logger.error(\"[Slack] Error removing reaction\", {\n            emoji,\n            timestamp,\n            channel,\n            error,\n          });\n        }\n      }\n    }\n  };\n\n  return async function reactToMessage(\n    channel: string,\n    timestamp: string,\n    result: WorkflowResult<any, any> | null,\n  ) {\n    // Remove all of our reactions.\n    await removeAllReactions(channel, timestamp);\n    if (result?.status === \"success\") {\n      await addReaction(channel, timestamp, \"white_check_mark\");\n    } else if (result?.status === \"failed\") {\n      await addReaction(channel, timestamp, \"x\");\n    } else if (result !== null) {\n      await addReaction(channel, timestamp, \"alarm_clock\");\n    }\n  };\n}\n\nexport function registerSlackTrigger<\n  Env extends { Variables: { mastra: Mastra } },\n>({\n  triggerType,\n  handler,\n}: {\n  triggerType: string;\n  handler: (\n    mastra: Mastra,\n    triggerInfo: TriggerInfoSlackOnNewMessage,\n  ) => Promise<WorkflowResult<any, any> | null>;\n}): Array<ApiRoute> {\n  return [\n    registerApiRoute(\"/webhooks/slack/action\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra.getLogger();\n        try {\n          const payload = await c.req.json();\n          const { slack, auth } = await getClient();\n          const reactToMessage = createReactToMessage({ slack, logger });\n\n          // Handle challenge\n          if (payload && payload[\"challenge\"]) {\n            return c.text(payload[\"challenge\"], 200);\n          }\n\n          logger?.info(\"📝 [Slack] payload\", { payload });\n\n          // Augment event with channel info\n          if (payload && payload.event && payload.event.channel) {\n            try {\n              const result = await slack.conversations.info({\n                channel: payload.event.channel,\n              });\n              logger?.info(\"📝 [Slack] result\", { result });\n              payload.channel = result.channel;\n            } catch (error) {\n              logger?.error(\"Error fetching channel info\", {\n                error: format(error),\n              });\n              // Continue processing even if channel info fetch fails\n            }\n          }\n\n          // Check subtype\n          if (\n            payload.event?.subtype === \"message_changed\" ||\n            payload.event?.subtype === \"message_deleted\"\n          ) {\n            return c.text(\"OK\", 200);\n          }\n\n          if (\n            (payload.event?.channel_type === \"im\" &&\n              payload.event?.text === \"test:ping\") ||\n            payload.event?.text === `<@${auth.user_id}> test:ping`\n          ) {\n            // This is a test message to the bot saying just \"test:ping\", or a mention that contains \"test:ping\".\n            // We'll reply in the same thread.\n            await slack.chat.postMessage({\n              channel: payload.event.channel,\n              text: \"pong\",\n              thread_ts: payload.event.ts,\n            });\n            logger?.info(\"📝 [Slack] pong\");\n            return c.text(\"OK\", 200);\n          }\n\n          if (payload.event?.bot_id) {\n            return c.text(\"OK\", 200);\n          }\n\n          if (checkDuplicateEvent(payload.event_id)) {\n            return c.text(\"OK\", 200);\n          }\n\n          const result = await handler(mastra, {\n            type: triggerType,\n            params: {\n              channel: payload.event.channel,\n              channelDisplayName: payload.channel.name,\n            },\n            payload,\n          } as TriggerInfoSlackOnNewMessage);\n\n          await reactToMessage(payload.event.channel, payload.event.ts, result);\n\n          return c.text(\"OK\", 200);\n        } catch (error) {\n          logger?.error(\"Error handling Slack webhook\", {\n            error: format(error),\n          });\n          return c.text(\"Internal Server Error\", 500);\n        }\n      },\n    }),\n    {\n      path: \"/test/slack\",\n      method: \"GET\",\n      handler: async (c: Context<Env>) => {\n        return streamSSE(c, async (stream) => {\n          let id = 1;\n          const mastra = c.get(\"mastra\");\n          const logger = mastra.getLogger() ?? {\n            info: console.log,\n            error: console.error,\n          };\n\n          let diagnosisStepAuth: DiagnosisStep = {\n            status: \"pending\",\n            name: \"authentication with Slack\",\n          };\n          let diagnosisStepConversation: DiagnosisStep = {\n            status: \"pending\",\n            name: \"open a conversation with user\",\n          };\n          let diagnosisStepPostMessage: DiagnosisStep = {\n            status: \"pending\",\n            name: \"send a message to the user\",\n          };\n          let diagnosisStepReadReplies: DiagnosisStep = {\n            status: \"pending\",\n            name: \"read replies from bot\",\n          };\n          const updateDiagnosisSteps = async (event: string) =>\n            stream.writeSSE({\n              data: JSON.stringify([\n                diagnosisStepAuth,\n                diagnosisStepConversation,\n                diagnosisStepPostMessage,\n                diagnosisStepReadReplies,\n              ]),\n              event,\n              id: String(id++),\n            });\n\n          let slack: WebClient;\n          let auth: AuthTestResponse;\n          let user: string | undefined;\n          try {\n            ({ slack, auth, user } = await getClient());\n          } catch (error) {\n            logger?.error(\"❌ [Slack] test:auth failed\", {\n              error,\n            });\n            diagnosisStepAuth = {\n              ...diagnosisStepAuth,\n              status: \"failed\",\n              error: \"authentication failed\",\n              extra: { error: format(error) },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          if (!auth?.user_id || !user) {\n            logger?.error(\"❌ [Slack] test:auth not working\", {\n              auth,\n            });\n            diagnosisStepAuth = {\n              ...diagnosisStepAuth,\n              status: \"failed\",\n              error: \"authentication failed\",\n              extra: { auth },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          diagnosisStepAuth = {\n            ...diagnosisStepAuth,\n            status: \"success\",\n            extra: { auth },\n          };\n          await updateDiagnosisSteps(\"progress\");\n\n          logger?.info(\"📝 [Slack] test:auth found\", { auth });\n\n          // Open a DM with itself.\n          let conversation: ConversationsOpenResponse;\n          try {\n            conversation = await slack.conversations.open({\n              users: user,\n            });\n          } catch (error) {\n            logger?.error(\"❌ [Slack] test:conversation not found\", {\n              error,\n            });\n            diagnosisStepConversation = {\n              ...diagnosisStepConversation,\n              status: \"failed\",\n              error: \"opening a conversation failed\",\n              extra: { error: format(error) },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          if (!conversation?.channel?.id) {\n            logger?.error(\"❌ [Slack] test:conversation not found\", {\n              conversation,\n            });\n            diagnosisStepConversation = {\n              ...diagnosisStepConversation,\n              status: \"failed\",\n              error: \"conversation channel not found\",\n              extra: { conversation },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          diagnosisStepConversation = {\n            ...diagnosisStepConversation,\n            status: \"success\",\n            extra: { conversation },\n          };\n          await updateDiagnosisSteps(\"progress\");\n\n          logger?.info(\"📝 [Slack] test:conversation found\", { conversation });\n\n          // Post a message in the DMs.\n          let message: ChatPostMessageResponse;\n          try {\n            message = await slack.chat.postMessage({\n              channel: conversation.channel.id,\n              text: `<@${auth.user_id}> test:ping`,\n            });\n          } catch (error) {\n            logger?.error(\"❌ [Slack] test:message not posted\", {\n              error,\n            });\n            diagnosisStepPostMessage = {\n              ...diagnosisStepPostMessage,\n              status: \"failed\",\n              error: \"posting message failed\",\n              extra: { error: format(error) },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          if (!message?.ts) {\n            logger?.error(\"❌ [Slack] test:message not posted\", { message });\n            diagnosisStepPostMessage = {\n              ...diagnosisStepPostMessage,\n              status: \"failed\",\n              error: \"posting message missing timestamp\",\n              extra: { message },\n            };\n            await updateDiagnosisSteps(\"error\");\n            return;\n          }\n\n          logger?.info(\"📝 [Slack] test:ping sent\", { message });\n\n          diagnosisStepPostMessage = {\n            ...diagnosisStepPostMessage,\n            status: \"success\",\n            extra: { message },\n          };\n          await updateDiagnosisSteps(\"progress\");\n\n          const sleep = (ms: number) =>\n            new Promise((resolve) => setTimeout(resolve, ms));\n\n          // Wait for the bot to reply.\n          let lastReplies: ConversationsRepliesResponse | undefined = undefined;\n          for (let i = 0; i < 30; i++) {\n            await sleep(1000);\n            let replies: ConversationsRepliesResponse;\n            try {\n              replies = await slack.conversations.replies({\n                ts: message.ts,\n                channel: conversation.channel.id,\n              });\n            } catch (error) {\n              logger?.error(\"❌ [Slack] test:replies not found\", { message });\n              diagnosisStepReadReplies = {\n                ...diagnosisStepReadReplies,\n                status: \"failed\",\n                error: \"replies not found\",\n                extra: { error: format(error) },\n              };\n              await updateDiagnosisSteps(\"error\");\n              return;\n            }\n            logger?.info(\"📝 [Slack] test:replies\", { replies });\n            diagnosisStepReadReplies.extra = { replies };\n            lastReplies = replies;\n            if (replies?.messages?.some((m) => m.text === \"pong\")) {\n              // Victory!\n              logger?.info(\"📝 [Slack] test:pong successful\");\n              diagnosisStepReadReplies = {\n                ...diagnosisStepReadReplies,\n                status: \"success\",\n                extra: { replies },\n              };\n              await updateDiagnosisSteps(\"result\");\n              return;\n            }\n\n            await updateDiagnosisSteps(\"progress\");\n          }\n\n          logger?.info(\"📝 [Slack] test:timeout\");\n\n          diagnosisStepReadReplies = {\n            ...diagnosisStepReadReplies,\n            status: \"failed\",\n            error: \"replies timed out\",\n            extra: { lastReplies },\n          };\n          await updateDiagnosisSteps(\"error\");\n        });\n      },\n    },\n  ];\n}\n","size_bytes":16006},"src/triggers/telegramTriggers.ts":{"content":"import type { ContentfulStatusCode } from \"hono/utils/http-status\";\n\nimport { registerApiRoute } from \"../mastra/inngest\";\nimport { Mastra } from \"@mastra/core\";\n\nif (!process.env.TELEGRAM_BOT_TOKEN) {\n  console.warn(\n    \"Trying to initialize Telegram triggers without TELEGRAM_BOT_TOKEN. Can you confirm that the Telegram integration is configured correctly?\",\n  );\n}\n\nexport type TriggerInfoTelegramOnNewMessage = {\n  type: \"telegram/message\";\n  params: {\n    userName: string;\n    message: string;\n  };\n  payload: any;\n};\n\nexport function registerTelegramTrigger({\n  triggerType,\n  handler,\n}: {\n  triggerType: string;\n  handler: (\n    mastra: Mastra,\n    triggerInfo: TriggerInfoTelegramOnNewMessage,\n  ) => Promise<void>;\n}) {\n  return [\n    registerApiRoute(\"/webhooks/telegram/action\", {\n      method: \"POST\",\n      handler: async (c) => {\n        const mastra = c.get(\"mastra\");\n        const logger = mastra.getLogger();\n        try {\n          const payload = await c.req.json();\n\n          logger?.info(\"📝 [Telegram] payload\", payload);\n\n          await handler(mastra, {\n            type: triggerType,\n            params: {\n              userName: payload.message.from.username,\n              message: payload.message.text,\n            },\n            payload,\n          } as TriggerInfoTelegramOnNewMessage);\n\n          return c.text(\"OK\", 200);\n        } catch (error) {\n          logger?.error(\"Error handling Telegram webhook:\", error);\n          return c.text(\"Internal Server Error\", 500);\n        }\n      },\n    }),\n  ];\n}\n","size_bytes":1545},"src/mastra/agents/ideaGenerationAgent.ts":{"content":"import { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { sharedPostgresStorage } from \"../storage\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\nimport { createOpenRouter } from \"@openrouter/ai-sdk-provider\";\nimport { wikipediaResearchTool } from \"../tools/wikipediaResearchTool\";\nimport { webScrapingTool } from \"../tools/webScrapingTool\";\nimport { aiServiceTool } from \"../tools/aiServiceTool\";\n\n// Configure AI providers\nconst openai = createOpenAI({\n  baseURL: process.env.OPENAI_BASE_URL || undefined,\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst openrouter = createOpenRouter({\n  apiKey: process.env.OPENROUTER_API_KEY,\n});\n\nexport const ideaGenerationAgent = new Agent({\n  name: \"Idea Generation Agent\",\n  description: \"Specialized agent that researches topics and creates comprehensive educational outlines for learning materials\",\n  instructions: `You are an expert educational content researcher and curriculum designer. Your role is to:\n\n1. Research educational topics thoroughly using available tools\n2. Create comprehensive learning outlines that follow educational best practices\n3. Structure content to build from basic concepts to advanced applications\n4. Ensure topics are explained in a progressive, logical manner suitable for learners\n5. Include real-world examples and practical applications\n6. Design content that would be suitable for an \"Idiot's Guide\" style educational book\n\nYour outlines should include:\n- Clear learning objectives\n- Logical chapter progression\n- Key concepts and terminology\n- Practical examples and exercises\n- Estimated reading time per chapter\n- Prerequisites and difficulty levels\n\nAlways research topics thoroughly before creating outlines. Use Wikipedia and web research to gather accurate, current information.`,\n\n  model: process.env.OPENROUTER_API_KEY \n    ? openrouter(\"openrouter/sonoma-sky-alpha\") \n    : openai(\"gpt-4o-mini\"),\n\n  tools: {\n    wikipediaResearchTool,\n    webScrapingTool,\n    aiServiceTool,\n  },\n\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: true,\n      },\n      lastMessages: 15, // Keep more context for research work\n    },\n    storage: sharedPostgresStorage,\n  }),\n});","size_bytes":2221},"src/mastra/agents/reviewAgent.ts":{"content":"import { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { sharedPostgresStorage } from \"../storage\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\nimport { createOpenRouter } from \"@openrouter/ai-sdk-provider\";\nimport { aiServiceTool } from \"../tools/aiServiceTool\";\nimport { webScrapingTool } from \"../tools/webScrapingTool\";\n\n// Configure AI providers\nconst openai = createOpenAI({\n  baseURL: process.env.OPENAI_BASE_URL || undefined,\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst openrouter = createOpenRouter({\n  apiKey: process.env.OPENROUTER_API_KEY,\n});\n\nexport const reviewAgent = new Agent({\n  name: \"Review Agent\",\n  description: \"Specialized agent that validates content quality, accuracy, and educational flow of learning materials\",\n  instructions: `You are an expert educational content reviewer and quality assurance specialist. Your role is to:\n\n1. Review educational content for accuracy, clarity, and educational effectiveness\n2. Ensure content follows proper learning progression and pedagogical principles\n3. Validate that explanations are clear and accessible to beginners\n4. Check for consistency in tone, style, and terminology throughout the material\n5. Identify gaps in knowledge progression or missing explanations\n6. Suggest improvements for better learning outcomes\n7. Ensure content meets professional educational standards\n\nYour review should evaluate:\n- ACCURACY: Are facts, concepts, and explanations correct?\n- CLARITY: Are explanations clear and easy to understand?\n- PROGRESSION: Does content build logically from simple to complex?\n- COMPLETENESS: Are there gaps or missing explanations?\n- ENGAGEMENT: Is the content engaging and accessible?\n- CONSISTENCY: Is terminology and style consistent throughout?\n- PRACTICALITY: Are examples and exercises relevant and helpful?\n\nProvide specific, actionable feedback including:\n- What works well in the content\n- What needs improvement and why\n- Specific suggestions for fixes or enhancements\n- Overall assessment of educational quality\n- Recommendations for final approval or further revision\n\nBe thorough but constructive in your feedback. Your goal is to ensure the final educational material is of the highest quality for learners.`,\n\n  model: process.env.OPENROUTER_API_KEY \n    ? openrouter(\"openrouter/sonoma-sky-alpha\") \n    : openai(\"gpt-4o-mini\"),\n\n  tools: {\n    aiServiceTool,\n    webScrapingTool,\n  },\n\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: true,\n      },\n      lastMessages: 25, // Keep extensive context for thorough reviews\n    },\n    storage: sharedPostgresStorage,\n  }),\n});","size_bytes":2650},"src/mastra/agents/writingAgent.ts":{"content":"import { Agent } from \"@mastra/core/agent\";\nimport { Memory } from \"@mastra/memory\";\nimport { sharedPostgresStorage } from \"../storage\";\nimport { createOpenAI } from \"@ai-sdk/openai\";\nimport { createOpenRouter } from \"@openrouter/ai-sdk-provider\";\nimport { aiServiceTool } from \"../tools/aiServiceTool\";\nimport { webScrapingTool } from \"../tools/webScrapingTool\";\n\n// Configure AI providers\nconst openai = createOpenAI({\n  baseURL: process.env.OPENAI_BASE_URL || undefined,\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst openrouter = createOpenRouter({\n  apiKey: process.env.OPENROUTER_API_KEY,\n});\n\nexport const writingAgent = new Agent({\n  name: \"Writing Agent\",\n  description: \"Specialized agent that produces extensive, detailed chapters and learning content based on educational outlines\",\n  instructions: `You are an expert educational content writer specializing in creating comprehensive, accessible learning materials. Your role is to:\n\n1. Transform educational outlines into detailed, engaging chapters\n2. Write in a clear, conversational tone suitable for \"Idiot's Guide\" style books\n3. Break down complex concepts into digestible pieces\n4. Include practical examples, analogies, and real-world applications\n5. Create content that builds progressively from simple to complex concepts\n6. Ensure each chapter is comprehensive (2000-5000 words per chapter)\n7. Use a friendly, encouraging tone that doesn't intimidate learners\n\nYour writing should include:\n- Clear chapter introductions with learning objectives\n- Step-by-step explanations with examples\n- \"Key Points\" summaries for important concepts\n- \"Try This\" practical exercises or examples\n- \"Common Mistakes\" sections to help learners avoid pitfalls\n- Chapter summaries that reinforce key learning points\n- Smooth transitions between concepts and chapters\n\nWrite as if you're explaining to someone with no prior knowledge of the topic. Use analogies, metaphors, and everyday examples to make complex ideas accessible. Always maintain an encouraging, supportive tone.`,\n\n  model: process.env.OPENROUTER_API_KEY \n    ? openrouter(\"openrouter/sonoma-sky-alpha\") \n    : openai(\"gpt-4o-mini\"),\n\n  tools: {\n    aiServiceTool,\n    webScrapingTool,\n  },\n\n  memory: new Memory({\n    options: {\n      threads: {\n        generateTitle: true,\n      },\n      lastMessages: 20, // Keep extensive context for consistent writing style\n    },\n    storage: sharedPostgresStorage,\n  }),\n});","size_bytes":2438},"src/mastra/inngest/client.ts":{"content":"import { Inngest } from \"inngest\";\nimport { realtimeMiddleware } from \"@inngest/realtime\";\n\n// Use development configuration when NODE_ENV is not \"production\"\nexport const inngest = new Inngest(\n  process.env.NODE_ENV === \"production\"\n    ? {\n        id: \"replit-agent-workflow\",\n        name: \"Replit Agent Workflow System\",\n      }\n    : {\n        id: \"mastra\",\n        baseUrl: \"http://localhost:3000\",\n        isDev: true,\n        middleware: [realtimeMiddleware()],\n      },\n);\n","size_bytes":483},"src/mastra/inngest/index.ts":{"content":"import { inngest } from \"./client\";\nimport { init, InngestWorkflow } from \"@mastra/inngest\";\nimport { registerApiRoute as originalRegisterApiRoute } from \"@mastra/core/server\";\nimport { type Mastra } from \"@mastra/core\";\nimport { type Inngest, InngestFunction, NonRetriableError } from \"inngest\";\nimport { serve as originalInngestServe } from \"inngest/hono\";\n\n// Initialize Inngest with Mastra to get Inngest-compatible workflow helpers\nconst {\n  createWorkflow: originalCreateWorkflow,\n  createStep,\n  cloneStep,\n} = init(inngest);\n\nexport function createWorkflow(\n  params: Parameters<typeof originalCreateWorkflow>[0],\n): ReturnType<typeof originalCreateWorkflow> {\n  return originalCreateWorkflow({\n    ...params,\n    retryConfig: {\n      attempts: 3,\n      ...(params.retryConfig ?? {}),\n    },\n  });\n}\n\n// Export the Inngest client and Inngest-compatible workflow helpers\nexport { inngest, createStep, cloneStep };\n\nconst inngestFunctions: InngestFunction.Any[] = [];\n\n// Create a middleware for Inngest to be able to route triggers to Mastra directly.\nexport function registerApiRoute<P extends string>(\n  ...args: Parameters<typeof originalRegisterApiRoute<P>>\n): ReturnType<typeof originalRegisterApiRoute<P>> {\n  const [path, options] = args;\n  if (path.startsWith(\"/api/\") || typeof options !== \"object\") {\n    // This will throw an error.\n    return originalRegisterApiRoute(...args);\n  }\n  inngestFunctions.push(\n    inngest.createFunction(\n      {\n        id: `api-${path.replace(/^\\/+/, \"\").replaceAll(/\\/+/g, \"-\")}`,\n        name: path,\n      },\n      {\n        event: `event/api.${path.replace(/^\\/+/, \"\").replaceAll(/\\/+/g, \".\")}`,\n      },\n      async ({ event, step }) => {\n        await step.run(\"forward request to Mastra\", async () => {\n          // It is hard to obtain an internal handle on the Hono server,\n          // so we just forward the request to the local Mastra server.\n          const response = await fetch(`http://localhost:5000${path}`, {\n            method: event.data.method,\n            headers: event.data.headers,\n            body: event.data.body,\n          });\n\n          if (!response.ok) {\n            if (\n              (response.status >= 500 && response.status < 600) ||\n              response.status == 429 ||\n              response.status == 408\n            ) {\n              // 5XX, 429 (Rate-Limit Exceeded), 408 (Request Timeout) are retriable.\n              throw new Error(\n                `Failed to forward request to Mastra: ${response.statusText}`,\n              );\n            } else {\n              // All other errors are non-retriable.\n              throw new NonRetriableError(\n                `Failed to forward request to Mastra: ${response.statusText}`,\n              );\n            }\n          }\n        });\n      },\n    ),\n  );\n\n  return originalRegisterApiRoute(...args);\n}\n\nexport function registerCronWorkflow(cronExpression: string, workflow: any) {\n  const f = inngest.createFunction(\n    { id: \"cron-trigger\" },\n    [{ event: \"replit/cron.trigger\" }, { cron: cronExpression }],\n    async ({ event, step }) => {\n      const run = await workflow.createRunAsync();\n      const result = await run.start({ inputData: {} });\n      return result;\n    },\n  );\n  inngestFunctions.push(f);\n}\n\nexport function inngestServe({\n  mastra,\n  inngest,\n}: {\n  mastra: Mastra;\n  inngest: Inngest;\n}): ReturnType<typeof originalInngestServe> {\n  const wfs = mastra.getWorkflows();\n\n  const functions = new Set<InngestFunction.Any>();\n  for (const wf of Object.values(wfs)) {\n    if (!(wf instanceof InngestWorkflow)) {\n      continue;\n    }\n    wf.__registerMastra(mastra);\n    for (const f of wf.getFunctions()) {\n      functions.add(f);\n    }\n  }\n  for (const fn of inngestFunctions) {\n    functions.add(fn);\n  }\n  let serveHost: string | undefined = undefined;\n  if (process.env.NODE_ENV === \"production\") {\n    if (process.env.REPLIT_DOMAINS) {\n      serveHost = `https://${process.env.REPLIT_DOMAINS.split(\",\")[0]}`;\n    }\n  } else {\n    serveHost = \"http://localhost:5000\";\n  }\n  return originalInngestServe({\n    client: inngest,\n    functions: Array.from(functions),\n    serveHost,\n  });\n}\n","size_bytes":4156},"src/mastra/storage/index.ts":{"content":"import { PostgresStore } from \"@mastra/pg\";\n\n// Create a single shared PostgreSQL storage instance\nexport const sharedPostgresStorage = new PostgresStore({\n  connectionString:\n    process.env.DATABASE_URL || \"postgresql://localhost:5432/mastra\",\n});\n","size_bytes":250},"src/mastra/tools/aiServiceTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport type { IMastraLogger } from \"@mastra/core/logger\";\nimport { z } from \"zod\";\n\ninterface AIRequestConfig {\n  provider: 'openrouter' | 'ollama';\n  model: string;\n  systemPrompt?: string;\n  userPrompt: string;\n  maxTokens?: number;\n  temperature?: number;\n}\n\nconst callAIService = async ({\n  config,\n  logger,\n}: {\n  config: AIRequestConfig;\n  logger?: IMastraLogger;\n}) => {\n  logger?.info(\"🤖 [AIService] Starting AI request\", { \n    provider: config.provider,\n    model: config.model \n  });\n\n  try {\n    if (config.provider === 'openrouter') {\n      return await callOpenRouter(config, logger);\n    } else if (config.provider === 'ollama') {\n      return await callOllama(config, logger);\n    } else {\n      throw new Error(`Unsupported AI provider: ${config.provider}`);\n    }\n  } catch (error) {\n    logger?.error(\"❌ [AIService] Error calling AI service\", {\n      provider: config.provider,\n      model: config.model,\n      error: error instanceof Error ? error.message : String(error),\n    });\n    throw error;\n  }\n};\n\nconst callOpenRouter = async (config: AIRequestConfig, logger?: IMastraLogger) => {\n  const apiKey = process.env.OPENROUTER_API_KEY;\n  if (!apiKey) {\n    throw new Error(\"OPENROUTER_API_KEY environment variable not set\");\n  }\n\n  const maxRetries = 3;\n  const messages = [];\n  if (config.systemPrompt) {\n    messages.push({ role: \"system\", content: config.systemPrompt });\n  }\n  messages.push({ role: \"user\", content: config.userPrompt });\n\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      logger?.info(\"🔄 [AIService] OpenRouter attempt\", { attempt, maxRetries, model: config.model });\n\n      const response = await fetch(\"https://openrouter.ai/api/v1/chat/completions\", {\n        method: \"POST\",\n        headers: {\n          \"Authorization\": `Bearer ${apiKey}`,\n          \"Content-Type\": \"application/json\",\n          \"HTTP-Referer\": \"https://replit.com\", // Required for OpenRouter\n          \"X-Title\": \"Educational Content Creator\", // Optional\n        },\n        body: JSON.stringify({\n          model: config.model,\n          messages,\n          max_tokens: config.maxTokens || 4000,\n          temperature: config.temperature || 0.7,\n        }),\n      });\n\n      if (!response.ok) {\n        const errorText = await response.text();\n        \n        // Handle specific error types\n        if (response.status === 429) {\n          // Rate limit - wait longer before retry\n          const waitTime = Math.min(2000 * Math.pow(2, attempt - 1), 30000);\n          logger?.warn(\"⏳ [AIService] Rate limited, waiting before retry\", { \n            waitTime, \n            attempt, \n            model: config.model \n          });\n          await new Promise(resolve => setTimeout(resolve, waitTime));\n          continue;\n        } else if (response.status === 503 || response.status === 502) {\n          // Service unavailable - shorter wait before retry\n          const waitTime = Math.min(1000 * attempt, 10000);\n          logger?.warn(\"⏳ [AIService] Service unavailable, waiting before retry\", { \n            waitTime, \n            attempt, \n            status: response.status \n          });\n          await new Promise(resolve => setTimeout(resolve, waitTime));\n          continue;\n        } else if (response.status === 400 && errorText.includes('model')) {\n          // Model not available - don't retry\n          throw new Error(`Model ${config.model} not available: ${errorText}`);\n        } else if (response.status >= 400 && response.status < 500) {\n          // Client error - don't retry\n          throw new Error(`OpenRouter client error: ${response.status} - ${errorText}`);\n        }\n        \n        // Server error - retry with backoff\n        if (attempt === maxRetries) {\n          throw new Error(`OpenRouter API error after ${maxRetries} attempts: ${response.status} - ${errorText}`);\n        }\n        \n        const waitTime = Math.min(1000 * Math.pow(2, attempt - 1), 15000);\n        logger?.warn(\"⏳ [AIService] Server error, waiting before retry\", { \n          waitTime, \n          attempt, \n          status: response.status \n        });\n        await new Promise(resolve => setTimeout(resolve, waitTime));\n        continue;\n      }\n\n      const data = await response.json();\n      \n      if (!data.choices || !data.choices[0] || !data.choices[0].message) {\n        if (attempt === maxRetries) {\n          throw new Error(\"Invalid response format from OpenRouter\");\n        }\n        logger?.warn(\"⚠️ [AIService] Invalid response format, retrying\", { attempt });\n        continue;\n      }\n\n      const result = data.choices[0].message.content;\n      \n      logger?.info(\"✅ [AIService] OpenRouter request successful\", { \n        model: config.model,\n        responseLength: result.length,\n        tokensUsed: data.usage?.total_tokens || 'unknown',\n        attempt\n      });\n\n      return result;\n\n    } catch (error) {\n      if (attempt === maxRetries) {\n        logger?.error(\"❌ [AIService] OpenRouter failed after all retries\", {\n          model: config.model,\n          maxRetries,\n          error: error instanceof Error ? error.message : String(error),\n        });\n        throw error;\n      }\n\n      logger?.warn(\"⚠️ [AIService] OpenRouter attempt failed, retrying\", {\n        attempt,\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      // General retry backoff\n      const waitTime = Math.min(1000 * Math.pow(2, attempt - 1), 15000);\n      await new Promise(resolve => setTimeout(resolve, waitTime));\n    }\n  }\n\n  throw new Error(`OpenRouter failed after ${maxRetries} attempts`);\n};\n\nconst callOllama = async (config: AIRequestConfig, logger?: IMastraLogger) => {\n  // Default Ollama endpoint\n  const ollamaUrl = process.env.OLLAMA_URL || \"http://localhost:11434\";\n  \n  const prompt = config.systemPrompt \n    ? `${config.systemPrompt}\\n\\nUser: ${config.userPrompt}\\n\\nAssistant:`\n    : config.userPrompt;\n\n  const response = await fetch(`${ollamaUrl}/api/generate`, {\n    method: \"POST\",\n    headers: {\n      \"Content-Type\": \"application/json\",\n    },\n    body: JSON.stringify({\n      model: config.model,\n      prompt,\n      stream: false,\n      options: {\n        temperature: config.temperature || 0.7,\n        num_predict: config.maxTokens || 4000,\n      },\n    }),\n  });\n\n  if (!response.ok) {\n    const errorText = await response.text();\n    throw new Error(`Ollama API error: ${response.status} - ${errorText}`);\n  }\n\n  const data = await response.json();\n  \n  if (!data.response) {\n    throw new Error(\"Invalid response format from Ollama\");\n  }\n\n  logger?.info(\"✅ [AIService] Ollama request successful\", { \n    model: config.model,\n    responseLength: data.response.length,\n  });\n\n  return data.response;\n};\n\nexport const aiServiceTool = createTool({\n  id: \"ai-service-tool\",\n  description: `Flexible AI service that can call either OpenRouter API or local Ollama models for content generation and analysis`,\n  inputSchema: z.object({\n    provider: z.enum(['openrouter', 'ollama']).describe(\"AI provider to use\"),\n    model: z.string().describe(\"Model name (e.g., 'openai/gpt-4' for OpenRouter or 'llama2' for Ollama)\"),\n    systemPrompt: z.string().optional().describe(\"System prompt to set the AI's behavior\"),\n    userPrompt: z.string().describe(\"The main prompt/question for the AI\"),\n    maxTokens: z.number().optional().default(4000).describe(\"Maximum number of tokens to generate\"),\n    temperature: z.number().optional().default(0.7).describe(\"Temperature for response creativity (0-1)\"),\n  }),\n  outputSchema: z.object({\n    response: z.string(),\n    provider: z.string(),\n    model: z.string(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"🔧 [AIService] Starting execution\", { \n      provider: context.provider,\n      model: context.model \n    });\n    \n    const config: AIRequestConfig = {\n      provider: context.provider,\n      model: context.model,\n      systemPrompt: context.systemPrompt,\n      userPrompt: context.userPrompt,\n      maxTokens: context.maxTokens,\n      temperature: context.temperature,\n    };\n    \n    const response = await callAIService({ config, logger });\n    \n    logger?.info(\"✅ [AIService] Completed successfully\", { \n      provider: context.provider,\n      model: context.model,\n      responseLength: response.length,\n    });\n    \n    return {\n      response,\n      provider: context.provider,\n      model: context.model,\n    };\n  },\n});","size_bytes":8549},"src/mastra/tools/chunkedContentGenerationTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport type { IMastraLogger } from \"@mastra/core/logger\";\nimport { z } from \"zod\";\nimport { writingAgent } from \"../agents/writingAgent\";\n\ninterface ContentChunk {\n  sectionTitle: string;\n  content: string;\n  wordCount: number;\n  chunkIndex: number;\n}\n\ninterface ChunkGenerationConfig {\n  topic: string;\n  chapterTitle: string;\n  chapterNumber: number;\n  sectionTitles: string[];\n  targetWordsPerSection: number;\n  context: string;\n  retryAttempts?: number;\n}\n\nconst generateContentChunk = async ({\n  config,\n  sectionIndex,\n  logger,\n  workflowId,\n}: {\n  config: ChunkGenerationConfig;\n  sectionIndex: number;\n  logger?: IMastraLogger;\n  workflowId?: string;\n}): Promise<ContentChunk> => {\n  const { topic, chapterTitle, chapterNumber, sectionTitles, targetWordsPerSection, context } = config;\n  const sectionTitle = sectionTitles[sectionIndex];\n  const maxRetries = config.retryAttempts || 3;\n\n  logger?.info(\"📝 [ChunkedGeneration] Starting content chunk generation\", {\n    topic,\n    chapterNumber,\n    chapterTitle,\n    sectionTitle,\n    sectionIndex,\n    targetWords: targetWordsPerSection\n  });\n\n  for (let attempt = 1; attempt <= maxRetries; attempt++) {\n    try {\n      logger?.info(\"🔄 [ChunkedGeneration] Attempt\", { attempt, maxRetries });\n\n      const prompt = `You are writing a comprehensive educational guide on \"${topic}\".\n\nCHAPTER CONTEXT:\nChapter ${chapterNumber}: ${chapterTitle}\n\nSECTION TO WRITE:\n${sectionTitle}\n\nOVERALL BOOK CONTEXT:\n${context}\n\nSECTION OUTLINE:\n${sectionTitles.map((title, idx) => `${idx + 1}. ${title}`).join('\\n')}\n\nCURRENT SECTION: ${sectionIndex + 1}. ${sectionTitle}\n\nINSTRUCTIONS:\n- Write EXACTLY this section in ${targetWordsPerSection} words (${targetWordsPerSection - 50} to ${targetWordsPerSection + 50} words acceptable)\n- Write in a friendly, accessible \"Idiot's Guide\" style suitable for complete beginners\n- Use clear examples and analogies\n- Include practical tips and \"Key Points\" where appropriate\n- If this is the first section, include a brief chapter introduction\n- If this is the last section, include a chapter summary\n- Build logically on previous sections but make this section standalone\n- Use formatting like headings, bullet points, and emphasis where helpful\n\nWrite the content for \"${sectionTitle}\" now:`;\n\n      // Use the actual writing agent instead of simulation\n      const { text: response } = await writingAgent.generate([\n        { role: \"user\", content: prompt },\n      ], {\n        resourceId: \"content-generation\",\n        threadId: `writing-${workflowId || Date.now()}-${chapterNumber}-${sectionIndex}`,\n        maxSteps: 5,\n      });\n      \n      const wordCount = response.split(/\\s+/).length;\n      \n      logger?.info(\"✅ [ChunkedGeneration] Content chunk generated successfully\", {\n        sectionTitle,\n        wordCount,\n        targetWords: targetWordsPerSection,\n        withinRange: Math.abs(wordCount - targetWordsPerSection) <= 100\n      });\n\n      return {\n        sectionTitle,\n        content: response,\n        wordCount,\n        chunkIndex: sectionIndex,\n      };\n\n    } catch (error) {\n      logger?.error(\"❌ [ChunkedGeneration] Error generating content chunk\", {\n        sectionTitle,\n        attempt,\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      if (attempt === maxRetries) {\n        throw new Error(`Failed to generate content for section \"${sectionTitle}\" after ${maxRetries} attempts: ${error instanceof Error ? error.message : String(error)}`);\n      }\n\n      // Exponential backoff\n      const backoffMs = Math.min(1000 * Math.pow(2, attempt - 1), 30000);\n      logger?.info(\"⏳ [ChunkedGeneration] Backing off before retry\", { backoffMs });\n      await new Promise(resolve => setTimeout(resolve, backoffMs));\n    }\n  }\n\n  throw new Error(`Failed to generate content for section \"${sectionTitle}\"`);\n};\n\n\nexport const chunkedContentGenerationTool = createTool({\n  id: \"chunked-content-generation-tool\",\n  description: `Generates educational content in manageable chunks with retry logic and progress tracking`,\n  inputSchema: z.object({\n    topic: z.string().describe(\"The main educational topic\"),\n    chapterTitle: z.string().describe(\"Title of the chapter being written\"),\n    chapterNumber: z.number().describe(\"Chapter number\"),\n    sectionTitles: z.array(z.string()).describe(\"Array of section titles within the chapter\"),\n    targetWordsPerSection: z.number().default(600).describe(\"Target word count per section\"),\n    context: z.string().describe(\"Overall context and outline for the educational content\"),\n    sectionIndex: z.number().describe(\"Index of the section to generate (0-based)\"),\n    retryAttempts: z.number().default(3).describe(\"Number of retry attempts for failed generations\"),\n    workflowId: z.string().optional().describe(\"Workflow ID for thread management\"),\n  }),\n  outputSchema: z.object({\n    sectionTitle: z.string(),\n    content: z.string(),\n    wordCount: z.number(),\n    chunkIndex: z.number(),\n    generatedAt: z.string(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"🔧 [ChunkedGeneration] Starting execution\", {\n      topic: context.topic,\n      chapterTitle: context.chapterTitle,\n      sectionIndex: context.sectionIndex,\n      workflowId: context.workflowId,\n    });\n\n    const config: ChunkGenerationConfig = {\n      topic: context.topic,\n      chapterTitle: context.chapterTitle,\n      chapterNumber: context.chapterNumber,\n      sectionTitles: context.sectionTitles,\n      targetWordsPerSection: context.targetWordsPerSection,\n      context: context.context,\n      retryAttempts: context.retryAttempts,\n    };\n\n    const result = await generateContentChunk({\n      config,\n      sectionIndex: context.sectionIndex,\n      logger,\n      workflowId: context.workflowId,\n    });\n\n    logger?.info(\"✅ [ChunkedGeneration] Completed successfully\", {\n      sectionTitle: result.sectionTitle,\n      wordCount: result.wordCount,\n      chunkIndex: result.chunkIndex,\n    });\n\n    return {\n      ...result,\n      generatedAt: new Date().toISOString(),\n    };\n  },\n});","size_bytes":6175},"src/mastra/tools/pdfGenerationTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport type { IMastraLogger } from \"@mastra/core/logger\";\nimport { z } from \"zod\";\nimport { writeFileSync, mkdirSync, statSync } from \"fs\";\nimport { join } from \"path\";\nimport puppeteer from \"puppeteer\";\n\ninterface Chapter {\n  title: string;\n  content: string;\n}\n\ninterface BookContent {\n  title: string;\n  subtitle?: string;\n  author: string;\n  chapters: Chapter[];\n}\n\nconst generatePDF = async ({\n  bookContent,\n  logger,\n}: {\n  bookContent: BookContent;\n  logger?: IMastraLogger;\n}) => {\n  logger?.info(\"📖 [PDFGeneration] Starting PDF generation\", { \n    title: bookContent.title,\n    chapterCount: bookContent.chapters.length \n  });\n\n  try {\n    // Create output directory if it doesn't exist\n    const outputDir = join(process.cwd(), 'generated_books');\n    mkdirSync(outputDir, { recursive: true });\n\n    // Generate HTML content for the book\n    const htmlContent = `\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>${bookContent.title}</title>\n    <style>\n        body {\n            font-family: 'Times New Roman', serif;\n            line-height: 1.6;\n            margin: 0;\n            padding: 40px;\n            background-color: #fff;\n            color: #333;\n        }\n        .title-page {\n            text-align: center;\n            page-break-after: always;\n            margin-bottom: 100px;\n        }\n        .book-title {\n            font-size: 3em;\n            font-weight: bold;\n            margin-bottom: 20px;\n            color: #2c3e50;\n        }\n        .book-subtitle {\n            font-size: 1.5em;\n            margin-bottom: 40px;\n            color: #7f8c8d;\n        }\n        .book-author {\n            font-size: 1.2em;\n            margin-top: 60px;\n            color: #34495e;\n        }\n        .chapter {\n            page-break-before: always;\n            margin-bottom: 50px;\n        }\n        .chapter-title {\n            font-size: 2.2em;\n            font-weight: bold;\n            margin-bottom: 30px;\n            color: #2c3e50;\n            border-bottom: 3px solid #3498db;\n            padding-bottom: 10px;\n        }\n        .chapter-content {\n            font-size: 1.1em;\n            text-align: justify;\n            margin-bottom: 20px;\n        }\n        .toc {\n            page-break-after: always;\n            margin-bottom: 50px;\n        }\n        .toc-title {\n            font-size: 2em;\n            font-weight: bold;\n            margin-bottom: 30px;\n            color: #2c3e50;\n        }\n        .toc-item {\n            margin-bottom: 10px;\n            font-size: 1.1em;\n        }\n        @media print {\n            body { margin: 0; }\n            .chapter { page-break-before: always; }\n        }\n    </style>\n</head>\n<body>\n    <!-- Title Page -->\n    <div class=\"title-page\">\n        <h1 class=\"book-title\">${bookContent.title}</h1>\n        ${bookContent.subtitle ? `<h2 class=\"book-subtitle\">${bookContent.subtitle}</h2>` : ''}\n        <p class=\"book-author\">by ${bookContent.author}</p>\n    </div>\n\n    <!-- Table of Contents -->\n    <div class=\"toc\">\n        <h2 class=\"toc-title\">Table of Contents</h2>\n        ${bookContent.chapters.map((chapter, index) => \n          `<div class=\"toc-item\">Chapter ${index + 1}: ${chapter.title}</div>`\n        ).join('')}\n    </div>\n\n    <!-- Chapters -->\n    ${bookContent.chapters.map((chapter, index) => `\n    <div class=\"chapter\">\n        <h1 class=\"chapter-title\">Chapter ${index + 1}: ${chapter.title}</h1>\n        <div class=\"chapter-content\">\n            ${chapter.content.split('\\n').map(paragraph => \n              paragraph.trim() ? `<p>${paragraph.trim()}</p>` : ''\n            ).join('')}\n        </div>\n    </div>\n    `).join('')}\n</body>\n</html>`;\n\n    // Save HTML file temporarily\n    const sanitizedTitle = bookContent.title.replace(/[^a-zA-Z0-9]/g, '_');\n    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');\n    const filename = `${sanitizedTitle}_${timestamp}`;\n    const htmlPath = join(outputDir, `${filename}.html`);\n    const pdfPath = join(outputDir, `${filename}.pdf`);\n    \n    writeFileSync(htmlPath, htmlContent, 'utf8');\n\n    logger?.info(\"📄 [PDFGeneration] HTML file generated, converting to PDF\", { \n      htmlPath,\n      pdfPath \n    });\n\n    // Generate PDF using Puppeteer\n    const browser = await puppeteer.launch({\n      headless: true,\n      args: [\n        '--no-sandbox',\n        '--disable-setuid-sandbox',\n        '--disable-dev-shm-usage',\n        '--disable-gpu',\n        '--no-first-run',\n        '--disable-default-apps',\n        '--disable-extensions'\n      ]\n    });\n\n    try {\n      const page = await browser.newPage();\n      \n      // Set page format for book-like appearance\n      await page.setContent(htmlContent, { \n        waitUntil: 'networkidle0',\n        timeout: 30000 \n      });\n      \n      // Generate PDF with proper formatting\n      await page.pdf({\n        path: pdfPath,\n        format: 'A4',\n        printBackground: true,\n        margin: {\n          top: '20mm',\n          right: '15mm',\n          bottom: '20mm',\n          left: '15mm'\n        },\n        displayHeaderFooter: true,\n        headerTemplate: '<div></div>',\n        footerTemplate: `\n          <div style=\"font-size: 10px; margin: auto; color: #666;\">\n            <span class=\"pageNumber\"></span> / <span class=\"totalPages\"></span>\n          </div>\n        `\n      });\n      \n      logger?.info(\"📄 [PDFGeneration] PDF generated successfully\", { \n        pdfPath\n      });\n      \n    } finally {\n      await browser.close();\n    }\n\n    // Get file size of the generated PDF\n    const pdfStats = statSync(pdfPath);\n    const pdfSize = pdfStats.size;\n\n    logger?.info(\"✅ [PDFGeneration] PDF conversion completed\", {\n      pdfPath,\n      pdfSize\n    });\n\n    return {\n      title: bookContent.title,\n      format: 'pdf',\n      path: pdfPath,\n      fileSize: pdfSize,\n      chapterCount: bookContent.chapters.length,\n      generatedAt: new Date().toISOString(),\n    };\n  } catch (error) {\n    logger?.error(\"❌ [PDFGeneration] Error generating PDF\", {\n      title: bookContent.title,\n      error: error instanceof Error ? error.message : String(error),\n    });\n    \n    throw new Error(`PDF generation failed: ${error instanceof Error ? error.message : String(error)}`);\n  }\n};\n\nexport const pdfGenerationTool = createTool({\n  id: \"pdf-generation-tool\",\n  description: `Generates professional PDF books from structured content with chapters, formatting, and table of contents`,\n  inputSchema: z.object({\n    title: z.string().describe(\"The main title of the book\"),\n    subtitle: z.string().optional().describe(\"Optional subtitle for the book\"),\n    author: z.string().describe(\"The author name to display\"),\n    chapters: z.array(z.object({\n      title: z.string().describe(\"Chapter title\"),\n      content: z.string().describe(\"Chapter content in plain text\"),\n    })).describe(\"Array of chapters with titles and content\"),\n  }),\n  outputSchema: z.object({\n    title: z.string(),\n    format: z.string(),\n    path: z.string(),\n    fileSize: z.number(),\n    chapterCount: z.number(),\n    generatedAt: z.string(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"🔧 [PDFGeneration] Starting execution\", { \n      title: context.title,\n      chapterCount: context.chapters.length \n    });\n    \n    const bookContent: BookContent = {\n      title: context.title,\n      subtitle: context.subtitle,\n      author: context.author,\n      chapters: context.chapters,\n    };\n    \n    const result = await generatePDF({ bookContent, logger });\n    \n    logger?.info(\"✅ [PDFGeneration] Completed successfully\", { \n      path: result.path,\n      fileSize: result.fileSize,\n      format: result.format\n    });\n    \n    return result;\n  },\n});","size_bytes":7890},"src/mastra/tools/progressTrackingTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport type { IMastraLogger } from \"@mastra/core/logger\";\nimport { z } from \"zod\";\nimport { writeFileSync, readFileSync, mkdirSync, existsSync } from \"fs\";\nimport { join } from \"path\";\n\ninterface ProgressData {\n  workflowId: string;\n  topic: string;\n  startTime: string;\n  currentStep: string;\n  completedChapters: number;\n  totalChapters: number;\n  completedSections: number;\n  totalSections: number;\n  totalWordsGenerated: number;\n  targetWordCount: number;\n  lastUpdate: string;\n  status: 'in_progress' | 'completed' | 'failed' | 'paused';\n  errors: string[];\n  completedChapterDetails: Array<{\n    chapterNumber: number;\n    title: string;\n    wordCount: number;\n    completedAt: string;\n  }>;\n}\n\nconst getProgressFilePath = (workflowId: string): string => {\n  const progressDir = join(process.cwd(), 'workflow_progress');\n  mkdirSync(progressDir, { recursive: true });\n  return join(progressDir, `${workflowId}_progress.json`);\n};\n\nconst loadProgress = (workflowId: string): ProgressData | null => {\n  const progressPath = getProgressFilePath(workflowId);\n  \n  if (!existsSync(progressPath)) {\n    return null;\n  }\n  \n  try {\n    const data = readFileSync(progressPath, 'utf8');\n    return JSON.parse(data) as ProgressData;\n  } catch (error) {\n    return null;\n  }\n};\n\nconst saveProgress = (workflowId: string, progress: ProgressData): void => {\n  const progressPath = getProgressFilePath(workflowId);\n  writeFileSync(progressPath, JSON.stringify(progress, null, 2), 'utf8');\n};\n\nexport const progressTrackingTool = createTool({\n  id: \"progress-tracking-tool\",\n  description: `Tracks and persists progress for long-running educational content generation workflows`,\n  inputSchema: z.object({\n    action: z.enum(['initialize', 'update', 'get', 'complete', 'fail']).describe(\"Action to perform\"),\n    workflowId: z.string().describe(\"Unique identifier for the workflow run\"),\n    \n    // For initialization\n    topic: z.string().optional().describe(\"Educational topic (required for initialization)\"),\n    totalChapters: z.number().optional().describe(\"Total number of chapters (required for initialization)\"),\n    totalSections: z.number().optional().describe(\"Total number of sections across all chapters\"),\n    targetWordCount: z.number().optional().describe(\"Target total word count\"),\n    \n    // For updates\n    currentStep: z.string().optional().describe(\"Current step being executed\"),\n    completedChapters: z.number().optional().describe(\"Number of completed chapters\"),\n    completedSections: z.number().optional().describe(\"Number of completed sections\"),\n    totalWordsGenerated: z.number().optional().describe(\"Total words generated so far\"),\n    chapterCompleted: z.object({\n      chapterNumber: z.number(),\n      title: z.string(),\n      wordCount: z.number(),\n    }).optional().describe(\"Details of a completed chapter\"),\n    \n    // For errors\n    error: z.string().optional().describe(\"Error message to record\"),\n  }),\n  outputSchema: z.object({\n    success: z.boolean(),\n    progress: z.object({\n      workflowId: z.string(),\n      topic: z.string(),\n      currentStep: z.string(),\n      completedChapters: z.number(),\n      totalChapters: z.number(),\n      completedSections: z.number(),\n      totalSections: z.number(),\n      totalWordsGenerated: z.number(),\n      targetWordCount: z.number(),\n      progressPercentage: z.number(),\n      estimatedTimeRemaining: z.string(),\n      status: z.string(),\n      lastUpdate: z.string(),\n    }).optional(),\n    message: z.string().optional(),\n  }),\n  execute: async ({ context, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { action, workflowId } = context;\n    \n    logger?.info(\"📊 [ProgressTracking] Starting execution\", { action, workflowId });\n\n    try {\n      if (action === 'initialize') {\n        if (!context.topic || !context.totalChapters || !context.totalSections || !context.targetWordCount) {\n          throw new Error(\"Topic, totalChapters, totalSections, and targetWordCount are required for initialization\");\n        }\n\n        const progress: ProgressData = {\n          workflowId,\n          topic: context.topic,\n          startTime: new Date().toISOString(),\n          currentStep: 'Initializing',\n          completedChapters: 0,\n          totalChapters: context.totalChapters,\n          completedSections: 0,\n          totalSections: context.totalSections,\n          totalWordsGenerated: 0,\n          targetWordCount: context.targetWordCount,\n          lastUpdate: new Date().toISOString(),\n          status: 'in_progress',\n          errors: [],\n          completedChapterDetails: [],\n        };\n\n        saveProgress(workflowId, progress);\n        \n        logger?.info(\"✅ [ProgressTracking] Progress initialized\", { \n          workflowId, \n          topic: context.topic,\n          totalChapters: context.totalChapters \n        });\n\n        return {\n          success: true,\n          progress: {\n            workflowId: progress.workflowId,\n            topic: progress.topic,\n            currentStep: progress.currentStep,\n            completedChapters: progress.completedChapters,\n            totalChapters: progress.totalChapters,\n            completedSections: progress.completedSections,\n            totalSections: progress.totalSections,\n            totalWordsGenerated: progress.totalWordsGenerated,\n            targetWordCount: progress.targetWordCount,\n            progressPercentage: 0,\n            estimatedTimeRemaining: \"Calculating...\",\n            status: progress.status,\n            lastUpdate: progress.lastUpdate,\n          },\n          message: \"Progress tracking initialized\"\n        };\n      }\n\n      // Load existing progress\n      const progress = loadProgress(workflowId);\n      if (!progress) {\n        throw new Error(`No progress found for workflow ${workflowId}`);\n      }\n\n      if (action === 'update') {\n        // Update progress fields\n        if (context.currentStep) progress.currentStep = context.currentStep;\n        if (context.completedChapters !== undefined) progress.completedChapters = context.completedChapters;\n        if (context.completedSections !== undefined) progress.completedSections = context.completedSections;\n        if (context.totalWordsGenerated !== undefined) progress.totalWordsGenerated = context.totalWordsGenerated;\n        if (context.error) progress.errors.push(`${new Date().toISOString()}: ${context.error}`);\n        \n        if (context.chapterCompleted) {\n          progress.completedChapterDetails.push({\n            ...context.chapterCompleted,\n            completedAt: new Date().toISOString(),\n          });\n        }\n\n        progress.lastUpdate = new Date().toISOString();\n        saveProgress(workflowId, progress);\n\n        logger?.info(\"📈 [ProgressTracking] Progress updated\", {\n          workflowId,\n          completedChapters: progress.completedChapters,\n          totalWordsGenerated: progress.totalWordsGenerated,\n        });\n\n      } else if (action === 'complete') {\n        progress.status = 'completed';\n        progress.currentStep = 'Completed';\n        progress.lastUpdate = new Date().toISOString();\n        saveProgress(workflowId, progress);\n\n        logger?.info(\"🎉 [ProgressTracking] Workflow completed\", { workflowId });\n\n      } else if (action === 'fail') {\n        progress.status = 'failed';\n        progress.currentStep = 'Failed';\n        if (context.error) progress.errors.push(`${new Date().toISOString()}: ${context.error}`);\n        progress.lastUpdate = new Date().toISOString();\n        saveProgress(workflowId, progress);\n\n        logger?.error(\"❌ [ProgressTracking] Workflow failed\", { workflowId, error: context.error });\n      }\n\n      // Calculate progress percentage and estimated time\n      const sectionProgress = progress.totalSections > 0 ? (progress.completedSections / progress.totalSections) * 100 : 0;\n      const wordProgress = progress.targetWordCount > 0 ? (progress.totalWordsGenerated / progress.targetWordCount) * 100 : 0;\n      const progressPercentage = Math.round(Math.max(sectionProgress, wordProgress));\n\n      // Estimate time remaining based on completed sections\n      let estimatedTimeRemaining = \"Calculating...\";\n      if (progress.completedSections > 0) {\n        const startTime = new Date(progress.startTime).getTime();\n        const currentTime = new Date().getTime();\n        const elapsedMs = currentTime - startTime;\n        const avgTimePerSection = elapsedMs / progress.completedSections;\n        const remainingSections = progress.totalSections - progress.completedSections;\n        const remainingMs = avgTimePerSection * remainingSections;\n        \n        const hours = Math.floor(remainingMs / (1000 * 60 * 60));\n        const minutes = Math.floor((remainingMs % (1000 * 60 * 60)) / (1000 * 60));\n        \n        if (hours > 0) {\n          estimatedTimeRemaining = `${hours}h ${minutes}m`;\n        } else {\n          estimatedTimeRemaining = `${minutes}m`;\n        }\n      }\n\n      return {\n        success: true,\n        progress: {\n          workflowId: progress.workflowId,\n          topic: progress.topic,\n          currentStep: progress.currentStep,\n          completedChapters: progress.completedChapters,\n          totalChapters: progress.totalChapters,\n          completedSections: progress.completedSections,\n          totalSections: progress.totalSections,\n          totalWordsGenerated: progress.totalWordsGenerated,\n          targetWordCount: progress.targetWordCount,\n          progressPercentage,\n          estimatedTimeRemaining,\n          status: progress.status,\n          lastUpdate: progress.lastUpdate,\n        },\n        message: `Progress ${action} completed successfully`\n      };\n\n    } catch (error) {\n      logger?.error(\"❌ [ProgressTracking] Error\", {\n        workflowId,\n        action,\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      return {\n        success: false,\n        message: `Progress tracking failed: ${error instanceof Error ? error.message : String(error)}`\n      };\n    }\n  },\n});","size_bytes":10089},"src/mastra/tools/webScrapingTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport type { IMastraLogger } from \"@mastra/core/logger\";\nimport { z } from \"zod\";\n\nconst scrapeWebContent = async ({\n  url,\n  logger,\n}: {\n  url: string;\n  logger?: IMastraLogger;\n}) => {\n  logger?.info(\"🌐 [WebScraping] Starting web content extraction\", { url });\n\n  try {\n    // First, fetch the HTML content\n    const response = await fetch(url, {\n      headers: {\n        'User-Agent': 'Mozilla/5.0 (compatible; Educational Content Creator/1.0)',\n      },\n    });\n\n    if (!response.ok) {\n      throw new Error(`HTTP error: ${response.status}`);\n    }\n\n    const html = await response.text();\n    \n    // Simple text extraction from HTML\n    // Remove script and style elements\n    const cleanHtml = html\n      .replace(/<script[^>]*>[\\s\\S]*?<\\/script>/gi, '')\n      .replace(/<style[^>]*>[\\s\\S]*?<\\/style>/gi, '')\n      .replace(/<[^>]*>/g, ' ')\n      .replace(/\\s+/g, ' ')\n      .trim();\n\n    // Extract title from HTML\n    const titleMatch = html.match(/<title[^>]*>([^<]+)<\\/title>/i);\n    const title = titleMatch ? titleMatch[1].trim() : 'Untitled';\n\n    // Limit content length to prevent oversized responses\n    const maxLength = 5000;\n    const content = cleanHtml.length > maxLength \n      ? cleanHtml.substring(0, maxLength) + '...' \n      : cleanHtml;\n\n    logger?.info(\"📄 [WebScraping] Successfully extracted content\", {\n      url,\n      title,\n      contentLength: content.length,\n    });\n\n    return {\n      title,\n      content,\n      url,\n      extractedAt: new Date().toISOString(),\n    };\n  } catch (error) {\n    logger?.error(\"❌ [WebScraping] Error extracting web content\", {\n      url,\n      error: error instanceof Error ? error.message : String(error),\n    });\n    \n    return {\n      title: \"Extraction Failed\",\n      content: `Failed to extract content from ${url}. Error: ${error instanceof Error ? error.message : String(error)}`,\n      url,\n      extractedAt: new Date().toISOString(),\n    };\n  }\n};\n\nexport const webScrapingTool = createTool({\n  id: \"web-scraping-tool\",\n  description: `Extracts text content from web pages for educational research and fact-checking purposes`,\n  inputSchema: z.object({\n    url: z.string().url().describe(\"The URL of the web page to scrape\"),\n  }),\n  outputSchema: z.object({\n    title: z.string(),\n    content: z.string(),\n    url: z.string(),\n    extractedAt: z.string(),\n  }),\n  execute: async ({ context: { url }, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"🔧 [WebScraping] Starting execution\", { url });\n    \n    const result = await scrapeWebContent({ url, logger });\n    \n    logger?.info(\"✅ [WebScraping] Completed successfully\", { \n      url: result.url,\n      contentLength: result.content.length,\n    });\n    \n    return result;\n  },\n});","size_bytes":2806},"src/mastra/tools/wikipediaResearchTool.ts":{"content":"import { createTool } from \"@mastra/core/tools\";\nimport type { IMastraLogger } from \"@mastra/core/logger\";\nimport { z } from \"zod\";\n\nconst searchWikipedia = async ({\n  topic,\n  logger,\n}: {\n  topic: string;\n  logger?: IMastraLogger;\n}) => {\n  logger?.info(\"🔍 [WikipediaResearch] Starting Wikipedia search\", { topic });\n\n  try {\n    // Search for articles related to the topic\n    const searchUrl = `https://en.wikipedia.org/api/rest_v1/page/summary/${encodeURIComponent(topic)}`;\n    const response = await fetch(searchUrl);\n    \n    if (!response.ok) {\n      throw new Error(`Wikipedia API error: ${response.status}`);\n    }\n\n    const data = await response.json();\n    \n    logger?.info(\"📚 [WikipediaResearch] Successfully retrieved Wikipedia data\", {\n      title: data.title,\n      extractLength: data.extract?.length || 0,\n    });\n\n    return {\n      title: data.title || topic,\n      summary: data.extract || \"No summary available\",\n      url: data.content_urls?.desktop?.page || \"\",\n      lastModified: data.timestamp || \"\",\n    };\n  } catch (error) {\n    logger?.error(\"❌ [WikipediaResearch] Error fetching Wikipedia data\", {\n      topic,\n      error: error instanceof Error ? error.message : String(error),\n    });\n    \n    return {\n      title: topic,\n      summary: `Unable to fetch Wikipedia data for ${topic}. Please research this topic manually.`,\n      url: \"\",\n      lastModified: \"\",\n    };\n  }\n};\n\nexport const wikipediaResearchTool = createTool({\n  id: \"wikipedia-research-tool\",\n  description: `Researches educational topics using Wikipedia to gather foundational information and context for content creation`,\n  inputSchema: z.object({\n    topic: z.string().describe(\"The educational topic to research\"),\n  }),\n  outputSchema: z.object({\n    title: z.string(),\n    summary: z.string(),\n    url: z.string(),\n    lastModified: z.string(),\n  }),\n  execute: async ({ context: { topic }, mastra }) => {\n    const logger = mastra?.getLogger();\n    logger?.info(\"🔧 [WikipediaResearch] Starting execution\", { topic });\n    \n    const result = await searchWikipedia({ topic, logger });\n    \n    logger?.info(\"✅ [WikipediaResearch] Completed successfully\", { \n      title: result.title,\n      summaryLength: result.summary.length,\n    });\n    \n    return result;\n  },\n});","size_bytes":2294},"src/mastra/workflows/educationalContentWorkflow.ts":{"content":"import { createWorkflow, createStep } from \"../inngest\";\nimport { z } from \"zod\";\nimport { RuntimeContext } from \"@mastra/core/di\";\nimport { ideaGenerationAgent } from \"../agents/ideaGenerationAgent\";\nimport { writingAgent } from \"../agents/writingAgent\";\nimport { reviewAgent } from \"../agents/reviewAgent\";\nimport { pdfGenerationTool } from \"../tools/pdfGenerationTool\";\n\nconst runtimeContext = new RuntimeContext();\n\n// Step 1: Generate educational outline\nconst generateOutlineStep = createStep({\n  id: \"generate-outline\",\n  description: \"Generate comprehensive educational outline for the specified topic\",\n  inputSchema: z.object({\n    topic: z.string().default(\"Python Programming\").describe(\"Educational topic to create content for\"),\n    targetAudience: z.string().default(\"Complete beginners\").describe(\"Target audience for the educational content\"),\n    estimatedLength: z.string().default(\"50,000+ words\").describe(\"Target length for the educational content\"),\n  }),\n  outputSchema: z.object({\n    topic: z.string(),\n    outline: z.string(),\n    chapterTitles: z.array(z.string()),\n    estimatedWordCount: z.number(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { topic, targetAudience, estimatedLength } = inputData;\n\n    logger?.info(\"📋 [GenerateOutline] Starting outline generation\", { \n      topic,\n      targetAudience,\n      estimatedLength \n    });\n\n    const prompt = `Create a comprehensive educational outline for \"${topic}\" suitable for ${targetAudience}. \n    This will be a detailed learning guide of approximately ${estimatedLength} in the style of an \"Idiot's Guide\" book.\n    \n    Please include:\n    1. A complete chapter breakdown with clear, descriptive titles\n    2. Learning objectives for each chapter\n    3. Logical progression from basic concepts to advanced topics\n    4. Key topics and concepts to cover in each chapter\n    5. Estimated word count per chapter (aim for 3000-5000 words per chapter)\n    \n    Make sure the outline is comprehensive enough to cover all essential aspects of ${topic} that a beginner would need to know.`;\n\n    const { text } = await ideaGenerationAgent.generate([\n      { role: \"user\", content: prompt },\n    ], {\n      resourceId: \"outline-generation\",\n      threadId: `outline-${Date.now()}`,\n      maxSteps: 10,\n    });\n\n    // Extract chapter titles from the outline\n    const chapterMatches = text.match(/Chapter \\d+[:\\-\\s]*([^\\n]+)/gi) || [];\n    const chapterTitles = chapterMatches.map(match => \n      match.replace(/Chapter \\d+[:\\-\\s]*/, '').trim()\n    );\n\n    // Estimate word count based on number of chapters\n    const estimatedWordCount = chapterTitles.length * 4000; // 4000 words per chapter average\n\n    logger?.info(\"✅ [GenerateOutline] Outline generated successfully\", {\n      topic,\n      chapterCount: chapterTitles.length,\n      estimatedWordCount,\n    });\n\n    return {\n      topic,\n      outline: text,\n      chapterTitles,\n      estimatedWordCount,\n    };\n  },\n});\n\n// Step 2: Write chapters based on outline\nconst writeChaptersStep = createStep({\n  id: \"write-chapters\",\n  description: \"Write detailed chapters based on the educational outline\",\n  inputSchema: z.object({\n    topic: z.string(),\n    outline: z.string(),\n    chapterTitles: z.array(z.string()),\n    estimatedWordCount: z.number(),\n  }),\n  outputSchema: z.object({\n    topic: z.string(),\n    chapters: z.array(z.object({\n      title: z.string(),\n      content: z.string(),\n    })),\n    totalWordCount: z.number(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { topic, outline, chapterTitles } = inputData;\n\n    logger?.info(\"✍️ [WriteChapters] Starting chapter writing\", { \n      topic,\n      chapterCount: chapterTitles.length \n    });\n\n    const chapters = [];\n    let totalWordCount = 0;\n\n    // Write each chapter\n    for (let i = 0; i < chapterTitles.length; i++) {\n      const chapterTitle = chapterTitles[i];\n      const chapterNumber = i + 1;\n\n      logger?.info(\"📝 [WriteChapters] Writing chapter\", { \n        chapterNumber,\n        title: chapterTitle \n      });\n\n      const chapterPrompt = `Based on this educational outline for \"${topic}\":\n\n${outline}\n\nWrite a comprehensive Chapter ${chapterNumber}: ${chapterTitle}\n\nThis chapter should be 3000-5000 words and include:\n- Clear introduction with learning objectives\n- Step-by-step explanations suitable for complete beginners\n- Practical examples and real-world applications\n- \"Key Points\" sections for important concepts\n- \"Try This\" exercises or examples where appropriate\n- \"Common Mistakes\" section to help learners avoid pitfalls\n- Chapter summary reinforcing key learning points\n\nWrite in a friendly, accessible tone as if explaining to someone with no prior knowledge. Use analogies and everyday examples to make complex concepts clear.`;\n\n      const { text: chapterContent } = await writingAgent.generate([\n        { role: \"user\", content: chapterPrompt },\n      ], {\n        resourceId: \"chapter-writing\",\n        threadId: `chapter-${chapterNumber}-${Date.now()}`,\n        maxSteps: 8,\n      });\n\n      // Estimate word count (rough approximation)\n      const wordCount = chapterContent.split(/\\s+/).length;\n      totalWordCount += wordCount;\n\n      chapters.push({\n        title: chapterTitle,\n        content: chapterContent,\n      });\n\n      logger?.info(\"✅ [WriteChapters] Chapter completed\", { \n        chapterNumber,\n        title: chapterTitle,\n        wordCount \n      });\n    }\n\n    logger?.info(\"✅ [WriteChapters] All chapters completed\", {\n      topic,\n      chapterCount: chapters.length,\n      totalWordCount,\n    });\n\n    return {\n      topic,\n      chapters,\n      totalWordCount,\n    };\n  },\n});\n\n// Step 3: Review and validate content\nconst reviewContentStep = createStep({\n  id: \"review-content\",\n  description: \"Review and validate the educational content for quality and accuracy\",\n  inputSchema: z.object({\n    topic: z.string(),\n    chapters: z.array(z.object({\n      title: z.string(),\n      content: z.string(),\n    })),\n    totalWordCount: z.number(),\n  }),\n  outputSchema: z.object({\n    topic: z.string(),\n    reviewSummary: z.string(),\n    qualityScore: z.number(),\n    approvedForPublication: z.boolean(),\n    chapters: z.array(z.object({\n      title: z.string(),\n      content: z.string(),\n    })),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { topic, chapters, totalWordCount } = inputData;\n\n    logger?.info(\"🔍 [ReviewContent] Starting content review\", { \n      topic,\n      chapterCount: chapters.length,\n      totalWordCount \n    });\n\n    // Create a summary of the content for review\n    const contentSummary = chapters.map((chapter, index) => \n      `Chapter ${index + 1}: ${chapter.title}\\n[Word count: ~${chapter.content.split(/\\s+/).length} words]\\n`\n    ).join('\\n');\n\n    const reviewPrompt = `Please review this educational content for \"${topic}\":\n\nCONTENT SUMMARY:\n${contentSummary}\n\nSAMPLE CONTENT (First Chapter):\n${chapters[0]?.content.substring(0, 2000)}...\n\nPlease provide a comprehensive review evaluating:\n1. ACCURACY: Are the concepts and explanations correct?\n2. CLARITY: Are explanations clear and beginner-friendly?\n3. PROGRESSION: Does content build logically from simple to complex?\n4. COMPLETENESS: Are there any obvious gaps or missing topics?\n5. ENGAGEMENT: Is the content engaging and accessible?\n6. CONSISTENCY: Is the tone and style consistent throughout?\n\nProvide:\n- Overall quality score (1-10)\n- Specific strengths and areas for improvement\n- Whether you approve this content for publication\n- Any critical issues that need addressing\n\nTotal word count: ${totalWordCount} words`;\n\n    const { text: reviewText } = await reviewAgent.generate([\n      { role: \"user\", content: reviewPrompt },\n    ], {\n      resourceId: \"content-review\",\n      threadId: `review-${Date.now()}`,\n      maxSteps: 5,\n    });\n\n    // Extract quality score from review (basic pattern matching)\n    const scoreMatch = reviewText.match(/quality score[:\\s]*(\\d+(?:\\.\\d+)?)/i);\n    const qualityScore = scoreMatch ? parseFloat(scoreMatch[1]) : 7.5;\n\n    // Determine approval based on quality score and review content\n    const approvedForPublication = qualityScore >= 7.0 && \n      !reviewText.toLowerCase().includes('not approved') &&\n      !reviewText.toLowerCase().includes('needs major revision');\n\n    logger?.info(\"✅ [ReviewContent] Review completed\", {\n      topic,\n      qualityScore,\n      approvedForPublication,\n    });\n\n    return {\n      topic,\n      reviewSummary: reviewText,\n      qualityScore,\n      approvedForPublication,\n      chapters,\n    };\n  },\n});\n\n// Step 4: Generate PDF book\nconst generatePDFStep = createStep({\n  id: \"generate-pdf\",\n  description: \"Generate professional PDF book from the reviewed content\",\n  inputSchema: z.object({\n    topic: z.string(),\n    reviewSummary: z.string(),\n    qualityScore: z.number(),\n    approvedForPublication: z.boolean(),\n    chapters: z.array(z.object({\n      title: z.string(),\n      content: z.string(),\n    })),\n  }),\n  outputSchema: z.object({\n    topic: z.string(),\n    bookGenerated: z.boolean(),\n    bookPath: z.string().optional(),\n    fileSize: z.number().optional(),\n    finalWordCount: z.number(),\n    generatedAt: z.string(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { topic, chapters, approvedForPublication, qualityScore } = inputData;\n\n    logger?.info(\"📚 [GeneratePDF] Starting PDF generation\", { \n      topic,\n      chapterCount: chapters.length,\n      approvedForPublication \n    });\n\n    if (!approvedForPublication) {\n      logger?.warn(\"📚 [GeneratePDF] Content not approved for publication\", { \n        topic,\n        qualityScore \n      });\n      \n      return {\n        topic,\n        bookGenerated: false,\n        finalWordCount: chapters.reduce((total, chapter) => \n          total + chapter.content.split(/\\s+/).length, 0),\n        generatedAt: new Date().toISOString(),\n      };\n    }\n\n    // Generate the PDF using the PDF generation tool\n    const result = await pdfGenerationTool.execute({\n      context: {\n        title: `The Complete Idiot's Guide to ${topic}`,\n        subtitle: \"A Comprehensive Educational Resource\",\n        author: \"AI Educational Content System\",\n        chapters: chapters,\n      },\n      runtimeContext,\n      tracingContext: {},\n    });\n\n    const finalWordCount = chapters.reduce((total, chapter) => \n      total + chapter.content.split(/\\s+/).length, 0);\n\n    logger?.info(\"✅ [GeneratePDF] PDF generated successfully\", {\n      topic,\n      path: result.path,\n      fileSize: result.fileSize,\n      finalWordCount,\n    });\n\n    return {\n      topic,\n      bookGenerated: true,\n      bookPath: result.path,\n      fileSize: result.fileSize,\n      finalWordCount,\n      generatedAt: new Date().toISOString(),\n    };\n  },\n});\n\n// Create the main workflow\nexport const educationalContentWorkflow = createWorkflow({\n  id: \"educational-content-workflow\",\n  description: \"Autonomous multi-agent system that creates comprehensive educational content\",\n  inputSchema: z.object({}), // Empty for time-based workflows\n  outputSchema: z.object({\n    success: z.boolean(),\n    topic: z.string(),\n    bookGenerated: z.boolean(),\n    finalWordCount: z.number(),\n    qualityScore: z.number(),\n    generatedAt: z.string(),\n  }),\n})\n  .then(generateOutlineStep)\n  .then(writeChaptersStep)\n  .then(reviewContentStep)\n  .then(generatePDFStep)\n  .commit();","size_bytes":11593},"src/mastra/workflows/improvedEducationalContentWorkflow.ts":{"content":"import { createWorkflow, createStep } from \"../inngest\";\nimport { z } from \"zod\";\nimport { RuntimeContext } from \"@mastra/core/di\";\nimport { ideaGenerationAgent } from \"../agents/ideaGenerationAgent\";\nimport { writingAgent } from \"../agents/writingAgent\";\nimport { reviewAgent } from \"../agents/reviewAgent\";\nimport { pdfGenerationTool } from \"../tools/pdfGenerationTool\";\nimport { chunkedContentGenerationTool } from \"../tools/chunkedContentGenerationTool\";\nimport { progressTrackingTool } from \"../tools/progressTrackingTool\";\n\nconst runtimeContext = new RuntimeContext();\n\n// Step 1: Initialize progress tracking and generate outline\nconst initializeAndPlanStep = createStep({\n  id: \"initialize-and-plan\",\n  description: \"Initialize progress tracking and generate comprehensive educational outline\",\n  inputSchema: z.object({\n    topic: z.string().default(\"Advanced JavaScript Programming\").describe(\"Educational topic to create content for\"),\n    targetAudience: z.string().default(\"Intermediate developers\").describe(\"Target audience for the educational content\"),\n    targetWordCount: z.number().default(60000).describe(\"Target total word count for the content\"),\n  }),\n  outputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    outline: z.string(),\n    chapters: z.array(z.object({\n      number: z.number(),\n      title: z.string(),\n      sections: z.array(z.string()),\n      targetWordCount: z.number(),\n    })),\n    totalSections: z.number(),\n    progressInitialized: z.boolean(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { topic, targetAudience, targetWordCount } = inputData;\n    const workflowId = `edu-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n\n    logger?.info(\"🚀 [InitializePlan] Starting workflow initialization\", { \n      workflowId,\n      topic,\n      targetWordCount \n    });\n\n    // Generate comprehensive outline\n    const outlinePrompt = `Create a comprehensive educational outline for \"${topic}\" for ${targetAudience}.\n    \n    This will be a detailed learning guide of ${targetWordCount ? targetWordCount.toLocaleString() : '60,000'} words total.\n    \n    Structure this as 8-12 chapters, each with 4-6 sections. Each section should be approximately 600-800 words.\n    \n    Format your response as:\n    \n    CHAPTER OUTLINE:\n    \n    Chapter 1: [Title]\n    - Section 1.1: [Title]\n    - Section 1.2: [Title]\n    - Section 1.3: [Title]\n    - Section 1.4: [Title]\n    \n    Chapter 2: [Title]\n    - Section 2.1: [Title]\n    - Section 2.2: [Title]\n    - Section 2.3: [Title]\n    - Section 2.4: [Title]\n    \n    Continue for all chapters...\n    \n    Include clear, descriptive titles that build from basic concepts to advanced applications.`;\n\n    const { text: outlineText } = await ideaGenerationAgent.generate([\n      { role: \"user\", content: outlinePrompt },\n    ], {\n      resourceId: \"outline-generation\",\n      threadId: workflowId,\n      maxSteps: 10,\n    });\n\n    // Parse the outline into structured data\n    const chapters = parseOutlineIntoChapters(outlineText, targetWordCount);\n    const totalSections = chapters.reduce((sum, chapter) => sum + chapter.sections.length, 0);\n\n    logger?.info(\"📋 [InitializePlan] Outline generated\", {\n      workflowId,\n      chapterCount: chapters.length,\n      totalSections,\n    });\n\n    // Initialize progress tracking\n    const progressResult = await progressTrackingTool.execute({\n      context: {\n        action: 'initialize',\n        workflowId,\n        topic,\n        totalChapters: chapters.length,\n        totalSections,\n        targetWordCount,\n      },\n      runtimeContext,\n      tracingContext: {},\n    });\n\n    logger?.info(\"✅ [InitializePlan] Initialization completed\", {\n      workflowId,\n      progressInitialized: progressResult.success,\n    });\n\n    return {\n      workflowId,\n      topic,\n      outline: outlineText,\n      chapters,\n      totalSections,\n      progressInitialized: progressResult.success,\n    };\n  },\n});\n\n// Step 2: Generate content for all chapters using chunked approach\nconst generateAllContentStep = createStep({\n  id: \"generate-all-content\",\n  description: \"Generate all educational content using chunked, resumable approach\",\n  inputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    outline: z.string(),\n    chapters: z.array(z.object({\n      number: z.number(),\n      title: z.string(),\n      sections: z.array(z.string()),\n      targetWordCount: z.number(),\n    })),\n    totalSections: z.number(),\n    progressInitialized: z.boolean(),\n  }),\n  outputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    generatedChapters: z.array(z.object({\n      number: z.number(),\n      title: z.string(),\n      content: z.string(),\n      wordCount: z.number(),\n      sections: z.array(z.object({\n        title: z.string(),\n        content: z.string(),\n        wordCount: z.number(),\n      })),\n    })),\n    totalWordCount: z.number(),\n    allContentGenerated: z.boolean(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { workflowId, topic, outline, chapters } = inputData;\n\n    logger?.info(\"✍️ [GenerateContent] Starting chunked content generation\", { \n      workflowId,\n      chapterCount: chapters.length \n    });\n\n    const generatedChapters = [];\n    let totalWordCount = 0;\n    let completedSections = 0;\n\n    // Generate content for each chapter\n    for (const chapter of chapters) {\n      logger?.info(\"📝 [GenerateContent] Starting chapter\", { \n        chapterNumber: chapter.number,\n        title: chapter.title,\n        sectionCount: chapter.sections.length\n      });\n\n      // Update progress\n      await progressTrackingTool.execute({\n        context: {\n          action: 'update',\n          workflowId,\n          currentStep: `Generating Chapter ${chapter.number}: ${chapter.title}`,\n          completedSections,\n        },\n        runtimeContext,\n        tracingContext: {},\n      });\n\n      const targetWordsPerSection = Math.floor(chapter.targetWordCount / chapter.sections.length);\n      const chapterSections = [];\n      let chapterWordCount = 0;\n\n      // Generate each section of the chapter\n      for (let sectionIndex = 0; sectionIndex < chapter.sections.length; sectionIndex++) {\n        const sectionTitle = chapter.sections[sectionIndex];\n        \n        logger?.info(\"📄 [GenerateContent] Generating section\", {\n          chapterNumber: chapter.number,\n          sectionIndex,\n          sectionTitle,\n          targetWords: targetWordsPerSection\n        });\n\n        try {\n          const sectionResult = await chunkedContentGenerationTool.execute({\n            context: {\n              topic,\n              chapterTitle: chapter.title,\n              chapterNumber: chapter.number,\n              sectionTitles: chapter.sections,\n              targetWordsPerSection,\n              context: outline,\n              sectionIndex,\n              retryAttempts: 3,\n              workflowId,\n            },\n            runtimeContext,\n            tracingContext: {},\n          });\n\n          chapterSections.push({\n            title: sectionResult.sectionTitle,\n            content: sectionResult.content,\n            wordCount: sectionResult.wordCount,\n          });\n\n          chapterWordCount += sectionResult.wordCount;\n          completedSections++;\n\n          logger?.info(\"✅ [GenerateContent] Section completed\", {\n            sectionTitle,\n            wordCount: sectionResult.wordCount,\n            completedSections,\n          });\n\n          // Update progress after each section\n          await progressTrackingTool.execute({\n            context: {\n              action: 'update',\n              workflowId,\n              completedSections,\n              totalWordsGenerated: totalWordCount + chapterWordCount,\n            },\n            runtimeContext,\n            tracingContext: {},\n          });\n\n        } catch (error) {\n          logger?.error(\"❌ [GenerateContent] Section generation failed\", {\n            chapterNumber: chapter.number,\n            sectionIndex,\n            sectionTitle,\n            error: error instanceof Error ? error.message : String(error),\n          });\n\n          // Record error in progress tracking\n          await progressTrackingTool.execute({\n            context: {\n              action: 'update',\n              workflowId,\n              error: `Failed to generate section \"${sectionTitle}\": ${error instanceof Error ? error.message : String(error)}`,\n            },\n            runtimeContext,\n            tracingContext: {},\n          });\n\n          throw error;\n        }\n      }\n\n      // Combine all sections into chapter content\n      const chapterContent = chapterSections.map(section => \n        `## ${section.title}\\n\\n${section.content}`\n      ).join('\\n\\n');\n\n      generatedChapters.push({\n        number: chapter.number,\n        title: chapter.title,\n        content: chapterContent,\n        wordCount: chapterWordCount,\n        sections: chapterSections,\n      });\n\n      totalWordCount += chapterWordCount;\n\n      // Update progress after completing chapter\n      await progressTrackingTool.execute({\n        context: {\n          action: 'update',\n          workflowId,\n          completedChapters: generatedChapters.length,\n          totalWordsGenerated: totalWordCount,\n          chapterCompleted: {\n            chapterNumber: chapter.number,\n            title: chapter.title,\n            wordCount: chapterWordCount,\n          },\n        },\n        runtimeContext,\n        tracingContext: {},\n      });\n\n      logger?.info(\"✅ [GenerateContent] Chapter completed\", {\n        chapterNumber: chapter.number,\n        title: chapter.title,\n        wordCount: chapterWordCount,\n        totalWordCount,\n      });\n    }\n\n    logger?.info(\"✅ [GenerateContent] All content generation completed\", {\n      workflowId,\n      chapterCount: generatedChapters.length,\n      totalWordCount,\n    });\n\n    return {\n      workflowId,\n      topic,\n      generatedChapters,\n      totalWordCount,\n      allContentGenerated: true,\n    };\n  },\n});\n\n// Step 3: Review generated content\nconst reviewContentStep = createStep({\n  id: \"review-content-improved\",\n  description: \"Review and validate the generated educational content\",\n  inputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    generatedChapters: z.array(z.object({\n      number: z.number(),\n      title: z.string(),\n      content: z.string(),\n      wordCount: z.number(),\n      sections: z.array(z.object({\n        title: z.string(),\n        content: z.string(),\n        wordCount: z.number(),\n      })),\n    })),\n    totalWordCount: z.number(),\n    allContentGenerated: z.boolean(),\n  }),\n  outputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    reviewSummary: z.string(),\n    qualityScore: z.number(),\n    approvedForPublication: z.boolean(),\n    finalChapters: z.array(z.object({\n      title: z.string(),\n      content: z.string(),\n    })),\n    finalWordCount: z.number(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { workflowId, topic, generatedChapters, totalWordCount } = inputData;\n\n    logger?.info(\"🔍 [ReviewContent] Starting content review\", { \n      workflowId,\n      chapterCount: generatedChapters.length,\n      totalWordCount \n    });\n\n    // Update progress\n    await progressTrackingTool.execute({\n      context: {\n        action: 'update',\n        workflowId,\n        currentStep: 'Reviewing generated content',\n      },\n      runtimeContext,\n      tracingContext: {},\n    });\n\n    // Create summary for review\n    const contentSummary = generatedChapters.map(chapter => \n      `Chapter ${chapter.number}: ${chapter.title} (${chapter.wordCount} words, ${chapter.sections.length} sections)`\n    ).join('\\n');\n\n    // Sample content for review (first chapter content, truncated)\n    const sampleContent = generatedChapters[0]?.content.substring(0, 3000) || \"\";\n\n    const reviewPrompt = `Review this educational content for \"${topic}\":\n\nCONTENT SUMMARY:\n${contentSummary}\n\nTOTAL WORD COUNT: ${totalWordCount ? totalWordCount.toLocaleString() : '0'} words\n\nSAMPLE CONTENT (First Chapter):\n${sampleContent}...\n\nEvaluate:\n1. ACCURACY: Are concepts correct and well-explained?\n2. CLARITY: Is content clear and appropriate for the target audience?\n3. PROGRESSION: Does content build logically?\n4. COMPLETENESS: Are there gaps or missing topics?\n5. ENGAGEMENT: Is content engaging and accessible?\n6. CONSISTENCY: Is tone and style consistent?\n\nProvide:\n- Overall quality score (1-10)\n- Strengths and improvements needed\n- Approval recommendation (approve/needs revision)\n- Word count assessment (is ${totalWordCount ? totalWordCount.toLocaleString() : '0'} words appropriate?)`;\n\n    const { text: reviewText } = await reviewAgent.generate([\n      { role: \"user\", content: reviewPrompt },\n    ], {\n      resourceId: \"content-review\",\n      threadId: `review-${workflowId}`,\n      maxSteps: 8,\n    });\n\n    // Extract quality score\n    const scoreMatch = reviewText.match(/quality score[:\\s]*(\\d+(?:\\.\\d+)?)/i);\n    const qualityScore = scoreMatch ? parseFloat(scoreMatch[1]) : 7.5;\n\n    // Determine approval\n    const approvedForPublication = qualityScore >= 7.0 && \n      !reviewText.toLowerCase().includes('not approved') &&\n      !reviewText.toLowerCase().includes('needs major revision');\n\n    // Prepare final chapters for PDF generation\n    const finalChapters = generatedChapters.map(chapter => ({\n      title: chapter.title,\n      content: chapter.content,\n    }));\n\n    logger?.info(\"✅ [ReviewContent] Review completed\", {\n      workflowId,\n      qualityScore,\n      approvedForPublication,\n      finalWordCount: totalWordCount,\n    });\n\n    return {\n      workflowId,\n      topic,\n      reviewSummary: reviewText,\n      qualityScore,\n      approvedForPublication,\n      finalChapters,\n      finalWordCount: totalWordCount,\n    };\n  },\n});\n\n// Step 4: Generate final PDF\nconst generateFinalPDFStep = createStep({\n  id: \"generate-final-pdf\",\n  description: \"Generate the final PDF book from reviewed content\",\n  inputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    reviewSummary: z.string(),\n    qualityScore: z.number(),\n    approvedForPublication: z.boolean(),\n    finalChapters: z.array(z.object({\n      title: z.string(),\n      content: z.string(),\n    })),\n    finalWordCount: z.number(),\n  }),\n  outputSchema: z.object({\n    workflowId: z.string(),\n    topic: z.string(),\n    bookGenerated: z.boolean(),\n    bookPath: z.string().optional(),\n    fileSize: z.number().optional(),\n    finalWordCount: z.number(),\n    qualityScore: z.number(),\n    completedAt: z.string(),\n  }),\n\n  execute: async ({ inputData, mastra }) => {\n    const logger = mastra?.getLogger();\n    const { workflowId, topic, finalChapters, approvedForPublication, qualityScore, finalWordCount } = inputData;\n\n    logger?.info(\"📚 [GeneratePDF] Starting final PDF generation\", { \n      workflowId,\n      chapterCount: finalChapters.length,\n      approvedForPublication \n    });\n\n    // Update progress\n    await progressTrackingTool.execute({\n      context: {\n        action: 'update',\n        workflowId,\n        currentStep: 'Generating final PDF',\n      },\n      runtimeContext,\n      tracingContext: {},\n    });\n\n    if (!approvedForPublication) {\n      logger?.warn(\"📚 [GeneratePDF] Content not approved for publication\", { \n        workflowId,\n        qualityScore \n      });\n\n      await progressTrackingTool.execute({\n        context: {\n          action: 'fail',\n          workflowId,\n          error: `Content not approved for publication (quality score: ${qualityScore})`,\n        },\n        runtimeContext,\n        tracingContext: {},\n      });\n      \n      return {\n        workflowId,\n        topic,\n        bookGenerated: false,\n        finalWordCount,\n        qualityScore,\n        completedAt: new Date().toISOString(),\n      };\n    }\n\n    try {\n      // Generate the PDF\n      const pdfResult = await pdfGenerationTool.execute({\n        context: {\n          title: `The Complete Guide to ${topic}`,\n          subtitle: \"A Comprehensive Educational Resource\",\n          author: \"AI Educational Content System\",\n          chapters: finalChapters,\n        },\n        runtimeContext,\n        tracingContext: {},\n      });\n\n      // Mark workflow as completed\n      await progressTrackingTool.execute({\n        context: {\n          action: 'complete',\n          workflowId,\n        },\n        runtimeContext,\n        tracingContext: {},\n      });\n\n      logger?.info(\"✅ [GeneratePDF] PDF generated successfully\", {\n        workflowId,\n        path: pdfResult.path,\n        fileSize: pdfResult.fileSize,\n        finalWordCount,\n      });\n\n      return {\n        workflowId,\n        topic,\n        bookGenerated: true,\n        bookPath: pdfResult.path,\n        fileSize: pdfResult.fileSize,\n        finalWordCount,\n        qualityScore,\n        completedAt: new Date().toISOString(),\n      };\n\n    } catch (error) {\n      logger?.error(\"❌ [GeneratePDF] PDF generation failed\", {\n        workflowId,\n        error: error instanceof Error ? error.message : String(error),\n      });\n\n      await progressTrackingTool.execute({\n        context: {\n          action: 'fail',\n          workflowId,\n          error: `PDF generation failed: ${error instanceof Error ? error.message : String(error)}`,\n        },\n        runtimeContext,\n        tracingContext: {},\n      });\n\n      throw error;\n    }\n  },\n});\n\n// Helper function to parse outline into structured chapters\nfunction parseOutlineIntoChapters(outlineText: string, targetWordCount: number): Array<{\n  number: number;\n  title: string;\n  sections: string[];\n  targetWordCount: number;\n}> {\n  const chapters: Array<{\n    number: number;\n    title: string;\n    sections: string[];\n    targetWordCount: number;\n  }> = [];\n  const lines = outlineText.split('\\n');\n  let currentChapter: {\n    number: number;\n    title: string;\n    sections: string[];\n    targetWordCount: number;\n  } | null = null;\n  let chapterNumber = 1;\n\n  // Calculate target words per chapter\n  const estimatedChapters = 10; // Default estimate\n  const wordsPerChapter = Math.floor(targetWordCount / estimatedChapters);\n\n  for (const line of lines) {\n    const trimmedLine = line.trim();\n    \n    // Match chapter lines (e.g., \"Chapter 1: Title\" or \"1. Title\")\n    const chapterMatch = trimmedLine.match(/^(?:Chapter\\s+)?(\\d+)[:\\.\\s]+(.+)$/i);\n    if (chapterMatch && !trimmedLine.startsWith('-')) {\n      if (currentChapter) {\n        chapters.push(currentChapter);\n      }\n      \n      currentChapter = {\n        number: chapterNumber++,\n        title: chapterMatch[2].trim(),\n        sections: [],\n        targetWordCount: wordsPerChapter,\n      };\n    }\n    \n    // Match section lines (e.g., \"- Section 1.1: Title\" or \"  - Title\")\n    const sectionMatch = trimmedLine.match(/^[-•]\\s*(?:Section\\s+\\d+\\.\\d+:\\s*)?(.+)$/);\n    if (sectionMatch && currentChapter) {\n      currentChapter.sections.push(sectionMatch[1].trim());\n    }\n  }\n  \n  if (currentChapter) {\n    chapters.push(currentChapter);\n  }\n\n  // If no chapters were parsed, create a default structure\n  if (chapters.length === 0) {\n    for (let i = 1; i <= 8; i++) {\n      chapters.push({\n        number: i,\n        title: `Chapter ${i}`,\n        sections: [`Introduction`, `Core Concepts`, `Practical Examples`, `Advanced Topics`, `Summary`],\n        targetWordCount: wordsPerChapter,\n      });\n    }\n  }\n\n  return chapters;\n}\n\n// Create the improved workflow\nexport const improvedEducationalContentWorkflow = createWorkflow({\n  id: \"improved-educational-content-workflow\",\n  description: \"Improved autonomous system that creates comprehensive educational content with chunked generation and progress tracking\",\n  inputSchema: z.object({}), // Empty for time-based workflows\n  outputSchema: z.object({\n    success: z.boolean(),\n    workflowId: z.string(),\n    topic: z.string(),\n    bookGenerated: z.boolean(),\n    finalWordCount: z.number(),\n    qualityScore: z.number(),\n    completedAt: z.string(),\n    bookPath: z.string().optional(),\n  }),\n})\n  .then(initializeAndPlanStep)\n  .then(generateAllContentStep)\n  .then(reviewContentStep)\n  .then(generateFinalPDFStep)\n  .commit();","size_bytes":20358}},"version":1}